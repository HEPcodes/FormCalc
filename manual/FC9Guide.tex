\documentclass[twoside,11pt]{article}
\usepackage{a4wide,amsmath,amssymb,array,axodrawth,makeidx,alltt,graphicx,mathpple}
%\usepackage{colordvi}
\usepackage{pstricks}

\graphicspath{{figures/}}


\special{!TeXDict begin
%/Blue{0 0 1 setrgbcolor}DC
%/Red{1 0 0 setrgbcolor}DC
/PastelRed{1 .8 .8 setrgbcolor}DC
/PastelBlue{.8 .8 1 setrgbcolor}DC
/PastelGreen{.54 .9 .54 setrgbcolor}DC
%/OliveGreen{.216 .60 .030 setrgbcolor}DC
end }

%\newrgbcolor{PastelRed}{1 .8 .8}
%\newrgbcolor{PastelBlue}{.8 .8 1}
%\newrgbcolor{PastelGreen}{.54 .9 .54}

\makeindex
\def\indextt#1{\index{#1@\texttt{#1}}}

\makeatletter
%\renewcommand{\rmdefault}{ppl}
%\DeclareSymbolFont{operators}{OT1}{pplcm}{m}{n}
%\DeclareSymbolFont{letters}{OML}{pplcm}{m}{it}
%%\DeclareSymbolFont{symbols}{OMS}{pzccm}{m}{n}
%\DeclareSymbolFont{largesymbols}{OMX}{psycm}{m}{n}
%\DeclareSymbolFont{bold}{OT1}{ppl}{bx}{n}
%\DeclareSymbolFont{italic}{OT1}{ppl}{m}{it}
%\DeclareMathAlphabet{\mathrm}{OT1}{ppl}{m}{n}
%\DeclareMathAlphabet{\mathbf}{OT1}{ppl}{bx}{n}
%\DeclareMathAlphabet{\mathit}{OT1}{ppl}{m}{it}

%\renewcommand\bibname{References}
\renewcommand{\baselinestretch}{1.2}
\renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{8pt}
\renewcommand{\arraycolsep}{8pt}
%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\theenumi)\,}
\advance\footnotesep 4pt
\def\thefootnote{\fnsymbol{footnote}}
\parskip=4pt
\parindent=0pt
\itemsep=0pt
\pagestyle{headings}
\raggedbottom
\sloppy

\def\bbox{\vskip .5\baselineskip\par
  \newbox\grey\setbox\grey=\vbox\bgroup\ignorespaces}
\def\ebox{\egroup%
  \hbox{%
    \special{ps: gsave
      initmatrix currentpoint translate 1 65781 div dup scale % 1bp = 65781sp
      newpath
      0 -\number\dp\grey\space moveto
      \number\wd\grey\space dup dup 0 rlineto
      \number\ht\grey\space lineto
      neg 0 rlineto
      closepath
      gsave .9 setgray fill grestore
      0 setlinewidth stroke
      grestore}%
    \box\grey}%
  \vskip .5\baselineskip\par}

\def\oldcr#1{\let\temp=\\#1\let\\=\temp}
\def\biitab{\bbox%
  \begin{tabular}{>{\oldcr\raggedleft\hspace{0pt}}p{.35\linewidth}%
                  >{\oldcr\raggedright\hspace{0pt}}p{.57\linewidth}}}
\def\biiitab#1{\bbox%
  \hspace*{5pt}
  \begin{tabular}{>{\oldcr\raggedright\hspace{0pt}}p{.235\linewidth}%
                  >{\oldcr\raggedright\hspace{0pt}}p{.185\linewidth}%
                  >{\oldcr\raggedright\hspace{0pt}}p{.44\linewidth}}
  \textit{#1} & \textit{default value} \\ \hline}
\def\etab{\end{tabular}\ebox}
\def\ownline#1{\rlap{#1}\hspace*{13.5em} &}

\def\realcplx#1{\Code{#1}\{\Code{Real},\Code{Complex}\}}

\def\greyed#1{\special{ps: .5 setgray}#1\special{ps: 0 setgray}}

%\let\dots\textellipsis
\def\FA{\textit{FeynArts}}
\def\FC{\textit{FormCalc}}
\def\FO{\textit{FORM}}
\def\FF{\textit{FF}}
\def\LT{\textit{LoopTools}}
\def\cuba{\textsc{Cuba}}
\def\mma{\textit{Mathematica}}
\def\Var#1{\ensuremath{\mathit{#1}}}
%\def\Var#1{\ensuremath{\mathit{\Red{#1}}}}
\def\VX{\Var{X}}
\def\Va{\Var{a}}
\def\Vb{\Var{b}}
\def\Vc{\Var{c}}
\def\Vd{\Var{d}}
\def\Ve{\Var{e}}
\def\Vf{\Var{f}}
\def\Vh{\Var{h}}
\def\Vi{\Var{i}}
\def\Vj{\Var{j}}
\def\Vm{\Var{m}}
\def\Vn{\Var{n}}
\def\Vp{\Var{p}}
\def\Vq{\Var{q}}
\def\Vr{\Var{r}}
\def\Vs{\Var{s}}
\def\Vt{\Var{t}}
\def\Vv{\Var{v}}
\def\MtoN.#1{\Code{\Var{M}to\Var{N}.#1}}
\def\Code#1{\ensuremath{\texttt{#1}}}
%\def\Code#1{\ensuremath{\texttt{\Red{#1}}}}
\def\sqrtS#1{(\sqrt s)^{\mathrm{#1}}}
\def\limfunc#1{\mathop{\mathrm{#1}}}
\def\Re{\limfunc{Re}}
\def\Retilde{\limfunc{\widetilde{Re}}}
\def\Imtilde{\limfunc{\widetilde{Im}}}
\def\Tr{\limfunc{Tr}}
\def\unity{{\rm 1\mskip-4.25mu l}}
\def\ie{i.e.\ }
\def\eg{e.g.\ }
\def\lbrac{\symbol{123}}
\def\rbrac{\symbol{125}}
\def\Brac#1{\lbrac#1\rbrac}
\def\uscore{\symbol{95}}
\def\backsl{\symbol{92}}
\def\pipe{\symbol{124}}
\def\home{\symbol{126}}
\def\power{\symbol{94}}
\def\ri{\mathrm{i}}
\def\rd{\mathrm{d}}
\def\M{\mathcal{M}}
\def\Mtree{\M_{\text{tree}}}
\def\Mloop{\M_{\text{1-loop}}}
\def\O{\mathcal{O}}
\def\rsub#1#2{#1_{\mathrm{#2}}}
\def\MW{\rsub MW}
\def\MZ{\rsub MZ}
\def\pin{\rsub p{in}}
\def\pout{\rsub p{out}}
\def\pslash{\rlap{\hspace*{.8bp}/}p}
\def\qslash{\rlap{/}q}
\def\kslash{\rlap{\hspace*{-.8bp}/}k}
\def\sslash{\rlap{\hspace*{-1bp}/}s}
\def\vp{\vphantom{p}}
\def\ket#1{\left| #1\right\rangle }
\def\bra#1{\left\langle #1\right| }
\def\braket#1#2{\left\langle #1\vphantom{#2}
  \right. \kern-2.5pt\left| #2\vphantom{#1}\right\rangle}

\hyphenation{Feyn-Arts}

\begin{document}

\thispagestyle{empty}

\vspace*{.7\textheight}

\newbox\titlebox\setbox\titlebox=\hbox{\underline{%
\vrule width 0pt height 0pt depth 2ex%
\Huge \FC~9.6~~~User's Guide}}

\hfill\copy\titlebox

\vspace*{1ex}

\hfill\hbox{Sep 6, 2021~~~~~Thomas Hahn}

\bigskip
\bigskip

\hfill\parbox{\wd\titlebox}{%
\begin{small}
Abstract:
\FC\ is a \mma\ package which calculates and simplifies tree-level and
one-loop Feynman diagrams.  It accepts diagrams generated with \FA\ 3
and returns the results in a way well suited for further numerical or
analytical evaluation.
\end{small}}

\clearpage

\vspace*{.5\textheight}
\vfill

\hrule

\medskip

\begin{scriptsize}
The dreadful legal stuff:
\FC\ is free software, but is not in the public domain.
Instead it is covered by the GNU library general public license.
In plain English this means:

1) We don't promise that this software works.   
(But if you find any bugs, please let us know!)

2) You can use this software for whatever you want.
You don't have to pay us.

3) You may not pretend that you wrote this software.
If you use it in a program, you must acknowledge
somewhere in your publication that you've used  
our code.

If you're a lawyer, you will rejoice at the exact wording of the license 
at \Code{http://gnu.org/licenses/lgpl.html}.

\FC\ is available from \Code{http://feynarts.de/formcalc}.

\FA\ is available from \Code{http://feynarts.de}.

\FO\ is available from \Code{http://nikhef.nl/\home form}.

\LT\ is available from \Code{http://feynarts.de/looptools}.

If you make this software available to others please provide them with
this manual, too.  There exists a low-traffic mailing list where updates 
will be announced.  Contact \Code{hahn@feynarts.de} to be added to this
list.

If you find any bugs, or want to make suggestions, or just write fan mail,
address it to:
\vspace*{-2ex}
\begin{quote}
Thomas Hahn \\
Max-Planck-Institut f\"ur Physik \\
(Werner-Heisenberg-Institut) \\
F\"ohringer Ring 6 \\
D--80805 Munich, Germany \\
e-mail: \Code{hahn@feynarts.de}
\end{quote}
\end{scriptsize}

\clearpage

\tableofcontents

\clearpage

\section{General Considerations}
\label{sect:general}

With the increasing accuracy of experimental data, one-loop calculations
have in many cases come to be regarded as the lowest approximation
acceptable to publish the results in a respected journal.  \FC\ goes a big
step towards automating these calculations.

\FC\ is a \mma\ package which calculates and simplifies tree-level and
one-loop Feynman diagrams.  It accepts diagrams generated with \FA\ 3
\cite{Ha00} and returns the results in a way well suited for further
numerical (or analytical) evaluation.

Internally, \FC\ performs most of the hard work (\eg working out
fermionic traces) in \FO, by Jos Vermaseren \cite{Ve00}.  A substantial
part of the \mma\ code indeed acts as a driver that threads the \FA\
amplitudes through \FO\ in an appropriate way.  The concept is rather
straightforward: the symbolic expressions of the diagrams are prepared
in an input file for \FO, then \FO\ is run, and finally the results are
read back into \mma.  The interfacing is completely shielded from the user
and is handled internally by the \FC\ functions.  The following diagram
shows schematically how \FC\ interacts with \FO:
\begin{center}
\includegraphics{FormCalc}
\end{center}
\FC\ combines the speed of \FO\ with the powerful instruction set of
\mma\ and the latter greatly facilitates further processing of the
results.  Owing to \FO's speed, \FC\ can process, for example, the
1000-odd one-loop diagrams of W--W scattering in the Standard Model in
a few minutes on ordinary hardware.

One important aspect of \FC\ is that it automatically gathers spinor
chains, scalar products of vectors, and antisymmetric tensors contracted
with vectors, and introduces abbreviations for them.  In calculations with
non-scalar external particles where such objects are ubiquitous, code
produced from the \FC\ output (say, in Fortran) can be significantly
shorter and faster than without the abbreviations.%
\index{abbreviations}

\FC\ can work in $D$ and 4 dimensions.  In $D$ dimensions it uses standard
dimensional regularization to treat ultraviolet divergences, in 4
dimensions it uses the method of constrained differential renormalization,
which at the one-loop level is equivalent to dimensional reduction.  
Details on these methods can be found in \cite{HaP98}.%
\index{dimensional!regularization}%
\index{dimensional!reduction}%
\index{constrained differential\\renormalization}

A one-loop calculation generally includes three steps:
\begin{center}
\renewcommand{\arraystretch}{1}
\begin{tabular}{|c|c|l|l}
\cline{1-1} \cline{3-3}
		&& $\bullet$ Create the topologies \\
Diagram		&& $\bullet$ Insert fields \\
generation	&& $\bullet$ Apply the Feynman rules \\
		&& $\bullet$ Paint the diagrams
& \smash{\raise 4.6ex%
\hbox{$\left.\vrule width 0pt depth 5.7ex height 0pt\right\}$ \FA}} \\
\cline{1-1} \cline{3-3}
\multicolumn{1}{c}{$\downarrow$} \\
\cline{1-1} \cline{3-3}
		&& $\bullet$ Contract indices \\
Algebraic	&& $\bullet$ Calculate traces \\
simplification	&& $\bullet$ Reduce tensor integrals \\
		&& $\bullet$ Introduce abbreviations
& \smash{\lower 1.1ex%
\hbox{$\left.\vrule width 0pt depth 12ex height 0pt\right\}$ \FC}} \\
\cline{1-1} \cline{3-3}
\multicolumn{1}{c}{$\downarrow$} \\
\cline{1-1} \cline{3-3}
		&& $\bullet$ Convert \mma\ output \\
Numerical	&& \qquad to Fortran/C code \\
evaluation	&& $\bullet$ Supply a driver program \\
		&& $\bullet$ Implementation of the integrals
& $\left.\mathstrut\right\}\,$ \LT \\
\cline{1-1} \cline{3-3}
\end{tabular}
\renewcommand{\arraystretch}{1.2}
\end{center}
The automation of the calculation is fairly complete in \FC, \ie \FC\ can
eventually produce a complete program to calculate the squared matrix
element for a given process.  The only thing the user has to supply is a
driver program which calls the generated subroutines.  The \FC\
distribution includes a directory of tools and sample programs which can
be modified and/or extended for a particular application.  To demonstrate
how a full process is calculated, several non-trivial one-loop
calculations in the electroweak Standard Model are included in the \FC\
package.%
\index{numerical evaluation}%
\index{demo programs}

It is nevertheless important to realize that the code is generated only 
at the very end of the calculation (if at all), \ie the calculation 
proceeds analytically as far as possible.  At all intermediate stages, 
the results are \mma\ expressions which are considerably easier to 
modify than Fortran or C code.


\section{Installation}
\label{sect:install}

\index{installation}%
To run \FC\ you need \mma\ 5 or above, a Fortran compiler, and 
\Code{gcc}, the GNU C compiler.  \FC\ comes in a compressed tar archive 
\Code{FormCalc-$n$.$m$.tar.gz}.  To install it, create a directory for 
\FC\ and unpack the archive there, \eg
\bbox
\begin{alltt}
   gunzip -c FormCalc-\(n\).\(m\).tar.gz | tar xvf -
   cd FormCalc-\(n\).\(m\)
   ./compile
\end{alltt}
\ebox
The last line compiles the C programs that come with \FC.  The 
\Code{compile} script puts the binaries in a system-dependent directory, 
\eg \Code{Linux-x86-64}.  A single \FC\ installation can thus be 
NFS-mounted on different systems once \Code{compile} has been run on 
each.%
\index{compiling}


\section{Generating the Diagrams}

\index{diagram generation}%
\index{FeynArts@\FA}%
\FC\ calculates diagrams generated by \FA\ Version 3 or above.
Do not use the \FA\ function \Code{ToFA1Conventions} to convert the 
output of \Code{CreateFeynAmp} to the old conventions of \FA\ 1.

\FC\ can be loaded together with \FA\ into one \mma\ session.  The \FA\
results thus need not be saved in a file before calculating them with
\FC. 

\index{levels in diagrams}%
\index{generic amplitude}%
\FC\ can deal with any mixture of fully inserted diagrams and generic
diagrams with insertions.  The former are produced by \FA\ if only one
level is requested (via \Code{InsertionLevel}, \Code{AmplitudeLevel}, or
\Code{PickLevel}; see the \FA\ manual).  While both types of input
eventually produce the same results, it is important to understand that
it is the generic amplitude which is the most `costly' part to simplify,
so using the latter type of diagrams, where many insertions are derived
from one generic amplitude, can significantly speed up the calculation.

Usually it is also helpful to organize the diagrams into classes like
self-energies, vertex corrections and box corrections; it will also make
the amplitudes easier to handle since it reduces their size.

%------------------------------------------------------------------------

\section{Algebraically Simplifying Diagrams}

\subsection{\Code{CalcFeynAmp}}

\FA\ always produces purely symbolic amplitudes and refrains from 
simplifying them in any way so as not to be restricted to a certain 
class of theories.  The resulting expressions cannot be used directly 
\eg in a Fortran program, but must first be simplified algebraically.  
The function for this is \Code{CalcFeynAmp}.
\biitab
\Code{CalcFeynAmp[\Var{a_1},\,\Var{a_2},\,$\dots$]} &
	calculate the sum of amplitudes $\Var{a_1} + \Var{a_2} + \ldots$ \\
\etab%
\indextt{CalcFeynAmp}%
\Code{CalcFeynAmp} performs the following simplifications:
\index{index contractions}%
\index{fermion traces}%
\index{open fermion chains}%
\index{Dirac equation}%
\index{local terms}%
\index{tensor reduction}%
\begin{itemize}
\item	indices are contracted as far as possible,
\item	fermion traces are evaluated,
\item	open fermion chains are simplified using the Dirac equation,
\item	colour structures are simplified using the SU($N$) algebra,
\item	the tensor reduction is performed,
\item   local terms are added\footnote{%
		In $D$ dimensions, the divergent integrals are expanded
		in $\varepsilon = (4 - D)/2$ up to order $\varepsilon^0$
		and the $\frac 1\varepsilon$ poles are subtracted.  The
		$\frac 1\varepsilon$ poles give rise to local terms when
		multiplied with $D$'s from outside the integral
		(\eg from a $g^\mu{}_\mu$).
		In 4 dimensions, local terms are added depending on the
		contractions of indices of the tensor integrals according
		to the prescription of constrained differential
		renormalization \cite{dACTP98}.},
\item	the results are partially factored,
\item	abbreviations are introduced.
\end{itemize}
The output of \Code{CreateFeynAmp} can be fed directly into
\Code{CalcFeynAmp}.  Technically, this means that the arguments of
\Code{CalcFeynAmp} must be \Code{FeynAmpList} objects.
\Code{CalcFeynAmp} is invoked as
\begin{verbatim}
   amps = CreateFeynAmp[...];
   result = CalcFeynAmp[amps]
\end{verbatim}
The results are returned in the form
\begin{alltt}
   Amp[\Var{in} -> \Var{out}][\Var{r\sb{1}},\,\Var{r\sb{2}},\,\(\dots\)]
\end{alltt}
The lists \Var{in} and \Var{out} in the head of \Code{Amp} specify the 
the external particles to which the result belongs.  The presence of a 
particle's mass in the header does not imply that the amplitudes were 
calculated for on-shell particles.

The actual result is split into parts \Var{r_1}, \Var{r_2}, $\dots$, 
such that index sums (marked by \Code{SumOver}) always apply to the 
whole of each part.  It is possible to extend this splitting also to 
powers of coupling constants, such that part \Var{r_i} has a definite 
coupling order.  To this end one needs to wrap the coupling constants of 
interest in \Code{PowerOf}, for example
\begin{verbatim}
   CalcFeynAmp[amp /. g -> g PowerOf[g]]
\end{verbatim}
The function \Code{PowerOf} is there only to keep track of the coupling 
order and can be replaced by 1 at the end of the calculation.

The full result -- the sum of the parts -- can trivially be recovered by 
applying \Code{Plus} to the outcome of \Code{CalcFeynAmp}, \ie 
\Code{Plus@@\:CalcFeynAmp[amps]}.

\Code{CalcFeynAmp} has the following options:
\biiitab{option}
\Code{CalcLevel} & \Code{Automatic} &
	which level of the amplitude to select (\Code{Automatic} selects
	\Code{Particles} level, if available, otherwise \Code{Classes} 
	level) \\
\Code{Dimension} & \Code{D} &
	the space-time dimension in which the calculation is performed
	(\Code{D}, \Code{4}, or \Code{0}) \\
\Code{NoCostly} & \Code{False} &
	whether to turn off potentially time-consuming simplifications
	in FORM \\
\Code{FermionChains} & \Code{Weyl} &
	how to treat external fermion chains
	(\Code{Weyl}, \Code{Chiral}, or \Code{VA}) \\
\Code{FermionOrder} & \Code{Automatic} &
	the preferred ordering of external spinors in Dirac chains \\
\Code{Evanescent} & \Code{False} &
	whether to keep track of fermionic operators across Fierz
	transformations \\
\Code{InsertionPolicy} & \Code{Default} &
	how the level insertions are processed \\
\Code{SortDen} & \Code{True} &
	whether to sort the denominators of the loop integrals \\
\Code{PaVeReduce} & \Code{False} &
	whether to analytically reduce tensor to scalar integrals
\etab

\biiitab{option}
\Code{SimplifyQ2} & \Code{True} &
	whether to simplify $q^2$ in the numerator \\
\Code{OPP} & \Code{100} &
	the $N$ in $N$-point function above which OPP loop integrals
	are emitted \\
\Code{OPPQSlash} & \Code{False} &
	whether to introduce $\tilde\mu$ also on $\qslash$ \\
\Code{Gamma5Test} & \Code{False} &
	whether to substitute
	$\gamma_5\to\gamma_5 (1 + \Code{Gamma5Test} (D - 4))$ \\
\Code{Gamma5ToEps} & \Code{False} &
	whether to substitute
	$\gamma_5\to \frac 1{4!}\varepsilon_{\mu\nu\rho\sigma}
	\gamma^\mu \gamma^\nu \gamma^\rho \gamma^\sigma$ in fermion 
	traces \\
\Code{NoExpand} & \Code{\Brac{}} &
	sums containing any of the symbols in the list are not
	expanded \\
\Code{NoBracket} & \Code{\Brac{}} &
	symbols not to be included in the bracketing in FORM \\
\Code{MomRules} & \Code{\Brac{}} &
	extra rules for transforming momenta \\
\Code{PreFunction} & \Code{Identity} &
	a function applied to the amplitudes before any simplification \\
\Code{PostFunction} & \Code{Identity} &
	a function applied to the amplitudes after all simplifications \\
\Code{FileTag} & \Code{"amp"} &
	the middle part of the temporary \FO\ file's name \\
\Code{RetainFile} & \Code{False} &
	whether to retain the temporary \FO\ input file \\
\Code{EditCode} & \Code{False} &
	whether to display the \FO\ code in an editor before sending
	it to \FO
\etab%
\indextt{CalcLevel}%
\indextt{Dimension}%
\indextt{NoCostly}%
\indextt{FermionChains}%
\indextt{FermionOrder}%
\indextt{Evanescent}%
\indextt{InsertionPolicy}%
\indextt{SortDen}%
\indextt{PaVeReduce}%
\indextt{SimplifyQ2}%
\indextt{OPP}%
\indextt{OPPQSlash}%
\indextt{Gamma5Test}%
\indextt{Gamma5ToEps}%
\indextt{NoExpand}%
\indextt{NoBracket}%
\indextt{MomRules}%
\indextt{PreFunction}%
\indextt{PostFunction}%
\indextt{FileTag}%
\indextt{RetainFile}%
\indextt{EditCode}

\indextt{Classes}%
\indextt{Particles}%
\indextt{Automatic}%
\index{levels in diagrams}%
\Code{CalcLevel} is used to select the desired level in the calculation.
In general a diagram can have both \Code{Classes} and \Code{Particles}
insertions.  The default value \Code{Automatic} selects the deepest
level available, \ie \Code{Particles} level, if available, otherwise
\Code{Classes} level.

\index{regularization method}%
\index{dimensional!regularization}%
\index{dimensional!reduction}%
\index{constrained differential\\renormalization}%
\index{UV divergences}%
\Code{Dimension} specifies the space-time dimension in which to perform
the calculation.  It can take the values \Code{D} and \Code{4}.  This is a
question of how UV-divergent expressions are treated.  The essential 
points of both methods are outlined in the following.  For a more thorough
discussion see \cite{HaP98}.
\begin{itemize}
\item	\Code{Dimension -> D} corresponds to \textbf{dimensional
	regularization} \cite{tHV72}.  Dimensionally regularizing an
	expression involves actually two things: analytic continuation of
	the momenta (and other four-vectors) in the number of dimensions,
	$D$, and an extension to $D$ dimensions of the Lorentz covariants
	($\gamma_\mu$, $g_{\mu\nu}$, etc.).  The second part is achieved
	by treating the covariants as formal objects obeying certain
	algebraic relations.  Problems only appear for identities that
	depend on the 4-dimensional nature of the objects involved.  In
	particular, the extension of $\gamma_5$ to $D$ dimensions is
	problematic.  \FC\ employs a naive scheme \cite{ChFH79} and works
	with an anticommuting $\gamma_5$ in all	dimensions.

\item	\Code{Dimension -> 4} selects \textbf{constrained differential 
	renormalization} (CDR) \cite{dACTP98}.  This technique cures UV 
	divergences by substituting badly-behaved expressions by 
	derivatives of well-behaved ones in coordinate space.  The 
	regularized coordinate-space expressions are then 
	Fourier-transformed back to momentum space.  CDR works completely 
	in 4 dimensions.  At one-loop level it has been shown \cite{HaP98} 
	to be equivalent to regularization by \textbf{dimensional reduction} 
	\cite{Si79}, which is a modified version of dimensional 
	regularization: while the integration momenta are still 
	$D$-dimensional as in dimensional regularization, all other 
	tensors and spinors are kept 4-dimensional.  Although the results 
	are the same, it should be stressed that the conceptual approach 
	in CDR is quite different from dimensional reduction.

\item	\Code{Dimension -> 0} keeps the whole amplitude $D$-dimensional.
	No rational terms are added and the $D$-dependency is expressed
	through \Code{Dminus4}.
\end{itemize}%
\indextt{Dminus4}

\Code{NoCostly} switches off simplifications in the FORM code which are 
typically fast but can cause `endless' computations on certain 
amplitudes.

\indextt{Weyl}%
\indextt{Chiral}%
\indextt{VA}%
\indextt{HelicityME}%
\index{fermion chain}%
\index{spinor chain}%
\Code{FermionChains} determines how fermion chains are returned. 
\Code{Weyl}, the default, selects Weyl chains.  \Code{Chiral} and
\Code{VA} select Dirac chains in the chiral ($P_L/P_R$) and
vector/axial-vector ($\unity/\gamma_5$) decomposition, respectively. 
Note that to fully evaluate amplitudes containing Dirac chains,
helicity matrix elements must be computed with \Code{HelicityME}.  For
more details on the conceptual treatment of external fermions in \FC, 
see \cite{Ha02, Ha04a}.

\index{ordering}%
\index{external spinors}%
\index{Fierz transformation}%

The \Code{FermionOrder} option means different things for Dirac and for 
Weyl chains:

For Dirac spinor chains (\Code{FermionChains -> Chiral} or \Code{VA}) 
\Code{FermionOrder} determines the ordering of the external fermions 
within the chains.  Choices are \Code{None}, \Code{Fierz}, 
\Code{Automatic}, \Code{Colour}, or an explicit ordering, \eg 
\Code{\Brac{2,\,1,\,4,\,3}} (corresponding to fermion chains of the form 
$\bra{2}\Gamma\ket{1} \bra{4}\Gamma'\ket{3}$).  \Code{None} applies no 
reordering.  \Code{Fierz} applies the Fierz identities \cite{Ni05} 
twice, thus simplifying the chains but keeping the original order.  
\Code{Colour} applies the ordering of the external colour indices (after 
simplification) to the spinors. \Code{Automatic} chooses a 
lexicographical ordering (small numbers before large numbers).

For Weyl spinor chains (\Code{FermionChains -> Weyl}) 
\Code{FermionOrder} determines the structuring of the amplitude with 
respect to the fermion chains: Setting \Code{FermionOrder -> Mat} wraps 
the Weyl fermion chains in \Code{Mat}, like their Dirac counterpart, so 
that they end up at the outermost level of the amplitude and their 
coefficients can be read off easily.  The matrix elements of the form
\Code{Mat[F\Vi,\,F\Vj]} are computed with the \Code{WeylME} function.

The \Code{Evanescent} option toggles whether fermionic operators are 
tracked across Fierz transformations by emitting terms of the form 
\Code{Evanescent[\Var{original\,operator},\,\Var{Fierzed\,operator}]}.

\index{insertions}%
\index{kinematical simplification}%
\index{loop integral!symmetrized}%
\Code{InsertionPolicy} specifies how the level insertions are applied
and can take the values \Code{Begin}, \Code{Default}, or an integer. 
\Code{Begin} applies the insertions at the beginning of the FORM code
(this ensures that all loop integrals are fully symmetrized).  
\Code{Default} applies them after simplifying the generic amplitudes
(this is fastest).  An integer does the same, except that insertions
with a \Code{LeafCount} larger than that integer are inserted only 
after the amplitude comes back from FORM (this is a workaround for
the rare cases where the FORM code aborts due to very long insertions).

\Code{SortDen} determines whether the denominators of loop integrals
shall be sorted.  This is usually done to reduce the number of loop
integrals appearing in an amplitude.  Sorting may be turned off for
testing and in few cases may even lead to shorter amplitudes.

\Code{PaVeReduce} governs the tensor reduction.  \Code{False} retains
the one-loop tensor-coefficient functions.  \Code{Raw} reduces them
to scalar integrals but keeps the Gram determinants in the denominator
in terms of dot products.  \Code{True} simplifies the Gram determinants
using invariants.

\Code{SimplifyQ2} controls simplification of terms involving the
integration momentum $q$ squared.  If set to True, powers of $q^2$ in 
the numerator are cancelled by a denominator, except for OPP integrals,
where conversely lower-$N$ integrals are put on a common denominator
with higher-$N$ integrals to reduce OPP calls, as in:
$N_2/(D_0 D_1) + N_3/(D_0 D_1 D_2) \to (N_2 D_2 + N_3)/(D_0 D_1 D_2)$.

\index{rational terms}%
\index{OPP}%
\index{CutTools}%
\index{Samurai}%
\index{tensor coefficients}%
\index{numerator function}%
OPP specifies an integer $N$ starting from which an $N$-point function is 
treated with OPP methods.  For example, \Code{OPP -> 4} means that $A$, 
$B$, $C$ functions are reduced with Passarino--Veltman and $D$ and up with 
OPP.  A negative $N$ indicates that the rational terms for the OPP 
integrals shall be added analytically whereas otherwise their computation 
is left to the OPP package (CutTools or Samurai).

The integration momentum $q$ starts life as a $D$-dimensional object.  
In OPP, any $q^2$ surviving \Code{SimplifyQ2} is substituted by $q^2 - 
\tilde\mu^2$, after which $q$ is considered 4-dimensional.  The 
dimensionful scale $\tilde\mu$ enables the OPP libraries to reconstruct 
the $R_2$ rational terms.  \Code{OPPQSlash} extends this treatment to 
the $\qslash$ on external fermion chains, \ie also substitutes 
$\qslash\to \qslash + \ri\gamma_5\tilde\mu$, where odd powers of 
$\tilde\mu$ are eventually set to zero.

\index{test, $\gamma_5$}
\Code{Gamma5Test -> True} substitutes each $\gamma_5$ by
$\gamma_5 (1 + \Code{Gamma5Test} (D - 4))$ and it can be tested whether 
the final result depends on the variable \Code{Gamma5Test} (which it 
shouldn't).

\index{'t Hooft--Veltman scheme}%
\index{Breitenlohner--Maison scheme}%
\Code{Gamma5ToEps -> True} substitutes all $\gamma_5$ in fermion 
traces by $\frac 1{4!} \varepsilon_{\mu\nu\rho\sigma} \gamma^\mu 
\gamma^\nu \gamma^\rho \gamma^\sigma$.  This effectively implements the 
't~Hooft--Veltman--Breitenlohner--Maison $\gamma_5$-scheme.  External 
fermion chains are intentionally exempt since at least the Weyl 
formalism needs chiral chains.  Take care that due to the larger number 
of Lorentz indices the computation time may increase significantly.

\index{expansion}%
\index{non-expansion of terms}%
\Code{NoExpand} prohibits the expansion of sums containing certain symbols.
In certain cases, expressions can become unnecessarily bloated if all
terms are fully expanded, as \FO\ always does.  For example, if gauge
eigenstates are rotated into mass eigenstates, the couplings typically
contain linear combinations of the form $U_{i1} c_1 + U_{i2} c_2$.  It is
not difficult to see that the number of terms generated by the full
expansion of such couplings can be considerable, in particular if several
of them appear in a diagram.  \Code{NoExpand} turns off the automatic
expansion, in this example one would select \Code{NoExpand -> $U$}.

\Code{NoBracket} prevents the given symbols to be included in the
internal `multiplication brackets' in FORM.  This bracketing is done for
performance but prevents the symbols from partaking in further evaluation.

\Code{MomRules} specifies a set of rules for transforming momenta.  The 
notation is that of the final amplitude, \ie \Code{k1}, \dots, 
\Code{k\Vn} for the momenta, \Code{e1}, \dots, \Code{e\Vn} for the 
polarization vectors.

\Code{PreFunction} and \Code{PostFunction} specify functions to be 
applied to the amplitude before and after all simplifcations have been 
made.  These options are typically used to apply a function to all 
amplitudes in a calculation, even in indirect calls to 
\Code{CalcFeynAmp}, such as through \Code{CalcRenConst}.

\label{retainedit}%
\index{debugging options}%
\Code{RetainFile} and \Code{EditCode} are options used for debugging
purposes.  The temporary file to which the input for \FO\ is written is
not removed when \Code{RetainFile -> True} is set.  The name of this file
is typically something like \Code{fc-amp-1.frm}.  The middle part,
\Code{amp}, can be chosen with the \Code{FileTag} option, to
disambiguate files from different \Code{CalcFeynAmp} calls.  
\Code{EditCode} is more comfortable in that it places the temporary file 
in an editor window before sending it to \FO.  The command line for the 
editor is specified in the variable \Code{\$Editor}.  
\Code{EditCode -> Modal} invokes the \Code{\$EditorModal} command, which 
is supposed to be modal (non-detached), \ie continues only after the 
editor is closed, thus continuing with possibly modified FORM code.%
\indextt{\$Editor}
\indextt{\$EditorModal}

\indextt{ReadFormDebug}%
In truly obnoxious cases the function \Code{ReadFormDebug[\Var{bits}]} 
can be used to enable debugging output on stderr for subsequent 
\Code{CalcFeynAmp} calls according to the \Var{bits} bit pattern.  
\Code{ReadFormDebug[\Var{bits},\,\Var{file}]} writes the output to 
\Var{file} instead.  For currently defined bit patterns and their 
meaning please see the header of \Code{ReadForm.tm}.

\biitab
\Code{FormPre[\Var{amp}]} &
	a function to set up the following \Code{Form*}
	simplification functions, \Var{amp} is the raw amplitude \\
\Code{FormSub[\Var{subexpr}]} &
	a function applied to subexpressions extracted by FORM \\
\Code{FormDot[\Var{dotprods}]} &
	a function applied to combinations of dot products by FORM \\
\Code{FormMat[\Var{matcoeff}]} &
	a function applied to the coefficients of matrix elements
	(\Code{Mat[\dots]}) in the FORM output \\
\Code{FormNum[\Var{numfunc}]} &
	a function applied to numerator functions in the FORM
	output (OPP only) \\
\Code{FormQC[\Var{qcoeff}]} &
	a function applied to loop-momentum-independent parts of
	the OPP numerator in the FORM output \\
\Code{FormQF[\Var{qfunc}]} &
	a function applied to loop-momentum-dependent parts of
	the OPP numerator in the FORM output \\
\Code{\$FormAbbrDepth} &
	minimum depth an expression has to have to be abbreviated
\etab%
\indextt{FormPre}%
\indextt{FormSub}%
\indextt{FormDot}%
\indextt{FormMat}%
\indextt{FormQC}%
\indextt{FormQF}%
\indextt{\$FormAbbrDepth}%
\Code{CalcFeynAmp} wraps the above functions around various parts of the 
FORM output for simplification upon return to Mathematica.  These are 
typically relatively short expressions which can be simplified efficiently 
in Mathematica.  The default settings try to balance execution time 
against simplification efficiency.  Occasionally, though, Mathematica 
will spend excessive time on simplification and in this case one or 
several of the above should be redefined, \eg set to \Code{Identity}.  
Alternately, one can use \FC's \Code{Profile} function to narrow down
performance problems, as in: \Code{FormMat = Profile[FormMat]}.%
\indextt{Profile}


\subsection{\Code{DeclareProcess}}

For the calculation of an amplitude, many definitions have to be set up 
internally.  This happens in the \Code{DeclareProcess} function.  
\biitab
\Code{DeclareProcess[\Var{a_1},\,\Var{a_2},\,$\dots$]} &
	set up internal definitions for the calculation of the
	amplitudes \Var{a_1}, \Var{a_2}, \dots \\
\etab%
\indextt{DeclareProcess}%
Usually it is not necessary to invoke this function explicitly, as
\Code{CalcFeynAmp} does so.  All \Code{DeclareProcess} options can
be specified with \Code{CalcFeynAmp} and are passed on.

Functions that need the internal definitions set up by
\Code{DeclareProcess} are \Code{CalcFeynAmp}, \Code{HelicityME},
and \Code{PolarizationSum}.  Invoking \Code{DeclareProcess} directly
could be useful \eg if one needs to change the options between calls
to \Code{CalcFeynAmp} and \Code{HelicityME}, or if one wants to call
\Code{HelicityME} in a new session without a previous
\Code{CalcFeynAmp}.%
\indextt{CalcFeynAmp}%
\indextt{HelicityME}%
\indextt{PolarizationSum}

The output of \Code{DeclareProcess} is a \Code{FormAmp} object.
\biitab
\Code{FormAmp[\Var{proc}][\Var{amps}]} &
	a preprocessed form of the \FA\ amplitudes \Var{amps}
	for process \Var{proc}
\etab%
\indextt{FormAmp}

\Code{DeclareProcess} has the following options.  They are also accepted by 
\Code{CalcFeynAmp} in direct invocations (and passed on to \Code{DeclareProcess}) 
but cannot be set permanently using \Code{SetOptions[CalcFeynAmp,\,\dots]} as as 
they are not \Code{CalcFeynAmp} options.
\biiitab{option}
\Code{OnShell} & \Code{True} &
	whether the external momenta are on shell,
	\ie $k_i^2 = m_i^2$ \\
\Code{Invariants} & \Code{True} &
	whether to introduce kinematical invariants like the
	Mandelstam variables \\
\Code{Transverse} & \Code{True} &
	whether polarization vectors should be orthogonal
	to the corresponding momentum,
	\ie $\varepsilon_i^\mu k_{i,\mu} = 0$ \\
\Code{Normalized} & \Code{True} &
	whether the polarization vectors should be assumed normalized,
	\ie $\varepsilon_i^\mu \varepsilon_{i,\mu}^* = -1$ \\
\Code{InvSimplify} & \Code{True} &
	whether to simplify combinations of invariants as much as
	possible \\
\Code{MomElim} & \Code{Automatic} &
	how to apply momentum conservation \\
\Code{DotExpand} & \Code{False} &
	whether to expand terms collected for momentum elimination \\
\Code{Antisymmetrize} & \Code{True} &
	whether Dirac chains shall be antisymmetrized
\etab%
\indextt{OnShell}%
\indextt{Invariants}%
\indextt{Transverse}%
\indextt{Normalized}%
\indextt{InvSimplify}%
\indextt{MomElim}%
\indextt{DotExpand}%
\indextt{Antisymmetrize}%

\index{mass shell}%
\index{on-shell particles}%
\indextt{ExceptDirac}%
\Code{OnShell} determines whether the external particles are on their mass
shell, \ie it sets $k_i^2 = m_i^2$ where $k_i$ is the $i$th external
momentum and $m_i$ the corresponding mass.  The special value 
\Code{ExceptDirac} works like \Code{True} except that the Dirac equation 
is not applied to on-shell momenta, \ie $\kslash_i$ inside fermion
chains are not substituted by $\pm m_i$.

\index{invariants}%
\index{Mandelstam variables}%
\Code{Invariants -> True} instructs \Code{CalcFeynAmp} to introduce
kinematical invariants.  In the case of a $2\to 2$ process, these are the
familiar Mandelstam variables.

\index{transversality}%
\index{polarization vector}%
\Code{Transverse -> True} enforces orthogonality of the polarization
vectors of external vector bosons and their corresponding momentum, \ie it
sets $\varepsilon_{i,\mu} k_i^\mu = 0$ where $k_i$ and $\varepsilon_i$ are
the $i$th external momentum and polarization vector, respectively.

\index{normalization}%
\Code{Normalized -> True} allows \Code{CalcFeynAmp} to exploit the 
normalization of the polarization vectors, \ie set $\varepsilon_i^\mu
\varepsilon_{i,\mu}^* = -1$.

The last four options can concisely be summarized as
\begin{center}
\begin{tabular}{l|l}
Option & Action \\ \hline
\Code{OnShell -> True} & $k_i\cdot k_i = m_i^2$ \\
\Code{Mandelstam -> True} & $k_i\cdot k_j =
  \pm\frac 12 \bigl[(s|t)_{ij} - m_i^2 - m_j^2\bigr]$ \\
\Code{Transverse -> True} & $\varepsilon_i\cdot k_i = 0$ \\
\Code{Normalized -> True} & $\varepsilon_i\cdot\varepsilon_i^* = -1$
\end{tabular}
\end{center}

\index{invariants!simplification}%
\Code{InvSimplify} controls whether \Code{CalcFeynAmp} should try to 
simplify combinations of invariants as much as possible.

\index{momentum conservation}%
\index{cancellation of terms}%
\Code{MomElim} controls in which way momentum conservation is used to
eliminate momenta.  \Code{False} performs no elimination, an integer
between 1 and the number of legs eliminates the specified momentum, and 
\Code{Automatic} tries all substitutions and chooses the one resulting 
in the fewest terms.

\Code{DotExpand} determines whether the terms collected for momentum 
elimination are expanded again.  This prevents kinematical invariants
from appearing in the abbreviations but typically leads to poorer
simplification of the amplitude.

\index{antisymmetrization}%
\Code{Antisymmetrize} determines whether to antisymmetrize Dirac chains.  
This does not affect Weyl chains, \ie has an effect only together with
\Code{FermionChains\,->\,Chiral} or \Code{VA}.  Antisymmetrized chains
carry a negative chirality identifier, \eg 
\Code{DiracChain[-6,\,$\mu$,\,$\nu$]} stands for
$P_R\,\frac 12 (\gamma_\mu\gamma_\nu - \gamma_\nu\gamma_\mu)$.

%------------------------------------------------------------------------

\subsection{Clearing, Combining, Selecting}

\Code{CalcFeynAmp} needs no declarations of the kinematics of the
underlying process; it uses the information \FA\ hands down.  This is
convenient, but it also requires certain care on the side of the user
because of the abbreviations \FC\ automatically introduces in the result
(see Sect.\ \ref{sect:abbr}).  Owing to the presence of momenta and
polarization vectors, abbreviations introduced for different processes
will in general have different values, even if they have the same
analytical form.  To ensure that processes with different kinematics 
cannot be mixed accidentally, \Code{CalcFeynAmp} refuses to calculate 
amplitudes belonging to a process whose kinematics differ from those of 
the last calculation unless \Code{ClearProcess[]} is invoked in between.
\biitab
\Code{ClearProcess[]} &
	removes internal definitions before calculating a new process
\etab%
\indextt{ClearProcess}%
\index{internal definitions}%
\index{kinematics}

\bigskip

The \Code{Combine} function combines amplitudes.  It works before and
after \Code{CalcFeynAmp}, \ie on either \Code{FeynAmpList} or \Code{Amp}
objects.  When trying to combine amplitudes from different processes, 
\Code{Combine} issues a warning only, but does not refuse to work as 
\Code{CalcFeynAmp}.
\biitab
\Code{Combine[\Var{amp_1},\,\Var{amp_2},\,$\dots$]} &
	combines \Var{amp_1}, \Var{amp_2}, $\dots$
\etab%
\indextt{Combine}

\bigskip

The following two functions are helpful to select diagrams.
\biitab
\Code{FermionicQ[\Var{d}]} &
	\Code{True} if diagram \Var{d} contains fermions \\
\Code{DiagramType[\Var{d}]} &
	returns the number of propagators in diagram \Var{d}
	not belonging to the loop
\etab
\index{selecting diagrams}%
\indextt{FermionicQ}%
\indextt{DiagramType}%
\Code{FermionicQ} is used for selecting diagrams that contain fermions, \ie
\begin{verbatim}
   ferm = CalcFeynAmp[ Select[amps, FermionicQ] ]
\end{verbatim}
\Code{DiagramType} returns the number of propagators not containing the
integration momentum.  To see how this classifies diagrams, imagine a
$2\to 2$ process without self-energy insertions on external legs (\ie in
an on-shell renormalization scheme).  There, \Code{DiagramType} gives 2 for
a self-energy diagram, 1 for a vertex-correction diagram, and 0 for a box
diagram, so that for instance
\begin{verbatim}
   vert = CalcFeynAmp[ Select[amps, DiagramType[#] == 1 &] ]
\end{verbatim}
calculates all vertex corrections.  \Code{DiagramType} is of course only a
very crude way of classifying diagrams and not nearly as powerful as the
options available in \FA, like \Code{ExcludeTopologies}.

\bigskip

Individual legs can be taken off-shell with the function \Code{OffShell}.
\biitab
\Code{OffShell[\Var{amp},\,\Vi\ -> $\mu_i$,\,$\dots$]} &
	enforce the relation $p_i^2 = \mu_i^2$ on the amplitudes
	\Var{amp}.
\etab
\indextt{OffShell}%
\Code{OffShell[\Var{amp},\,\Vi\ -> $\mu_i$,\,\Vj\ -> $\mu_j$,\,
$\dots$]} takes legs \Vi, \Vj, \dots\ off-shell by substituting the
true on-shell relation $p_i^2 = m_i^2$ by $p_i^2 = \mu_i^2$.  This is
different from setting the \Code{CalcFeynAmp} option \Code{OnShell ->
False} which takes all legs off-shell by not using $p_i^2 = m_i^2$ at all.

\bigskip

Finally, the following two functions serve to add factors to particular 
diagrams.
\biitab
\Code{MultiplyDiagrams[\Var{f}][\Var{amps}]} &
	multiply the diagrams in \Var{amps} with the factor returned
	by the function \Var{f} \\
\Code{TagDiagrams[\Var{amp}]} &
	multiply each diagram in \Var{amp} with the identifier
	\Code{Diagram[\Var{n}]}, where \Var{n} is the diagram's
	number
\etab
\indextt{MultiplyDiagrams}%
\indextt{TagDiagrams}%
\Code{MultiplyDiagrams[\Var{f}][\Var{amp}]} multiplies the diagrams in
\Var{amp} with factors depending on their contents.  The factor is
determined by the function \Var{f} which is applied to each diagram
either as \Var{f[\Var{amplitude}]}, for fully inserted diagrams, or
\Var{f[\Var{generic\ amplitude}, \Var{insertion}]}.  For example, to add
a QCD enhancement factor to all diagrams containing a quark mass, the
following function could be used as 
\Code{MultiplyDiagrams[QCDfactor][\Var{amps}]}:
\begin{verbatim}
  QCDfactor[args__] := QCDenh /; !FreeQ[{args}, Mf[3|4, __]]
  _QCDfactor = 1
\end{verbatim}

\Code{TagDiagrams} is a special case of \Code{MultiplyDiagrams} and 
multiplies each diagram with an identifier of the form 
\Code{Diagram[\Var{n}]}, where \Var{n} is the diagram's running number.  
This provides is very simple mechanism to identify the origin of terms 
in the final amplitude.

%------------------------------------------------------------------------

\subsection{Ingredients of Feynman amplitudes}

The results of \Code{CalcFeynAmp} contain the following symbols:

Momenta and polarization vectors are consecutively numbered, \ie the
incoming momenta are numbered \Code{k[1]}$\dots$\Code{k[\Var{\rsub 
n{in}}]} and the outgoing momenta \Code{k[\Var{\rsub n{in}} + 
1]}$\dots$\Code{k[\Var{\rsub n{in}} + \Var{\rsub n{out}}]}.%
\index{numbering of momenta}%
\biitab
\Code{k[\Vn]} &
	\Vn th external momentum \\
\Code{e[\Vn]}, \Code{ec[\Vn]} &
	\Vn th polarization vector and its conjugate \\
\Code{eT[\Vn]}, \Code{eTc[\Vn]} &
	\Vn th polarization tensor and its conjugate \\
\Code{Pair[\Vp,\,\Vq]} &
	scalar product of the four-vectors \Vp\ and \Vq \\
\Code{Eps[\Vp,\,\Vq,\,\Vr,\,\Vs]} &
	$-\ri\varepsilon_{\mu\nu\rho\sigma} \Vp^\mu \Vq^\nu
	\Vr^\rho \Vs^\sigma$ where $\varepsilon$ is the
	totally antisymmetric Levi-Civita tensor in 4 dimensions
	with sign convention $\varepsilon^{0123} = +1$ \\
\Code{Den[\Var{k^2},\,\Var{m^2}]} &
	the denominator $1/(\Var{k^2} - \Var{m^2})$ \\
\Code{Delta[\Vi,\,\Vj]} &
	the Kronecker delta $\delta_{\Vi\Vj}$ \\
\Code{IGram[\Vd]} &
	the denominator arising from the reduction of a
	tensor integral, equivalent to $1/\Vd$ \\
\Code{Finite} &
	a symbol multiplied with the local terms resulting from
	$D\cdot\textit{(divergent~integral)}$ \\
\etab
\indextt{k[\Vn]}%
\indextt{e[\Vn]}%
\indextt{s[\Vn]}%
\indextt{Pair}%
\indextt{Eps}%
\indextt{Den}%
\indextt{IGram}%
\indextt{Finite}%
\index{Levi-Civita tensor}%
\index{dot product}%
\index{momenta}%
\index{polarization vector}%
\index{kinematics}%
\index{denominator}%
\index{propagator denominator}%
\index{Gram determinant}%
\index{inverse Gram}%
Note the extra factor $-\ri$ in the definition of \Code{Eps} which
is included to reduce the number of explicit $\ri$'s in the final result.

About the use of \Code{Finite}: Whenever a divergent loop integral is 
multiplied by $D$ (coming \eg as $g_\mu^\mu$ from a self-contracted 
coupling), local terms arise because the $\varepsilon$ in $D = 4 - 
2\varepsilon$ cancels the $1/\varepsilon$-divergence of the integral, 
schematically:
$$
D\cdot\text{(loop integral)} = 4\cdot\text{(loop integral)} + 
\text{(local term)}\,.
$$
`Local' refers to the fact that these terms contain no loop integral 
anymore.

In dimensional regularization, a popular way of checking finiteness 
of an amplitude is to substitute the loop integrals by their divergent 
parts and test whether the coefficients of $\varepsilon^{-1}$ and 
$\varepsilon^{-2}$ work out to zero.  In \LT, for example, this is 
effected by setting $\Code{LTLAMBDA} = -1$ and $-2$, respectively.

The local terms would quite obviously spoil this cancellation and must 
be set to zero during the check.  To make this possible, 
\Code{CalcFeynAmp} multiplies the local terms it generates with the 
symbol \Code{Finite}, such that \Code{Finite} = 1 normally but 
\Code{Finite} = 0 when checking finiteness.

\biitab
\Code{S} &
	Mandelstam variable $s = (\Code{k[1]} + \Code{k[2]})^2$ \\
\Code{T} &
	Mandelstam variable $t = (\Code{k[1]} - \Code{k[3]})^2$ \\
\Code{U} &
	Mandelstam variable $u = (\Code{k[2]} - \Code{k[3]})^2$ \\
\Code{S\Vi\Vj} &
	invariant of the invariant-mass type,
	$\Code{S\Vi\Vj} = (\Code{k[\Vi]} + \Code{k[\Vj]})^2$ \\
\Code{T\Vi\Vj} &
	invariant of the momentum-transfer type,
	$\Code{T\Vi\Vj} = (\Code{k[\Vi]} - \Code{k[\Vj]})^2$
\etab%
\index{Mandelstam variables}%
\indextt{S}%
\indextt{T}%
\indextt{U}%
\index{Sij@S\Vi\Vj}%
\index{Tij@T\Vi\Vj}

SU($N$) structures are always simplified so that only chains of generators 
(\Code{SUNT}) remain.
\biitab
\Code{SUNT[\Va,\,\Vb,\,$\dots$,\,\Vi,\,\Vj]} &
	the product $(T^\Va T^\Vb\cdots)_{\Vi\Vj}$ of 
	SU($N$)-generators, where \Va, \Vb, $\dots$ are gluon 
	indices (1$\dots$ 8) and \Vi\ and \Vj\ are colour indices
	(1$\dots$ 3); note that \Code{SUNT[\Vi,\,\Vj]} represents the 
	identity $\delta_{\Vi\Vj}$ in colour space \\
\Code{SUNT[\Va,\,\Vb,\,$\dots$,\,0,\,0]} &
	the trace $\Tr(T^\Va T^\Vb\cdots)$
\etab
\indextt{SUNT}%
\index{generators of SU($N$)}%
\index{colour indices}%
\index{SU($N$) objects}%
\indextt{SUNN}%
The $N$ in SU($N$) is specified with the variable
\Code{SUNN}.
\biiitab{variable}
\Code{SUNN} & 3 & the $N$ in SU($N$)
\etab

Fermionic structures like spinor chains are returned as a
\Code{DiracChain} or \Code{WeylChain} object.  Lorentz indices 
connecting two \Code{DiracChain}s are denoted by \Code{Lor[\Vn]}, all
other Lorentz indices are implicit, \eg in the contraction with a 
vector.

\biitab
\Code{DiracChain} &
	the antisymmetrized product of Dirac matrices. \\
&	The actual Dirac matrix with which an argument is contracted is 
	not written out, \eg \Code{k[1]} stands for 
	$\gamma_\mu\,\Code{k[1]}^\mu$ inside a \Code{DiracChain}. \\
\textit{inside a }\Code{DiracChain}: \\
\Code{1} &
	$\unity$ \\
\Code{5} &
	$\gamma_5$ \\
\Code{6} &
	the chirality projector $P_R = \omega_+ = (\unity + \gamma_5)/2$ \\
\Code{7} &
	the chirality projector $P_L = \omega_- = (\unity - \gamma_5)/2$ \\
\Code{-\Vi} &
	antisymmetrized chain, $\Vi = 1, 5, 6, 7$ as above \\
\Code{Lor[\Vn]} &
	a Lorentz index connecting two \Code{DiracChain}s \\
\Code{Spinor[\Vp,\,\Vm,\,1]} &
	particle spinor $u(\Vp)$ for which
	$(\pslash - m) u(\Vp) = 0$ \\
\Code{Spinor[\Vp,\,\Vm,\,-1]} &
	antiparticle spinor $v(\Vp)$ for which
	$(\pslash + m) v(\Vp) = 0$
\etab%
\indextt{DiracChain}%
\indextt{Spinor}%
\indextt{Lor[\Vn]}%
\index{Dirac matrix}%
\index{Lorentz index}%
\index{chirality projector}

It should be especially noted that \Code{DiracChain}s, unlike
\Code{WeylChain}s, are antisymmetrized starting from \FC\ Version 6.  
Antisymmetrized chains are typically more convenient for analytical
purposes, for example there is the correspondence
$\Code{DiracChain[-1,$\mu$,$\nu$]} = \sigma_{\mu\nu}$.  Note that the 
antisymmetrization does not extend to the chirality projector
or $\gamma_5$, \eg
$$
\Code{DiracChain[-6,$\mu$,$\nu$,$\rho$]} = \frac{P_R}{3!} 
(\gamma_\mu\gamma_\nu\gamma_\rho 
- \gamma_\mu\gamma_\rho\gamma_\nu 
- \gamma_\nu\gamma_\mu\gamma_\rho 
+ \gamma_\nu\gamma_\rho\gamma_\mu
+ \gamma_\rho\gamma_\mu\gamma_\nu
- \gamma_\rho\gamma_\nu\gamma_\mu)\,.
$$

Weyl chains are quite similar in notation to the Dirac chains.  The
constituents of a \Code{WeylChain} always alternate between upper
($\sigma^{\dot AB} = \sigma$) and lower indices ($\sigma_{A\dot B} =
\bar\sigma$).  The arguments \Code{6} and \Code{7} hence fix the index
positions for the entire chain.
\biitab
\Code{WeylChain} &
	the product of sigma matrices. \\
&	The actual sigma matrix with which an argument is contracted is 
	not written out, \eg \Code{k[1]} stands for 
	$\sigma_\mu\Code{k[1]}^\mu$ inside a \Code{WeylChain}. \\
\textit{inside a }\Code{WeylChain}: \\
\Code{6} &
	signifies that the following sigma matrix has upper spinor
	indices \\
\Code{7} &
	signifies that the following sigma matrix has lower spinor
	indices \\
\Code{Spinor[\Vp,\,\Vm,\,\Vs,\,\Vd,\,\Ve]} &
	a 2-spinor, with $\Vd = 1(2)$ undotted (dotted) and
	$\Ve = 0(1)$ uncontracted (contracted) with $\varepsilon =
	\left(\begin{smallmatrix}
        0 & 1 \\ -1 & 0
	\end{smallmatrix}\right)$
\etab%
\indextt{WeylChain}%
\indextt{Spinor}%
\index{spinor metric}

\FC\ also introduces some model-dependent symbols.  It can be argued that
this is not a good practice, but the advantage of doing so is just too
great to ignore as it can speed up calculations by as much as 15\%.  This
is because \eg \Code{MW2} is an ordinary symbol while \Code{MW\power 2} is
a non-atomic expression (the internal representation is
\Code{Power[MW,\,2]}).

Setting \Code{\$NoModelSpecific = True} before loading FormCalc inhibits 
setting of the model-dependent symbols.
To change or add to these definitions, edit 
\Code{FormCalc/ModelSpecific.m}.  Currently, the following 
model-dependent symbols are defined:
\biitab
\textit{for the Standard Model:} \\
\Code{Alfa}, \Code{Alfa2} &
	the fine-structure constant $\alpha = \frac{e^2}{4\pi}$ and its
	square \\
\Code{Alfas}, \Code{Alfas2} &
	the ``strong fine-structure constant'' $\alpha_s =
	\frac{g_s^2}{4\pi}$ and its square (the spelling with an f was
	chosen so as not to collide with the CERNlib function
	\Code{ALPHAS2}) \\
\Code{CW2}, \Code{SW2} &
	$\cos^2\rsub{\theta}{W}$ and $\sin^2\rsub{\theta}{W}$ \\
\Code{MW2}, \Code{MZ2}, \Code{MH2} \linebreak
\Code{MLE2}, \Code{ME2}, \Code{MM2}, \Code{ML2} \linebreak
\Code{MQU2}, \Code{MU2}, \Code{MC2}, \Code{MT2} \linebreak
\Code{MQD2}, \Code{MD2}, \Code{MS2}, \Code{MB2} &
	the squares of various masses \\
\Code{SMSimplify[\Var{expr}]} &
	\Code{Simplify} with \Code{SW2 -> 1\,-\,CW2} and
	\Code{CW2 -> MW2/MZ2}
\etab
\index{model parameters}%
\indextt{Alfa}%
\indextt{Alfa2}%
\indextt{Alfas}%
\indextt{Alfas2}%
\indextt{CW2}%
\indextt{SW2}%
\indextt{MW2}%
\indextt{MZ2}%
\indextt{MH2}%
\indextt{MLE2}%
\indextt{ME2}%
\indextt{MM2}%
\indextt{ML2}%
\indextt{MQU2}%
\indextt{MU2}%
\indextt{MC2}%
\indextt{MT2}%
\indextt{MQD2}%
\indextt{MD2}%
\indextt{MS2}%
\indextt{MB2}%
\indextt{SMSimplify}%
\biitab
\textit{for the MSSM:} \\
\Code{CA2}, \Code{SA2},
\Code{CB2}, \Code{SB2}, \Code{TB2} &
	$\cos^2\alpha$, $\sin^2\alpha$,
	$\cos^2\beta$, $\sin^2\beta$, and $\tan^2\beta$ \\
\Code{CBA2}, \Code{SBA2} &
	$\cos^2(\beta - \alpha)$, $\sin^2(\beta - \alpha)$ \\
\Code{USfC}, \Code{UChaC}, \Code{VChaC}, \Code{ZNeuC} &
	the complex conjugates of various mixing matrix elements \\
\Code{MGl2}, \Code{MSf2}, \Code{MCha2}, \Code{MNeu2} \linebreak
\Code{Mh02}, \Code{MHH2}, \Code{MA02} \linebreak
\Code{MG02}, \Code{MHp2}, \Code{MGp2} &
	the squares of various masses \\
\Code{MSSMSimplify[\Var{expr}]} &
	\Code{Simplify} with SUSY trigonometric identities \\
&	(\eg \Code{SBA2 -> 1\,-\,CBA2}) \\
\Code{SUSYTrigExpand[\Var{expr}]} &
	express various trigonometric symbols \\
&	(\Code{SB2}, \Code{S2B}, etc.) through \Code{ca},
	\Code{sa}, \Code{cb}, \Code{sb} \\
\Code{SUSYTrigReduce[\Var{expr}]} &
	substitute back \Code{ca}, \Code{sa}, \Code{cb},
	\Code{sb} \\
\Code{SUSYTrigSimplify[\Var{expr}]} &
	perform trigonometric simplifications by applying
	\Code{SUSYTrigExpand}, \Code{Simplify}, and
	\Code{SUSYTrigReduce} in sequence
\etab
\indextt{CA2}%
\indextt{SA2}%
\indextt{CB2}%
\indextt{SB2}%
\indextt{TB2}%
\indextt{CBA2}%
\indextt{SBA2}%
\indextt{USfC}%
\indextt{UChaC}%
\indextt{VChaC}%
\indextt{ZNeuC}%
\indextt{MGl2}%
\indextt{MSf2}%
\indextt{MCha2}%
\indextt{MNeu2}%
\indextt{Mh02}%
\indextt{MHH2}%
\indextt{MA02}%
\indextt{MG02}%
\indextt{MGp2}%
\indextt{MHp2}%
\indextt{MSSMSimplify}%
\indextt{SUSYTrigExpand}%
\indextt{SUSYTrigReduce}%
\indextt{SUSYTrigSimplify}%

\indextt{Neglect}%
\index{neglecting masses}%
Often one wants to neglect certain variables, typically masses. 
Directly defining, say, \Code{ME = 0} may lead to problems, however, for
instance if \Code{ME} appears in a negative power, or in loop integrals
where neglecting it may cause singularities.  A better way is to assign
values to the function \Code{Neglect}, \eg \Code{Neglect[ME] = 0}, which
allows \FC\ to replace \Code{ME} by zero whenever this is safe.  Watch 
out for the built-in definitions mentioned above: since \eg 
\Code{ME\power 2} is automatically replaced by \Code{ME2}, one has to 
assign \Code{Neglect[ME] = Neglect[ME2] = 0} in order to have also the
even powers of the electron mass neglected.
\biitab
\Code{Neglect[\Vs] = 0} &
	replace \Vs\ by 0 except when it appears in negative powers or
	in loop integrals
\etab
To a certain extent it is also possible to use patterns in the argument 
of \Code{Neglect}.  Simple patterns like \Code{\uscore} and
\Code{\uscore\uscore} work always.  Since \FO's pattern matching is far
inferior to \mma's, though, it is not at all difficult to come up with
patterns which are not accepted by \FO.

\indextt{MassDim}%
\indextt{MassDim0}%
\indextt{MassDim1}%
\indextt{MassDim2}%
\index{dimension}%
Determining the mass dimension is an easy way of checking consistency
of an expression.  The function \Code{MassDim} substitutes all symbols
in the list \Code{MassDim0} by a random number, all symbols in
\Code{MassDim1} by \Code{Mass} times a random number, and all symbols 
in \Code{MassDim2} by $\Code{Mass}^2$ times a random number.  Symbols
not in \Code{MassDim\{0,1,2\}} are not replaced.  The random numbers
are supposed to guard against accidental cancellations.  An
expression consistent in the mass dimension should end up with just
one term of the form $(\Var{number})\cdot\Code{Mass}^n$.
\biitab
\Code{MassDim[\Var{expr}]} &
	replace the \Code{MassDim\Vn}-symbols in \Var{expr} by
	$(\Var{random~number})\cdot\Code{Mass}^n$ \\
\Code{MassDim0} &
	a list of symbols of mass dimension 0 \\
\Code{MassDim1} &
	a list of symbols of mass dimension 1 \\
\Code{MassDim2} &
	a list of symbols of mass dimension 2 \\
\Code{Mass} &
	a symbol representing the mass dimension
\etab


%------------------------------------------------------------------------

\subsection{Handling Abbreviations}
\label{sect:abbr}

\index{abbreviations}%
\Code{CalcFeynAmp} returns expressions where spinor chains, dot products of
vectors, and Levi-Civita tensors contracted with vectors have been
collected and abbreviated.  A term in such an expression may look like
\begin{alltt}
  C0i[cc12,\,MW2,\,MW2,\,S,\,MW2,\,MZ2,\,MW2] *
   ( -4 AbbSum16 Alfa2 CW2 MW2 S/SW2 + 32 AbbSum28 Alfa2 CW2 S^2/SW2 +
     4 AbbSum30 Alfa2 CW2 S^2/SW2 - 8 AbbSum7 Alfa2 CW2 S^2/SW2 +
     Abb1 Alfa2 CW2 S (T\,-\,U)/SW2 + 8 AbbSum29 Alfa2 CW2 S (T\,-\,U)/SW2 )
\end{alltt}
The first line stands for the tensor-coefficient function $C_{12}(\MW^2,
\MW^2, s, \MW^2, \MZ^2, \MW^2)$ which is multiplied with a linear
combination of abbreviations like \Code{Abb1} or \Code{AbbSum28} with
certain coefficients.  The coefficients of the abbreviations contain
kinematical variables, in this case the Mandelstam variables \Code{S},
\Code{T}, and \Code{U}, and parameters of the model, here \eg \Code{Alfa2} or 
\Code{MW2}.  This particular excerpt of code happens to be from a process 
without external fermions; otherwise spinor chains, abbreviated as
\Code{F\Vn}, would appear, too.
\index{Mandelstam variables}%
\index{short-hands}%
\index{model-dependent symbols}

The abbreviations like \Code{Abb1} or \Code{AbbSum29} can drastically
reduce the size of an amplitude, particularly so because they are nested
in three levels.  Consider \Code{AbbSum29} from the example above, which is
an abbreviation of about average length:\\
\begin{picture}(300,80)
\SetScale{.91666}
\Text(0,60)[bl]{\Code{~~~AbbSum29 = Abb2 + Abb22 + Abb23 + Abb3}}
\SetOffset(181,0)
\EBox(-19,55)(19,72)
\Line(-19,55)(-82,41)
\Line(-82,41)(-82,28)
\Line(19,55)(82,41)
\Line(82,41)(82,28)

\Text(0,30)[b]{\Code{Abb22 = Pair1 Pair3 Pair6}}
\SetOffset(204,0)
\EBox(-19,25)(19,42)
\Line(-19,25)(-77,13)
\Line(-77,13)(-77,0)
\Line(19,25)(77,13)
\Line(77,13)(77,0)

\Text(0,0)[b]{\Code{Pair3 = Pair[e[3],\,k[1]]}}
\end{picture}

Without abbreviations, the result would for each \Code{AbbSum29} contain
\begin{alltt}
   Pair[e[1],\,e[2]] Pair[e[3],\,k[1]] Pair[e[4],\,k[1]] +
   Pair[e[1],\,e[2]] Pair[e[3],\,k[2]] Pair[e[4],\,k[1]] +
   Pair[e[1],\,e[2]] Pair[e[3],\,k[1]] Pair[e[4],\,k[2]] +
   Pair[e[1],\,e[2]] Pair[e[3],\,k[2]] Pair[e[4],\,k[2]]
\end{alltt}

The size-reduction effect can be quantified by comparing the
\Code{LeafCount} of the expressions in \mma.  The leaf count is a measure
for the size of an expression, more precisely it counts the number of
subexpressions or ``leaves'' on the expression tree.  \Code{AbbSum29} has a
leaf count of 1 since it is just a plain symbol.  In comparison, its fully
expanded contents have a leaf count of 77.%
\index{size of the amplitude}%
\index{leaf count}%

\biitab
\Code{Abbr[]} &
	the list of abbreviations introduced so far \\
\Code{Abbr[\Var{patt}]} &
	the list of all abbreviations including (or excluding,
	if preceded by \Code{!}~(\Code{Not})) the pattern \Var{patt} \\
\Code{Unabbr[\Var{expr}]} &
	substitute back all abbreviations and subexpressions \\
\Code{Unabbr[\Var{expr},\,\Var{patt}]} &
	substitute back only those free of \Var{patt} \\
\Code{Unabbr[\Var{expr},\,!\Vf} \greyed{[\Code{,\,\Var{patt}}]}\Code{]} &
	do not substitute inside objects matching \Vf \\
\Code{OptimizeAbbr[\Va]} &
	optimize the abbreviations \Va \\
\Code{\$OptPrefix} &
	the prefix for additional abbreviations introduced by
	\Code{OptimizeAbbr} \\
\Code{SubstAbbr[\Var{exprlist},\,\Var{patt}]} &
	expand out abbreviations matching \Var{patt} in \Var{exprlist} \\
\Code{SubstSimpleAbbr[\Var{exprlist}]} &
	expand out `simple' abbreviations in \Var{exprlist}
\etab
\indextt{Abbr}%
\indextt{Unabbr}%
\indextt{OptimizeAbbr}%
\indextt{\$OptPrefix}%
\indextt{SubstAbbr}%
\indextt{SubstSimpleAbbr}%
The definitions of the abbreviations can be retrieved by \Code{Abbr[]} 
which returns a list of rules such that
\begin{verbatim}
   result //. Abbr[]
\end{verbatim}
gives the full, unabbreviated expression.  Needless to say, if one wants 
to use the results of \Code{CalcFeynAmp} outside of the present \FC\ 
session, the abbreviations have to be saved, too, \eg with
\begin{verbatim}
   Abbr[] >> abbr
\end{verbatim}
\Code{Abbr[\Var{patt}]} retrieves only the subset of abbreviations matching
at least one of the patterns \Var{patt}.  To exclude, rather than include,
a pattern, precede it by \Code{!}~(\Code{Not}).  For example,
\Code{Abbr[a,\,!b]} returns all abbreviations with \Code{a} but not \Code{b}.

\Code{OptimizeAbbr[\Va]} optimizes the list of abbreviations \Va.  The
optimization is done in two steps.  First, redundant parts are removed,
\eg the abbreviations
\begin{verbatim}
   AbbSum637 -> Abb109 - Abb187
   AbbSum504 -> Abb109 - Abb173 - Abb174 - Abb187
   AbbSum566 -> Abb109 + Abb173 + Abb174 - Abb187
\end{verbatim}
are replaced by
\begin{verbatim}
   AbbSum637 -> Abb109 - Abb187
   AbbSum504 -> AbbSum637 - Abb173 - Abb174
   AbbSum566 -> AbbSum637 + Abb173 + Abb174
\end{verbatim}
Then, in a second step, common subexpressions are eliminated, thereby
simplifying the last lines further to
\begin{verbatim}
   AbbSum637 -> Abb109 - Abb187
   Opt1 -> Abb173 + Abb174
   AbbSum504 -> AbbSum637 - Opt1
   AbbSum566 -> AbbSum637 + Opt1
\end{verbatim}
Optimizing the abbreviations may take some time but can also speed up
numerical computations considerably.  The prefix for the new 
abbreviations introduced by \Code{OptimizeAbbr}, \ie the \Code{Opt} in 
the \Code{Opt\Var{N}} in the example above, can be chosen through the 
global variable \Code{\$OptPrefix}.

\Code{SubstAbbr[\Var{exprlist},\,\Var{patt}]} expands out all 
abbreviations matching \Var{patt} in \Var{exprlist}; \eg if the 
abbreviation \Code{\Va\,->\,\Vb} matches, the definition 
\Code{\Va\,->\,\Vb} is removed and all \Va\ in \Var{exprlist} are 
substituted by \Vb, and the definition \Code{\Va\,->\,\Vb}.

\Code{SubstSimpleAbbr[\Var{exprlist}]} expands out `simple' 
abbreviations in \Var{exprlist}.  Abbreviations are `simple' if they are 
of the form \Var{(number)} or $\Var{(number)}\cdot\Var{(symbol)}$, or if 
the rhs's LeafCount is not larger than the lhs's.  Such expressions may 
show up in the abbreviation list \eg after further simplification.

%------------------------------------------------------------------------

\subsection{More Abbreviations}

The abbreviations above are introduced automatically by \FC, and for
specific quantities only.  There is also the possibility to abbreviate
arbitrary expressions with the \Code{Abbreviate} command.
\biitab
\Code{Abbreviate[\Var{expr},\,\Var{lev}]} &
	introduce abbreviations for subexpressions of \Var{expr} 
	starting at level \Var{lev} \\
\Code{Abbreviate[\Var{expr},\,\Vf]} &
	introduce abbreviations for subexpressions of \Var{expr}
	for which \Vf\ returns \Code{True} \\
\Code{AbbrevSet[\Var{rawexpr}]} &
	set up the \Code{AbbrevDo} function using \Var{rawexpr}
	for determining the summation indices \\
\Code{AbbrevDo[\Var{expr},\,\Vi]} &
	introduce abbreviations for subexpressions of \Var{expr}
	where \Vi\ is either a level or a function, as above \\
\Code{\$AbbPrefix} &
	the prefix for abbreviations introduced by
	\Code{Abbreviate}
\etab%
\indextt{Abbreviate}%
\indextt{AbbrevSet}%
\indextt{AbbrevDo}%
\indextt{\$AbbPrefix}

\Code{Abbreviate} introduces abbreviations for subexpressions starting
from a given depth of the expression.  Depending on this starting level,
the expression will be more or less thoroughly abbreviated.  A starting
level of 1 represents the extreme case where the result of
\Code{Abbreviate} is just a single symbol.  Currently, only sums are
considered for abbreviations.

The alternate invocation, with a function as second argument, introduces 
abbreviations for all subexpressions for which this function yields 
\Code{True}, like \Code{Select}.  This is useful, for example, to get a 
picture of the structure of an expression with respect to a certain 
object, as in
\begin{verbatim}
  Abbreviate[a + b + c + (d + e) x, FreeQ[#, x]&, MinLeafCount -> 0]
\end{verbatim}
which gives \Code{Sub2 + Sub1 x}, thus indicating that the original 
expression is linear in \Code{x}.

The functionality of \Code{Abbreviate} is actually separated into a
pair of functions \Code{AbbrevSet} and \Code{AbbrevDo}, \ie
\Code{Abbreviate} internally runs \Code{AbbrevSet} to define
\Code{AbbrevDo} and then executes \Code{AbbrevDo}.  The expression
given to \Code{AbbrevSet} is not itself abbreviated but used for
determining the summation indices; it could be \eg a raw amplitude.  
This is particularly important in cases where partial expressions
will be given to \Code{AbbrevDo} and where the summation indices
may not be correctly inferred because \Code{AbbrevDo} does not see
the full expression.  The definition of \Code{AbbrevDo} is not
automatically removed, so be careful not to execute \Code{AbbrevDo}
with an expression that is not a subexpression of the one given to
\Code{AbbrevSet}!

The prefix of the abbreviations are given by the global variable
\Code{\$AbbPrefix}.

\biiitab{option}
\Code{MinLeafCount} & \Code{10} &
	the mininum leaf count above which a subexpression becomes
	eligible for abbreviationing \\
\Code{Deny} & \Code{\Brac{k,\,q1}} &
	symbols which must not occur in abbreviations \\
\Code{Fuse} & \Code{True} &
	whether to fuse adjacent items for which the selection
	function is \Code{True} into one abbreviation \\
\Code{Preprocess} & \Code{Identity} &
	a function applied to subexpressions before abbreviationing
\etab%
\indextt{MinLeafCount}%
\indextt{Deny}%
\indextt{Fuse}%
\indextt{Preprocess}

\Code{MinLeafCount} determines the minimum leaf count a common
subexpression must have in order that a variable is introduced for it.

\Code{Deny} specifies an exclusion list of symbols which must not occur
in abbreviations.

\Code{Fuse} specifies whether adjacent items for which the selection 
function returns \Code{True} should be fused into one abbreviation.  It 
has no effect when \Code{Abbreviate} is invoked with a depth.  For 
example,
\begin{verbatim}
   Abbreviate[a Pair[1, 2] Pair[3, 4], MatchQ[#, _Pair]&,
     Fuse -> False, MinLeafCount -> 0]
\end{verbatim}
introduces two abbreviations, one for \Code{Pair[1, 2]} and one for 
\Code{Pair[3, 4]}, whereas with \Code{Fuse -> True} only one 
abbreviation for the product is introduced.

\Code{Preprocess} specifies a function to be applied to all subexpressions
before introducing abbreviations for them.

The abbreviations introduced with \Code{Abbreviate} are returned by 
\Code{Subexpr[]}.  This works similar to \Code{Abbr[]} and also the 
\Code{OptimizeAbbr} function can be applied in the same way.
\biitab
\Code{Subexpr[]} &
	the list of abbreviations introduced for subexpressions so far \\
\Code{Subexpr[\Var{args}]} &
	executes \Code{Abbreviate[\Var{args}]} locally, \ie without
	registering the subexpressions permanently, and returns
	\{list of subexpressions, abbreviated expression\}
\etab%
\indextt{SubExpr}

Introducing abbreviations for subexpression has three advantages:
\begin{itemize}
\item
The overall structure of the abbreviated expression becomes clearer.

\item
Duplicate subexpressions are computed only once.

\item
When writing out code for the abbreviations with \Code{WriteSquaredME}, 
the abbreviations are sorted into categories depending on their 
dependence on kinematical variables and are thus computed only as often 
as necessary.
\end{itemize}
The combined effect of the latter two points can easily lead to a 
speed-up by a factor 3.

%------------------------------------------------------------------------

\subsection{Resuming Previous Sessions}

Loading a list of abbreviations from a previous \FC\ session does
not by itself mean that \Code{CalcFeynAmp} or \Code{Abbreviate} will use
them in subsequent calculations.  To this end they must be registered
with the system.  There are two functions for this.
\biitab
\Code{RegisterAbbr[\Var{abbr}]} &
	register a list of abbreviations \\
\Code{RegisterSubexpr[\Var{subexpr}]} &
	register a list of subexpressions
\etab%
\indextt{RegisterAbbr}%
\indextt{RegisterSubexpr}%
\index{registering abbreviations}%
\index{abbreviations!registering}%
\index{subexpressions!registering}

\Code{RegisterAbbr} registers a list of abbreviations, \eg the output of
\Code{Abbr[]} in a previous session, such that future invocations of
\Code{CalcFeynAmp} will make use of them.  Note that abbreviations
introduced for different processes are in general not compatible.

\Code{RegisterSubexpr} registers a list of subexpressions, \eg the
output of \Code{Subexpr[]} in a previous session, such that future
invocations of \Code{Abbreviate} will make use of them.

For long-running calculations, the \Code{Keep} function is helpful to
store intermediate expressions, such that the calculation can be resumed
after a crash.  As a side effect, the intermediate results can be 
inspected easily, even while a batch job is in progress.
\biitab
\Code{Keep[\Var{expr},\,\Var{name},\,\Var{path}]} &
	loads \Code{\Var{path}/\Var{name}.m} if it exists, otherwise
	evaluates \Var{expr} and stores the result (together with the
	output of \Code{Abbr[]} and \Code{Subexpr[]}) in that file.  
	\Var{path} is optional and defaults to \Code{\$KeepDir} \\
\Code{Keep[\Var{expr},\,\Var{name}]} &
	same as \Code{Keep[\Var{expr},\,\Var{name},\,\$KeepDir]} \\
\Code{Keep[\Var{lhs} = \Var{rhs}]} &
	same as \Code{\Var{lhs} = Keep[\Var{rhs}, "\Var{lhs}"]} \\
\Code{\$KeepDir} &
	the default directory for storing intermediate expressions
\etab%
\indextt{Keep}%

\Code{Keep} has two basic arguments: a file (path and name) and an
expression.  If the file exists, it is loaded.  If not, the expression
is evaluated and the results stored in the file, thus creating a
checkpoint.  If the calculation crashes, it suffices to restart the very
same program, which will then load all parts of the calculation that
have been completed and resume at the point it left off.

The syntax of \Code{Keep} is constructed so as to make adding it to 
existing programs is as painless as possible.  For example, a statement 
like
\begin{verbatim}
  amps = CalcFeynAmp[...]
\end{verbatim}
simply becomes
\begin{verbatim}
  Keep[amps = CalcFeynAmp[...]]
\end{verbatim}
Needless to say, this logic fails to work if symbols are being 
re-assigned, \ie appear more than once on the left-hand side, as in
\begin{verbatim}
  Keep[amps = CalcFeynAmp[virt]]
  Keep[amps = Join[amps, CalcFeynAmp[counter]]
\end{verbatim}
Due to the first \Code{Keep} statement, the second will always find the
file \Code{keep/amps.m} and never execute the computation of the counter 
terms.

And there are other ways to confuse the system: mixing intermediate
results from different calculations, changing flags out of sync with the
intermediate results, etc.  In case of doubt, \ie if results seem
suspicious, remove all intermediate files and re-do the calculation from
scratch.

%------------------------------------------------------------------------

\subsection{Fermionic Matrix Elements}
\label{sect:helicityme}

\index{fermionic matrix elements}%
\index{external fermions}%
\index{spinor chain}%
When \Code{FermionChains -> Chiral} or \Code{VA} is chosen, an
amplitude involving external fermions will contain \Code{DiracChain}s, 
abbreviated as \Code{F\Vi}, \eg
\begin{alltt}
  F1 -> DiracChain[Spinor[k[2],\,ME,\,-1], 6, Lor[1], Spinor[k[1],\,ME,\,1]] *
        DiracChain[Spinor[k[3],\,MT,\,1], 6, Lor[1], Spinor[k[4],\,MT,\,-1]]
\end{alltt}
In physical observables such as the cross-section, where only the square
of the amplitude or interference terms can enter, these spinor chains can
be evaluated without reference to a concrete representation for the
spinors.  The point is that in terms like $|\M|^2$ or $2\Re(\M_0^*\M_1)$
only products $(\Code{F\Vi}\,\Code{F\Vj}^*)$ of spinor chains appear and 
these can be calculated using the density matrix for spinors
$$
\{u_\lambda(p)\bar u_\lambda(p), v_\lambda(p)\bar v_\lambda(p)\} =
\begin{cases}
\frac 12 (1\pm\lambda\gamma_5)\pslash &
  \text{for massless fermions\footnotemark} \\
\frac 12 (1 + \lambda\gamma_5\sslash)(\pslash\pm m) &
  \text{for massive fermions}
\end{cases}
$$%
\footnotetext{%
	In the limit $E\gg m$ the vector $s$ becomes increasingly
	parallel to $p$, \ie $s\sim p/m$, hence
	$$
	\left.\begin{aligned}
	\pslash (\pslash + m) &= p^2 + m\pslash = m (\pslash + m) \\
	\pslash (\pslash - m) &= p^2 - m\pslash = -m (\pslash - m)
	\end{aligned}\right\}
	\quad\Rightarrow\quad
	(1 + \lambda\gamma_5\sslash) (\pslash\pm m)
	\overset{E\gg m}{\longrightarrow}
	\left(1 + \lambda\gamma_5\frac{\pslash}{m}\right)(\pslash\pm m) =
	  (1\pm\lambda\gamma_5)\pslash\,.
	$$
}%
where $\lambda = \pm 1$ and $s$ is the helicity reference vector
corresponding to the momentum $p$.  $s$ is the unit vector in the 
direction of the spin axis in the particle's rest frame, boosted into the 
CMS.  It is identical to the longitudinal polarization vector of a vector 
boson, and fulfills $s\cdot p = 0$ and $s^2 = -1$.%
\index{density matrix}%
\index{helicity matrix elements}%
\index{helicity}%
\index{helicity reference vector}%
\index{spin}

\index{unpolarized case}%
In the unpolarized case the $\lambda$-dependent part adds up to zero, so
the projectors become
$$
\sum_{\lambda=\pm} u_\lambda(p)\bar u_\lambda(p) = \pslash + m\,,
\qquad
\sum_{\lambda=\pm} v_\lambda(p)\bar v_\lambda(p) = \pslash - m\,.
$$
Technically, one can use the same formula as in the polarized case by
putting $\lambda = 0$ and multiplying the result by 2 for each external
fermion.

\FC\ supplies the function \Code{HelicityME} to calculate the helicity 
matrix elements $(\Code{F\Vi}\,\Code{F\Vj}^*)$.
\biitab
\Code{HelicityME[\Var{\M_0},\,\Var{\M_1}]} &
	calculate the helicity matrix elements for all combinations
	of \Code{F\Vn} appearing in \Var{\M_0^* M_1} \\
\Code{All} &
	(used as either argument of \Code{HelicityME}:)
	instead of selecting the \Code{F}s which appear in an expression,
	simply take all \Code{F}s currently in the abbreviations \\
\Code{Mat[F\Vi,\,F\Vj]} &
	the helicity matrix element resulting from the spinor chains
	in \Code{F\Vi} and \Code{F\Vj} \\
\Code{Hel[\Vn]} &
	the helicity $\lambda_n$ of the \Vn th external particle \\
\Code{s[\Vn]} &
	the helicity reference vector of the \Vn th external particle
\etab
\indextt{HelicityME}%
\indextt{All}%
\indextt{Mat}%
\indextt{Hel[\Vn]}%
\indextt{s[\Vn]}%
\index{abbreviations}%
To be sure, \Code{HelicityME} does not calculate the full expression
$\M_0^*\M_1$, only the combinations of \Code{F}s that appear in this
product.  These are called \Code{Mat[F\Vi,\,F\Vj]} and depend on the 
helicities and helicity reference vectors of the external particles,
\Code{Hel[\Vn]} and \Code{s[\Vn]}.  See Sect.\ \ref{sect:squaredme} on
how to put together the complete expression $\M_0^*\M_1$.

If possible, specific values for the \Code{Hel[\Vn]} should be fixed in
advance, since that can dramatically speed up the calculation and also
lead to (much) more compact results.  For example, as mentioned before,
unpolarized matrix elements can be obtained by putting the helicities
$\lambda_n = 0$ and multiplying by 2 for each external fermion.  
Therefore, for calculating only the unpolarized amplitude, one could use
\begin{verbatim}
   _Hel = 0;
   mat = HelicityME[...]
\end{verbatim}
which is generally much faster than \verb=HelicityME[...] /. Hel[_] -> 0=.
(Don't forget that the matrix elements obtained in this way have yet to
be multiplied by 2 for each external fermion.)

\fbox{\parbox{.985\linewidth}{%
\textit{Note:} \Code{HelicityME} uses internal definitions set up
by \Code{CalcFeynAmp}.  It is therefore not advisable to mix the
evaluation of different processes.  For instance, wrong results can be
expected if one uses \Code{HelicityME} on results from process $A$
after having computed amplitudes from process $B$ with
\Code{CalcFeynAmp}.  Since this requires at least one invocation of
\Code{ClearProcess}, though, it is unlikely to happen accidentally.}}

\biiitab{option}
\Code{Dimension} & \Code{4} &
	the dimension to compute in \\
\Code{TreeSquare} & \Code{\$TreeSquare} (default: \Code{True}) &
	whether to include the matrix elements for the computation of
	$|\M_0|^2$ \\
\Code{LoopSquare} & \Code{\$LoopSquare} (default: \Code{False}) &
	whether to include the matrix elements for the computation of
	$|\M_1|^2$ \\
\Code{RetainFile} & \Code{False} &
	whether to retain the temporary \FO\ command file \\
\Code{EditCode} & \Code{False} &
	whether to display the \FO\ code in an editor before sending it
	to \FO
\etab
\indextt{Dimension}%
\indextt{TreeSquare}%
\indextt{\$TreeSquare}%
\indextt{LoopSquare}%
\indextt{\$LoopSquare}%
\indextt{RetainFile}%
\indextt{EditCode}%
\Code{Dimension} is used as in \Code{CalcFeynAmp}.  Only the value
\Code{0} has the effect of actually computing in $D$ dimensions,
however, since for \Code{D} and \Code{4} the limit $D\to 4$ has already
been taken in \Code{CalcFeynAmp}.  The dimensional dependence of the
result is expressed through \Code{Dminus4} and \Code{Dminus4Eps}, where
the latter represents the \Code{Dminus4} arising from the contraction of
Levi-Civita tensors.  For testing and comparison, the default
equivalence \Code{Dminus4Eps = Dminus4} can be unset.%
\indextt{Dminus4}%
\indextt{Dminus4Eps}

The \Code{TreeSquare} and \Code{LoopSquare} options govern whether, in 
addition to $\M_0^*\M_1$, also $|\M_0|^2$ and/or $|\M_1|^2$ will be 
needed.  This allows to obtain all helicity matrix elements for \eg the 
computation of $|\M_0|^2 + 2\Re\M_0^*\M_1$ with a single invocation of 
\Code{HelicityME}.  The selections are stored in the global variables 
\Code{\$TreeSquare} (\Code{True} by default) and \Code{\$LoopSquare} 
(\Code{False} by default) and used as defaults in further invocations of 
\Code{HelicityME}, \Code{ColourME}, \Code{WeylME}, and
\Code{WriteSquaredME}.

The options \Code{RetainFile}, and \Code{EditCode} are used in the same 
way as for \Code{CalcFeynAmp}, see page \pageref{retainedit}.

\bigskip

The matrix element method can be applied to Weyl chains, too, which
makes sense if one wishes to apply correction factors to the matrix 
elements.  To this end \Code{CalcFeynAmp} must be instructed to encode 
the Weyl fermion chains \Code{F\Vi} as matrix elements \Code{Mat[F\Vi]} 
(and sort the amplitude accordingly) with the option \Code{FermionOrder 
-> Mat}.  The corresponding bilinear matrix elements 
\Code{Mat[F\Vi,\,F\Vj]} needed to compute the squared amplitude are then 
computed with \Code{WeylME}.
\biitab
\Code{WeylME[\Var{\M_0},\,\Var{\M_1}]} &
	calculate the fermion matrix elements for all combinations of
	\Code{F\Vn} appearing inside \Code{Mat} in \Var{\M_0^*\M_1} \\
\Code{All} &
	(used as either argument of \Code{WeylME}:)
	instead of selecting the \Code{F}s which appear in an expression,
	simply take all \Code{F}s currently in the abbreviations
\etab
\indextt{WeylME}

Since about the only purpose of Weyl matrix elements is to insert
correction factors in front of the \Code{Mat[F\Vi,\,F\Vj]}, there is an 
explicit option for this.

\biiitab{option}
\Code{MatFactor} & \Code{(1 \&)} &
	a function $f$ which determines the correction factor
	\Code{\Vf[F\Vi,\,F\Vj]} multiplied with \Code{Mat[F\Vi,\,F\Vj]} \\
\Code{TreeSquare} & \Code{\$TreeSquare} (default: \Code{True}) &
	whether to include the matrix elements for the computation of
	$|\M_0|^2$ \\
\Code{LoopSquare} & \Code{\$LoopSquare} (default: \Code{False}) &
	whether to include the matrix elements for the computation of
	$|\M_1|^2$
\etab
\indextt{MatFactor}%
\indextt{TreeSquare}%
\indextt{\$TreeSquare}%
\indextt{LoopSquare}%
\indextt{\$LoopSquare}

%------------------------------------------------------------------------

\subsection{Colour Matrix Elements}

Diagrams involving quarks or gluons usually\footnote{%
	Diagrams generated with the \Code{SM.mod} model file contain no
	SU($N$) objects since in the electroweak sector colour can be
	taken care of by a trivial factor 3 for each quark loop.}
contain objects from the SU($N$) algebra.  These are simplified by
\Code{CalcFeynAmp} using the Cvitanovic algorithm \cite{Cv76} in an
extended version of the implementation in \cite{Ve96}.  The idea is to
transform all SU($N$) objects to products of generators $T^a_{ij}$ which
are generically denoted by \Code{SUNT} in \FC.  In the output, only two
types of objects can appear:
\begin{itemize}
\item
Chains (products) of generators with external colour indices; these are
denoted by $\Code{SUNT[\Va,\,\Vb,\,$\dots$,\,\Vi,\,\Vj]} =
(T^\Va T^\Vb\cdots)_{\Vi\Vj}$ where \Vi\ and \Vj\ are the
external colour indices and the \Va, \Vb, $\dots$ are the indices
of external gluons.  This notation includes also the identity in colour
space as the special case with no external gluons: $\delta_{\Vi\Vj} = 
\Code{SUNT[\Vi,\,\Vj]}$.

\item
Traces over products of generators; these are denoted by
$\Code{SUNT[\Va,\,\Vb,\,$\dots$,\,0,\,0]} = \Tr(T^\Va T^\Vb\cdots)$.
\end{itemize}
The situation is much the same as with fermionic structures: just as
an amplitude contains open spinor chains if external fermions are
involved, it also contains \Code{SUNT}s if external quarks or gluons are
involved.%
\indextt{SUNT}%
\index{colour indices}%
\index{gluon indices}%
\index{SU($N$) objects}%
\index{Cvitanovic algorithm}

For the \Code{SUNT} objects in the output, \FC\ introduces abbreviations
of the type \Code{SUN\Vn}.  These abbreviations can easily be
evaluated further if one computes the squared amplitude, because then
the external lines close and the Cvitanovic algorithm yields a simple
number for each combination of \Code{SUN\Vi} and \Code{SUN\Vj}. 
(One can think of the squared amplitude being decomposed into parts,
each of which is multiplied by a different colour factor.)  But this is
precisely the idea of helicity matrix elements applied to the SU($N$)
case!

Because of this close analogy, the combinations of \Code{SUN\Vi} and
\Code{SUN\Vj} are called colour matrix elements in \FC\ and are written
accordingly as \Code{Mat[SUN\Vi,\,SUN\Vj]}.  The function which computes 
them is \Code{ColourME}.  It is invoked just like \Code{HelicityME}.
\biitab
\Code{ColourME[\Var{\M_0},\,\Var{\M_1}]} &
	calculate the colour matrix elements for all combinations
	of \Code{SUN\Vn} appearing in \Var{\M_0^*\M_1} \\
\Code{All} &
	(used as either argument of \Code{ColourME}:)
	instead of selecting the \Code{SUN}s which appear in an 
	expression, simply take all \Code{SUN}s currently in 
	abbreviations \\
\Code{Mat[SUN\Vi,\,SUN\Vj]} &
	the colour matrix element resulting from the SU($N$) objects
	\Code{SUN\Vi} and \Code{SUN\Vj}
\etab
\indextt{ColourME}%
\indextt{All}%
\indextt{Mat}

\biiitab{option}
\Code{TreeSquare} & \Code{\$TreeSquare} (default: \Code{True}) &
	whether to include the matrix elements for the computation of
	$|\M_0|^2$ \\
\Code{LoopSquare} & \Code{\$LoopSquare} (default: \Code{False}) &
	whether to include the matrix elements for the computation of
	$|\M_1|^2$
\etab
\indextt{Dimension}%
\indextt{TreeSquare}%
\indextt{\$TreeSquare}%
\indextt{LoopSquare}%
\indextt{\$LoopSquare}

The core function behind \Code{ColourME} can also be used directly to
simplify colour structures.
\biitab
\Code{ColourSimplify[\Var{expr}]} &
	simplify colour objects in \Var{expr} \\
\Code{ColourSimplify[\Var{tree},\,\Var{loop}]} &
	simplifies the colour objects in $(\Var{tree}^*\:\Var{loop})$
\etab%
\indextt{ColourSimplify}

Furthermore, \FC\ implements a special case of \FA's \Code{DiagramGrouping} 
function in \Code{ColourGrouping}, which groups Feynman diagrams according
to their colour structures.  The correct grouping can only be done with
fully simplified colour structures, which is why this function is part
of \FC, not \FA.
\biitab
\Code{ColourGrouping[\Var{ins}]} &
	group the inserted topologies (output of \Code{InsertFields})
	according to their colour structures
\etab%
\indextt{ColourGrouping}

%------------------------------------------------------------------------

\subsection{Putting together the Squared Amplitude}
\label{sect:squaredme}
\index{squared matrix element}%

Now that \Code{CalcFeynAmp} has calculated the amplitudes and
\Code{HelicityME} and \Code{ColourME} have produced the helicity and colour
matrix elements, the remaining step is to piece together the squared
matrix element $|\M|^2$, or more generally products like $\M_0^*\M_1$.

This is non-trivial only if there are matrix elements of the form
\Code{Mat[\Vi,\,\Vj]} around, which have to be put in the right places.
Specifically, if $\M_0$ and $\M_1$ are written in the form
\begin{align*}
\M_0 &= a_{11}~\Code{F1}~\Code{SUN1} +
        a_{21}~\Code{F2}~\Code{SUN1} + \ldots
= \sum_{ij} a_{ij}~\Code{F}i~\Code{SUN}j
\quad\text{and} \\
\M_1 &= b_{11}~\Code{F1}~\Code{SUN1} +
        b_{21}~\Code{F2}~\Code{SUN1} + \ldots
= \sum_{ij} b_{ij}~\Code{F}i~\Code{SUN}j\,,
\end{align*}
their product becomes
\begin{align*}
\M_0^*\M_1 &\:\:{=}\:\:
a_{11}^* b_{11}~\Code{Mat[F1,\,F1]}~\Code{Mat[SUN1,\,SUN1]} + \\
&\:\:\hphantom{=}\:\:
a_{21}^* b_{11}~\Code{Mat[F1,\,F2]}~\Code{Mat[SUN1,\,SUN1]} + \ldots \\
&\:\:{=}\:\: \sum_{ijk\ell} a_{ik}^* b_{j\ell}~
    \Code{Mat[F\Vj,\,F\Vi]}~
    \Code{Mat[SUN\Var{\ell},\,SUN\Var{k}]}\,.
\end{align*}
The coefficients $a_{ik}$ and $b_{j\ell}$ are known as form factors.
For efficiency, they are usually computed separately in the numerical
evaluation, so that the final expression for the squared matrix element 
is easily summed up \eg in Fortran as
\begin{alltt}
        do 1 i = 1, \textrm{(\# of \Code{F}s in \Var{\M\sb{0}})}
        do 1 j = 1, \textrm{(\# of \Code{F}s in \Var{\M\sb{1}})}
        do 1 k = 1, \textrm{(\# of \Code{SUN}s in \Var{\M\sb{0}})}
        do 1 l = 1, \textrm{(\# of \Code{SUN}s in \Var{\M\sb{1}})}
          result = result + Conjugate(a(i,k))*b(j,l)*MatF(j,i)*MatSUN(l,k)
  1     continue
\end{alltt}
While this is arguably the most economic way to evaluate a squared
amplitude numerically, it is also possible to directly obtain the squared
matrix element as a \mma\ expression.  The function which does this is
\Code{SquaredME}.
\biitab
\Code{SquaredME[\Var{\M_0},\,\Var{\M_1}]} &
	calculates $\Var{\M_0^*}\Var{\M_1}$, taking care to put the
	\Code{Mat[\Vi,\,\Vj]} in the right places
\etab
\indextt{SquaredME}%
\Code{SquaredME} is called in much the same way as \Code{HelicityME}.
Because of the number of terms that are generated, this function is most
useful only for rather small amplitudes.

\indextt{FF}%
\indextt{FFC}%
For clarity of output the result of \Code{SquaredME} is given as two 
pieces: \Code{\Brac{\Var{expr},\,\Var{rul}}}, where \Var{expr} is
the squared amplitude expressed in terms of form factors \Code{FF} and 
\Code{FFC}, and \Var{rul} is a list of rules which provide the values
of the \Code{FF} and \Code{FFC}.

\indextt{Mat}%
\Code{SquaredME} does not insert the actual values for the 
\Code{Mat[\Vi,\,\Vj]}.  This can easily be done later by applying the 
output of \Code{HelicityME}, \Code{ColourME}, or \Code{WeylME}, which 
are lists of rules substituting the \Code{Mat[\Vi,\,\Vj]} by their 
values.  That is to say, \Code{SquaredME} and 
\Code{HelicityME}/\Code{ColourME}/\Code{WeylME} perform complementary 
tasks: the former builds up the squared amplitude in terms of the 
\Code{Mat[\Vi,\,\Vj]} whereas the latter calculate the 
\Code{Mat[\Vi,\,\Vj]}.

%------------------------------------------------------------------------

\subsection{Polarization Sums}
\index{polarization sum}

In the presence of external gauge bosons, the output of \Code{SquaredME} 
will still contain polarization vectors (in general implicitly, \ie 
through the abbreviations).  For unpolarized gauge bosons, the latter
can be eliminated by means of the identities
\begin{align*}
\sum_{\lambda = 1}^3
\varepsilon_\mu^*(k, \lambda) \varepsilon_\nu(k, \lambda)
&= -g_{\mu\nu} + \frac{k_\mu k_\nu}{m^2}
&& \text{for massive particles,} \\
\sum_{\lambda = 1}^2
\varepsilon_\mu^*(k, \lambda) \varepsilon_\nu(k, \lambda)
&= -g_{\mu\nu} - \frac{\eta^2 k_\mu k_\nu}{(\eta\cdot k)^2} +
\frac{\eta_\mu k_\nu + \eta_\nu k_\mu}{\eta\cdot k}
&& \text{for massless particles.}
\end{align*}
In the massless case the polarization sum is gauge dependent and $\eta$
is an external four-vector which fulfills $\eta\cdot\varepsilon = 0$ and
$\eta\cdot k\neq 0$.  \FC\ makes the additional assumption 
$\eta\cdot\eta = 0$ and also drops the $\eta^2$ term above.  For a 
gauge-invariant quantity, the $\eta$-dependence should ultimately 
cancel.

\FC\ provides the function \Code{PolarizationSum} to apply the above 
identities.
\biitab
\Code{PolarizationSum[\Var{expr}]} &
	sums \Var{expr} over the polarizations of external gauge bosons
\etab%
\indextt{PolarizationSum}%
It is assumed that \Var{expr} is the squared amplitude into which the
helicity matrix elements have already been inserted.  Alternately,
\Var{expr} may also be given as an amplitude directly, in which case
\Code{PolarizationSum} will first invoke \Code{SquaredME} and
\Code{HelicityME} (with \Code{\uscore Hel = 0}) to obtain the squared
amplitude.  \Code{PolarizationSum} cannot simplify Weyl chains, such as
\Code{CalcFeynAmp} introduces with \Code{FermionChains -> Weyl} (the
default).

\biiitab{option}
\Code{SumLegs} & \Code{All} &
	which external legs to include in the polarization sum \\
\Code{Dimension} & \Code{4} &
	the dimension to compute in \\
\Code{GaugeTerms} & \Code{True} &
	whether to retain $\eta$-dependent terms \\
\Code{NoBracket} & (taken from \Code{CalcFeynAmp}) &
	symbols not to be included in the bracketing in FORM \\
\Code{RetainFile} & \Code{False} &
	whether to retain the temporary \FO\ command file \\
\Code{EditCode} & \Code{False} &
	whether to display the \FO\ code in an editor before sending it
	to \FO
\etab
\indextt{SumLegs}%
\indextt{Dimension}%
\indextt{GaugeTerms}%
\indextt{NoBracket}%
\indextt{RetainFile}%
\indextt{EditCode}%
\Code{SumLegs} allows to restrict the polarization sum to fewer than all 
external vector bosons.  For example, \Code{SumLegs\,->\,\Brac{3,4}} 
sums only vector bosons on legs 3 and 4.

\Code{Dimension} is used as in \Code{CalcFeynAmp}.  Only the value
\Code{0} has the effect of actually computing in $D$ dimensions,
however, since for \Code{D} and \Code{4} the limit $D\to 4$ has already
been taken in \Code{CalcFeynAmp}.  The dimensional dependence of the
result is expressed through \Code{Dminus4} and \Code{Dminus4Eps}, where
the latter represents the \Code{Dminus4} arising from the contraction of
Levi-Civita tensors.  For testing and comparison, the default
equivalence \Code{Dminus4Eps = Dminus4} can be unset.%
\indextt{Dminus4}%
\indextt{Dminus4Eps}

\Code{GaugeTerms} retains terms containing the gauge-dependent auxiliary 
vector $\eta$.  More precisely, the $\eta$-terms are actually introduced 
at first, to let potential cancellations of $\eta$'s in the numerator 
against the denominator occur, but set to zero later.

The options \Code{MomElim}, \Code{DotExpand}, \Code{NoBracket},
\Code{RetainFile}, and \Code{EditCode} are used in the same way as for
\Code{CalcFeynAmp}, see page \pageref{retainedit}.

\fbox{\parbox{.985\linewidth}{%
\textit{Note:} \Code{PolarizationSum} uses internal definitions set up
by \Code{CalcFeynAmp}.  It is therefore not advisable to mix the
evaluation of different processes.  For instance, wrong results can be
expected if one uses \Code{PolarizationSum} on results from process $A$
after having computed amplitudes from process $B$ with
\Code{CalcFeynAmp}.  Since this requires at least one invocation of
\Code{ClearProcess}, though, it is unlikely to happen accidentally.}}

%------------------------------------------------------------------------

\subsection{Analytic Unsquared Amplitudes}%
\index{analytic amplitudes}%
\index{unsquared}%
\index{components}%
\index{four-vector}%
\index{vector}

The `smallest' object appearing in the output of \Code{CalcFeynAmp} is a
four-vector, \ie \FC\ does not normally go into components.  Those are
usually inserted only in the numerical part.  This has advantages: for
example, the analytical expression does not reflect a particular
phase-space parameterization.

One can also obtain an analytic expression in terms of kinematic 
invariants (but no four-vectors) by squaring the amplitude and computing 
the polarization sums, as outlined above.  This has the advantage of 
being independent of the representation of the spinors and vectors, but 
of course the size of the expression is significantly increased by 
squaring.

As a third alternative, one can obtain an analytical expression for the
unsquared amplitude.  This requires to go into components, however. 

To this end one has to load the extra package \Code{VecSet}:
\begin{verbatim}
   << FormCalc`tools`VecSet`
\end{verbatim}
and for each external vector invoke the function \Code{VecSet}, which 
has the same syntax as its Fortran namesake, \eg
\begin{verbatim}
   VecSet[1, m1, p1, {0, 0, 1}]
\end{verbatim}

\biitab
\Code{VecSet[\Vn,\,\Vm,\,\Vp,\,\Brac{\Var{e_x},\Var{e_y},\Var{e_z}}]} &
	set the momentum, polarization vectors, and spinors for
	particle \Vn\ with mass \Vm\ and three-momentum
	\Var{\vec p = p\,\{e_x, e_y, e_z\}}
\etab%
\indextt{VecSet}

The amplitude is then evaluated with the function \Code{ToComponents}, 
\eg
\begin{verbatim}
   ToComponents[amp, "+-+-"]
\end{verbatim}
This delivers an expression in terms of the phase-space parameters 
used in \Code{VecSet}.

\biitab
\Code{ToComponents[\Var{amp},\,"\Var{p_1 p_2}\rlap{$\dots$\,\Var{p_n}"]}} & \\
\Code{ToComponents[\Var{amp},\,\lbrac\Var{p_1},\,\Var{p_2},\rlap{$\dots$,\,\Var{p_n}\rbrac]}} & \\
&	evaluate \Var{amp} by substituting four-vectors by their
	component-wise representation using external polarizations
	\Var{p_1, \dots, p_n}
\etab%
\indextt{ToComponents}

\Code{ToComponents[\Var{amp},\,\Var{pol}]} actually plugs the components
of the four-vectors and spinors into the amplitude \Var{amp}.  The
external polarizations \Var{pol} can be given either as a string with
elements \Code{+}, \Code{-}, \Code{0} for right-handed, left-handed, and
longitudinal polarization, or as a list of integers \Code{+1},
\Code{-1}, \Code{0}.

%------------------------------------------------------------------------

\subsection{Checking Ultraviolet Finiteness}%
\index{ultraviolet divergences}

One way of checking ultraviolet finiteness is to replace the one-loop
integrals by their divergent parts and see if the coefficient of the
divergence adds up to zero.  The function \Code{UVDivergentPart} takes 
the $1/(D - 4)$-term of each one-loop integral.  This means that
non-divergent integrals are set to zero.
\biitab
\Code{UVDivergentPart[\Var{expr}]} &
	replace all loop integrals in \Var{expr} by their UV-divergent
	part \\
\Code{UVSeries[\Var{expr}]} &
	similar to \Code{UVDivergentPart}, but perform a series
	expansion in \Code{Dminus4} \\
\Code{Divergence} &
	a symbol representing the dimensionally regularized
	divergence $\Delta = 2/(4 - D)$ \\
\Code{Dminus4} &
	a symbol representing $D - 4$
\etab
\indextt{UVDivergentPart}%
\indextt{UVSeries}%
\indextt{Divergence}%
\indextt{Dminus4}%
To assert UV finiteness of an expression, one can check that the following
expression is true:
\begin{verbatim}
   uvcheck = UVDivergentPart[expr] //Simplify;
   FreeQ[uvcheck, Divergence]
\end{verbatim}
Note that models may have `hidden' parameter relations, \eg
\Code{CW2 = MW2/MZ2} in the Standard Model, which need to be inserted
in order to find analytical cancellation of divergences.

The UV-divergent integrals are
\begin{align*}
A_0(m^2) &= m^2\Delta + \O(1)\,, &
	C_{00} &= \frac{\Delta}{4} + \O(1)\,, \\
A_{00}(m^2) &= \frac{m^4}{4}\Delta + \O(1)\,, &
	C_{00i} &= -\frac{\Delta}{12} + \O(1)\,, \\
B_0 &= \Delta + \O(1)\,, &
	C_{00ii} &= \frac{\Delta}{24} + \O(1)\,, \\
B_1 &= -\frac{\Delta}{2} + \O(1)\,, &
	C_{00ij} &= \frac{\Delta}{48} + \O(1)\,, \\
B_{00}(p^2, m_1^2, m_2^2) &=
	\left(\frac{m_1^2 + m_2^2}{4} - \frac{p^2}{12}\right)\Delta + \O(1)\,, &
	D_{0000} &= \frac{\Delta}{24} + \O(1)\,, \\
\frac{\partial B_{00}(p^2, m_1^2, m_2^2)}{\partial p^2} &=
	  -\frac{\Delta}{12} + \O(1)\,, &
	D_{0000i} &= -\frac{\Delta}{96} + \O(1)\,, \\
B_{11} &=
	\frac{\Delta}{3} + \O(1)\,,
\end{align*}
\vspace*{-5ex}%
%\\[-8ex]%
\begin{gather*}
C_{0000}(p_1^2, p_2^2, p_3^2, m_1^2, m_2^2, m_3^2) =
	\left(\frac{m_1^2 + m_2^2 + m_3^2}{24} -
	  \frac{p_1^2 + p_2^2 + p_3^2}{96}\right)\Delta + \O(1)\,.
\end{gather*}
Alternatively, UV finiteness may be checked numerically by varying the
parameters that regularize the infinity in the loop integrals, $\mu$ and
$\Delta$, on which the final result must not depend.  This is of course
limited to the achievable numerical precision.

%------------------------------------------------------------------------

\subsection{Useful Functions}%
\index{useful functions}

\biitab
\Code{DenCollect[\Var{expr}]} &
	collects terms in \Var{expr} whose denominators are identical
	up to a numerical constant \\
\Code{DenCollect[\Var{expr},\,\Var{wrap}]} &
	additionally applies \Var{wrap} to the collected numerators \\
\Code{TagCollect[\Var{expr},\,\Var{tag},\,\Var{wrap}]} &
	apply \Var{wrap} to the part of \Var{expr} tagged by \Var{tag} \\
\Code{TermCollect[\Var{expr}]} &
	combines terms with common factors, \ie does
	$a b + a c + d \to a (b + c) + d$ \\
\Code{Pool[\Var{expr}]} &
	same as \Code{TermCollect} but different method \\
\Code{Pool[\Var{expr},\,\Var{wrap}]} &
	additionally applies \Var{wrap} to the $b + c$ part \\
\Code{DotSimplify[\Var{f_1},\,\Var{f_2}][\Var{expr}]} &
	simplify \Var{expr} with \Var{f_1} if free of any \Code{NoBracket}
	items, else with \Var{f_2} \\
\Code{OnSize[\Var{n_1},\Var{f_1},\,\Var{n_2},\Var{f_2},\,$\dots$,\,
  \rlap{\Var{f_{\text{def}}}][\Var{expr}]}} & \\
&	returns \Code{\Var{f_1}[\Var{expr}]} if 
	\Code{LeafCount[\Var{expr}] < \Var{n_1}}, \linebreak
	\hphantom{returns} \Code{\Var{f_2}[\Var{expr}]} if
	\Code{LeafCount[\Var{expr}] < \Var{n_2}}, etc., \linebreak
	and \Code{\Var{f_{\text{def}}}[\Var{expr}]} if the expression
	is still larger \\
\Code{ExprHeads[\Var{expr}]} &
	return all non-system symbols and heads in \Var{expr} \\
\Code{ExprParts[\Var{expr},\,\Var{H},\,\Var{h}]} &
	return all subexpressions of \Var{expr} which must contain the 
	symbols and functions in \Var{H} and may contain those in 
	\Var{h} \\
\Code{Creep[\Vf,\,\Var{patt\dots}][\Var{expr}]} &
	applies \Vf\ to subexpressions of \Var{expr} which contain
	only the patterns in \Var{patt} \\
\Code{MapOnly[\Vf,\,\Vh,\,\Var{patt\dots}][\Var{expr}]} &
	maps \Vf\ onto subexpressions of head \Vh\ in \Var{expr}
	which must contain each of \Var{patt} and no other symbols \\
\Code{ApplyUnitarity[\Var{expr},\,\Var{U},\,\Vd,\,\Vs]} &
	simplify \Var{expr} by exploiting the unitarity of the 
	\Vd-dimensional matrix \Var{U}.  The optional argument \Vs\ 
	specifies the simplification function to use internally 
	(default: \Code{FullSimplify}) \\
\Code{ExpandSums[\Var{expr}]} &
	turns all pieces of \Var{expr} multiplied with \Code{SumOver}
	into an actual sum \\
\Code{SplitTerms[\Vf,\,\Var{expr},\,\Vn]} &
	applies \Vf\ to \Var{expr}, \Vn\ terms at a time
\etab
\indextt{DenCollect}%
\indextt{TagCollect}%
\indextt{Pool}%
\indextt{OnSize}%
\indextt{Creep}%
\indextt{MapOnly}%
\indextt{ApplyUnitarity}%
\indextt{ExpandSums}%
\indextt{SplitTerms}

\Code{DenCollect} works essentially like \mma's \Code{Collect}, except 
that it takes the denominators appearing in the expression to collect
for.  Note that this refers to the `real' denominators, \ie parts of 
the expression with negative powers, not to the symbol
\Code{Den[\Var{p},\,\Var{m}]} which stands for propagator denominators 
(the latter can easily be collected with \Code{Collect}).

\Code{TagCollect} collects an expression with respect to powers of a tag
and applies a function to the term linear in the tag.  This function is
typically used to apply a function to the tagged part of an expression,
such as is possible with the \Code{MultiplyDiagrams} function.

\Code{TermCollect} and \Code{Pool} combine terms with common factors.  
Unlike \Code{Factor}, they look at the terms pairwise and can thus do $a b 
+ a c + d \to a (b + c) + d$ fast.  Unlike \Code{Simplify}, they do not 
modify $b$ and $c$.

\Code{DotSimplify} simplifies an expression with two different functions
depending on whether it contains any of the objects listed in the
\Code{NoBracket} option (\ie during the execution of \Code{CalcFeynAmp}
or \Code{PolarizationSum}).

The \Code{OnSize} function is similar to the \Code{Switch} statement, 
but for the size of the expression.  This can be useful when simplifying 
expressions because \Code{Simplify} (even \Code{FullSimplify}) is fast 
and efficient on short expressions, but tends to be slow on large ones.  
For example,
\begin{verbatim}
  OnSize[100, FullSimplify, 500, Simplify, TermCollect]
\end{verbatim}
applies \Code{FullSimplify} if the expression's leaf count is less than 
100, \Code{Simplify} if it is between 100 and 500, and 
\Code{TermCollect} above.

\Code{ExprHeads} and \Code{ExprParts} can be used to determine which 
parts of an expression are eligible \eg for simplification.  
\Code{ExprHeads} returns all non-system symbols and heads of an 
expression, while \Code{ExprParts} picks subexpressions in which only 
selected heads appear.

\Code{Creep} and \Code{MapOnly} have similar functionality: they deliver 
a `payload' function to parts of an expression containing certain 
objects only.
\Code{Creep} `creeps' into an expression and applies its function as 
soon as a subexpression contains only the given patterns.
\Code{MapOnly} is more restrictive about the subexpression: For example, 
\Code{MapOnly[\Vf,\,\Vh,\,\Va|\Vb,\,\Vc][\Var{expr}]} applies \Vf\ to
subexpressions of head \Vh\ which must contain \Vc\ \underline{and} \Va\ 
or \Vb, and may not contain symbols other than \Va, \Vb, \Vc.  
Making $c$ optional,
\Code{MapOnly[\Vf,\,\Vh,\,\Va|\Vb,\,\uscore|\Vc][\Var{expr}]} 
maps \Vf\ onto subexpressions which must contain \Va\ or \Vb\ and must 
not contain variables other that \Va, \Vb, \Vc.

\Code{ApplyUnitarity} exploits unitarity of a given matrix to simplify 
an expression.  The two unitarity relations $UU^\dagger = \unity$ (or
$\sum_{j = 1}^d U_{ij} U_{kj}^* = \delta_{ik}$) and $U^\dagger U =
\unity$ (or $\sum_{j = 1}^d U_{jk} U_{ji}^* = \delta_{ik}$) are used to
substitute sums of $UU^*$ elements with more than $\lceil d/2\rceil$
terms.  For example, \Code{ApplyUnitarity[\Var{expr},\,CKM,\,3]} might 
replace $\Code{CKM}_{12} \Code{CKM}_{13}^* + \Code{CKM}_{22} 
\Code{CKM}_{23}^*$ by $-\Code{CKM}_{32}\Code{CKM}_{33}^*$.

\Code{ExpandSums} turns expressions where index summations are denoted 
by \Code{SumOver[\Vi,\,\Vr]} multiplied with the term to be summed over,
such as appear \eg in the output of \Code{CalcFeynAmp}, into ordinary 
Mathematica sums.  \Code{ExpandSums[\Var{expr},\,\Vh]} uses head \Vh\ 
instead of \Code{Plus}.

\Code{SplitTerms} applies a function to a sum in batches, with at most 
the prescribed number of terms in each invocation.  For all other heads 
the function is applied to the whole expression.


\section{Tools for the Numerical Evaluation}
\index{numerical evaluation}%

The numerical evaluation has to take care of two important issues, and 
therefore resolves into two conceptual steps:
\begin{enumerate}
\item
Almost invariably, the numerical evaluation in \mma\ itself is too slow.
Even though the speed of numerical computations has been improved greatly
in \mma\ 4, the sheer amount of number crunching \eg in a scan over
parameter space places the numerical evaluation safely in the domain of a
compiled language.  This makes it necessary to translate the \mma\
expressions resulting from \Code{CalcFeynAmp}, \Code{HelicityME}, etc.\ into
a high-level language program.

For \FC, Fortran has been chosen because of its proven track record for 
fast and reliable number crunching, the availability of good compilers 
on all platforms, and the possibility to link with existing code.  
Code generation in C99 is available, too.

This first step of writing out a subroutine from the calculated 
amplitudes can be implemented in a reasonably general way and is handled 
by \FC\ itself, see Sect.\ \ref{sect:fortran}.

\item
What \FC\ generates is just the subroutine for computing the squared
matrix element.  Of course, this subroutine has to be provided with the
proper kinematics, the model parameters have to be initialized, etc.  In
other words, a driver program is required to invoke the generated
subroutine.  Unfortunately, it is not really possible to write one single
fully automated driver program that fits each and every purpose, or
rather, such a program would be difficult to use beyond its designed
scope.

The driver programs that come with \FC\ together are a powerful tool for
computing cross-sections, but moreover they have expressly been written
to give an example of how the generated code can be used.  Extending or
modifying this code for a related purpose should be fairly
straightforward.
\end{enumerate}

The following files in the \FC\ distribution are used for the numerical 
evaluation:
\begin{tabbing}
\Code{drivers/}\hspace{.18\linewidth} \=
	driver programs for running the generated code: \\
\quad\Code{process.h} \>
	the process specification \\
\quad\Code{run.F} \>
	the parameters for a particular ``run'' \\
\quad\Code{partonic.h} \>
	the partonic composition of the final result \\
\quad\Code{extra.h} \>
	extra user definitions \\
\quad\Code{distrib.h} \>
	definitions for distributed computing \\
\quad\Code{makefile.in} \>
	the makefile minus the flags set by configure \\
\quad\Code{configure} \>
	a script to find out library locations and compiler flags \\
\quad\Code{do} \>
	a script to automate computations
\\[1ex]
\Code{drivers/F/} \>
	Fortran driver programs for running the generated code: \\
\quad\Code{main.F} \>
	the main program (command-line processing) \\
\quad\Code{xsection.F,.h} \>
	the code for computing the cross-section \\
\quad\Code{parton.h} \>
	setup for a single partonic process \\
\quad\Code{num.h} \>
	definitions for the OPP numerator functions \\
\quad\Code{lumi\uscore *.F} \>
	luminosity computation \\
\quad\MtoN.F,.h \>
	the kinematics for a $\Var{M}\to\Var{N}$ process \\
\quad\Code{softphoton.F} \>
	the integrals for the soft-photon approximation \\
\quad\Code{util.h} \>
	definitions for the functions in \Code{util.a} \\
\quad\Code{const.h} \>
	general constants and regularization parameters \\
\quad\Code{inline.h} \>
	inline versions of the util functions \\
\quad\Code{contains.h} \>
	inline versions of the util functions \\
\quad\Code{user.h} \>
	user definitions, \eg choice of model, OPP, etc. \\
\quad\Code{decl.h} \>
	combined declarations file, included `everywhere'
\\[1ex]
\Code{drivers/C/} \>
	C driver programs for running the generated code: \\
\quad\Code{num.h} \>
	definitions for the OPP numerator functions \\
\quad\Code{model\uscore *.h} \>
	declarations of the model parameters \\
\quad\Code{util.h} \>
	definitions for the functions in \Code{util.a} \\
\quad\Code{const.h} \>
	general constants and regularization parameters \\
\quad\Code{inline.h} \>
	inline versions of the util functions \\
\quad\Code{contains.h} \>
	inline versions of the util functions \\
\quad\Code{user.h} \>
	user definitions, \eg choice of model, OPP, etc. \\
\quad\Code{decl.h} \>
	combined declarations file, included `everywhere'
\\[1ex]
\Code{drivers/models/} \>
	code for model initialization: \\
\quad\Code{model\uscore *.F,.h} \>
	initialization of the stock models
\\[1ex]
\Code{drivers/tools/} \>
	helper scripts: \\
\quad\Code{mktm} \>
	a script to set up Mathematica interfacing code (internal use) \\
\quad\Code{data} \>
	a script for extracting the actual data out of the log files \\
\quad\Code{pnuglot} \>
	a script for plotting data files using gnuplot \\
\quad\Code{sfx} \>
	a script for making a self-extracting archive \\
\quad\Code{turnoff} \>
	a script to turn on or off the evaluation of code modules \\
\quad\Code{submit} \>
	a script to submit parallel jobs
\\[1ex]
\Code{tools/} \>
	tools for the numerical evaluation: \\
\quad\Code{ReadData.tm} \>
	a program for reading data files into \mma \\
\quad\Code{reorder.c} \>
	a utility to reorganize parameters in data files \\
\quad\Code{ScanGraphics.m} \>
	special \mma\ graphics functions for parameter scans \\
\quad\Code{bfunc.m} \>
	the explicit expressions for the one- and two-point functions \\
\quad\Code{btensor.m} \>
	the explicit tensor reduction for the one- and two-point \\
\>	functions in \mma
\end{tabbing}%
\indextt{drivers}%
\indextt{main.F,.h}%
\indextt{xsection.F,.h}%
\indextt{process.h}%
\indextt{run.F}%
\indextt{model\uscore *.F}%
\indextt{lumi\uscore *.F}%
\indextt{1to2.F,.h}%
\indextt{1to3.F,.h}%
\indextt{2to2.F,.h}%
\indextt{2to3.F,.h}%
\indextt{2to4.F,.h}%
\indextt{configure}%
\indextt{makefile.in}%
\indextt{mktm}%
\indextt{data}%
\indextt{pnuglot}%
\indextt{sfx}%
\indextt{turnoff}%
\indextt{submit}%
\indextt{tools}%
\indextt{ReadData.tm}%
\indextt{reorder.c}%
\indextt{ScanGraphics.m}
\indextt{bfunc.m}%
\indextt{btensor.m}

%------------------------------------------------------------------------

\subsection{Generating code}%
\label{sect:fortran}%
\index{Fortran code}%
\index{C code}%
\index{code generation}

In most cases, the numerical evaluation of an amplitude directly in \mma\
is too slow, so the \FC\ results need to be converted to a Fortran or C
program.  The simplest way to do this in \mma\ is something like
\Code{FortranForm[result] >> file.f}.  Unfortunately, this leaves a lot to
be done manually.

\indextt{WriteSquaredME}%
\indextt{WriteRenConst}%
\index{squared matrix element}%
\index{renormalization constants}%
\FC\ contains two much more sophisticated functions for generating code, 
\Code{WriteSquaredME} and \Code{WriteRenConst}.  The philosophy is that 
the user should not have to modify the generated code.  This eliminates 
the source of many errors, but of course goes well beyond simple 
translation to code: the code has to be encapsulated (\ie no loose ends 
the user has to bother with), and all necessary subsidiary files, such 
as include files and a makefile, have to be produced.  Also, the code 
has to be chopped into pieces small enough that the compiler will accept 
them.

Before using \Code{WriteSquaredME} or \Code{WriteRenConst}, a directory 
must be created for the code and the driver programs copied into this 
directory.  This is done with \Code{SetupCodeDir}:
\biitab
\Code{SetupCodeDir[\Var{dir}]} &
	create a directory \Var{dir} and install the driver programs
	necessary to compile the generated code in this directory
\etab
\indextt{SetupCodeDir}%
Note that \Code{SetupCodeDir} never removes or overwrites the directory 
or the driver programs, so a code directory, once created, stays put.
The \Code{Drivers} option can be used to load customized driver programs
from a local directory.
\biiitab{Option}
\Code{Model} & \Code{\$Model} &
	the model used for the calculation, the presently initialized
	one by default \\
\Code{Folder} & \Code{models} &
	the subdirectory of the generated directory into which
	the model initialization code is written \\
\Code{FileHeader} & \Code{FileHeader} &
	the header for generated code files, same as for
	\Code{WriteSquaredME} by default \\
\Code{Drivers} & \Code{"drivers"} &
	a directory containing customized versions of the driver 
	programs which take precedence over the default ones in
	\Code{\$DriversDir}
\etab
\indextt{Model}%
\indextt{Folder}%
\indextt{FileHeader}%
\indextt{Drivers}%
\indextt{\$DriversDir}%
To illustrate the concept, consider generating code for a particular 
scattering process.  The first time, no customized drivers exist and 
hence \FC\ copies the default drivers into the code directory.  The user 
then modifies, say, \Code{run.F}.  To preserve the changes, he copies 
the file to a local directory, \eg \Code{drivers}, and specifies this 
directory with the \Code{Drivers} option.  Now even if the code 
directory is deleted, generating the code anew will recover all 
modifications because the customized version of \Code{process.h} in 
\Code{drivers} supersedes the default \Code{run.F} in 
\Code{\$DriversDir}.

\index{model initialization}%
\index{FeynRules}%
\Code{SetupCodeDir} also sets up the model initialization, which
consists of three files:
\begin{itemize}
\item \Code{models/model.F}, which contains the initialization routines
  of Sect.~\ref{sec:numpar},
\item \Code{models/model.Fh}, model declarations for Fortran code,
\item \Code{models/model.ch}, model declarations for C code.
\end{itemize}
For \FA's stock models the generated initialization merely uses/combines
the ready-made files in \Code{drivers/models}.

Models created with the \FA\ interface of \textit{FeynRules}
\cite{Al09,Al14} come with a \Code{\Var{model}.pars} file containing 
numerical values for model parameters, and also the model file
\Code{\Var{model}.mod} itself uses shorthands in coupling expressions, 
defined in the variable \Code{FA\$Couplings}.

\emph{Important:} the numbers listed in the \Code{.pars} file are 
usually tree-level values and may need to be adapted depending on the 
renormalization scheme, which is in the sole responsibility of the
user.

The generation of initialization code is silently skipped if neither
stock initialization files nor a \Code{.pars} file is found on the
\Code{\$ModelPath}.

\smallskip

The function \Code{WriteSquaredME} assumes that there is a tree-level
amplitude $\Mtree$ and a one-loop amplitude $\Mloop$ and writes out a
subroutine called \Code{SquaredME} which computes the two expressions
$|\Mtree|^2$ and $2\Re\Mtree^*\Mloop$.  When there is no tree-level
contribution, $0$ and $|\Mloop|^2$ are returned, and when there is no
one-loop contribution, $|\Mtree|^2$ and 0 are returned.
\biitab
\ownline{\Code{WriteSquaredME[\Var{tree},\,\Var{loop},\,%
\Var{mat},\,\Var{abbr},\,$\dots$,\,\Var{dir}]}} \\
&	generate code for a subroutine \Code{SquaredME} in the
	directory \Var{dir}; this subroutine computes the squared
	matrix element composed of the amplitudes \Var{tree} (for the
	tree-level part) and \Var{loop} (for the one-loop part)
\etab
The \Var{tree} and \Var{loop} arguments are results of
\Code{CalcFeynAmp} and are trailed by all other objects necessary for
the computation of the squared matrix element, like helicity matrix
elements, colour matrix elements, and the abbreviations.  An empty list,
\Code{\Brac{}}, is used to indicate that the \Var{tree} or \Var{loop} 
argument is missing. Since \Code{WriteSquaredME} has a rather 
complicated invocation, it is perhaps best to give an example:
\begin{verbatim}
   born = CalcFeynAmp[bornamps];
   self = CalcFeynAmp[selfamps];
   vert = CalcFeynAmp[vertamps];
   box = CalcFeynAmp[boxamps];
   col = ColourME[All, born];
   dir = SetupCodeDir["fortran"];
   WriteSquaredME[born, {self, vert, box}, col, Abbr[], dir]
\end{verbatim}
The \Var{tree} and \Var{loop} arguments of \Code{WriteSquaredME} have
the somewhat unorthodox feature that the naming of the code modules
depends on the variable names chosen for the components of the
amplitude (like \Code{born}, \Code{self}, etc., in the example above). 
That is, \Code{WriteSquaredME} tries to name the code modules like
the variables, \eg the amplitude contained in the variable \Code{self}
will be written to files of the form \Code{self*.F}.  This is only
possible if the variable names are plain symbols; in all other cases,
the modules are named \Code{Tree1.F}, \Code{Tree2.F}, or \Code{Loop1.F},
\Code{Loop2.F}, etc.

The following options can be used with \Code{WriteSquaredME}.
\biiitab{option}
\Code{ExtraRules} & \Code{\Brac{}} &
	extra rules to be applied to the expressions before the loop
	integrals are abbreviated \\
\Code{TreeSquare} & \Code{\$TreeSquare} (default: \Code{True}) &
	whether the square of the tree-level amplitude is added to
	the result \\
\Code{LoopSquare} & \Code{\$LoopSquare} (default: \Code{False}) &
	whether the square of the one-loop amplitude is added to
	the result \\
\Code{Folder} & \Code{"squaredme"} &
	the subdirectory of the code directory into which the
	generated code is written \\
\Code{FilePrefix} & \Code{""} &
	a string prepended to the filenames of the generated code \\
\Code{SymbolPrefix} & \Code{""} &
	a string prepended to global symbols to prevent collision
	of names when more than one process is linked \\
\Code{FileHeader} & \Code{"\#if 0\backsl n\backsl} &
	the file header \\[-\parskip]
	& \Code{* \%f\backsl n\backsl} \\[-\parskip]
	& \Code{* \%d\backsl n\backsl} \\[-\parskip]
	& \rlap{\Code{* generated by FormCalc \Vm.\Vn\ \%t\backsl n\backsl}} \\[-\parskip]
	& \rlap{\Code{\#endif\backsl n\backsl n"}} \\
\Code{SubroutineIncludes} & \Code{FileIncludes} &
	per-subroutine \Code{\#include} statements \\
\Code{FileIncludes} & &
	per-file \Code{\#include} statements \\[-\parskip]
	& \rlap{\Code{\lbrac"\#include \backsl"decl.h\backsl",}} \\[-\parskip]
	& \rlap{\Code{~"\#include \backsl"inline.h\backsl"\backsl n",}} \\[-\parskip]
	& \rlap{\Code{~"\#include \backsl"contains.h\backsl"\backsl n"\rbrac}}
\etab%
\indextt{ExtraRules}%
\indextt{TreeSquare}%
\indextt{\$TreeSquare}%
\indextt{LoopSquare}%
\indextt{\$LoopSquare}%
\indextt{Folder}%
\indextt{FilePrefix}%
\indextt{SymbolPrefix}%
\indextt{FileHeader}%
\indextt{SubroutineIncludes}%
\indextt{FileIncludes}

\Code{ExtraRules} specifies additional transformation rules that are 
applied to the modules of the amplitude before the loop integrals are 
abbreviated.

\Code{TreeSquare} determines whether the square of the tree-level
amplitude is added to the result.  This is the case unless one needs to
compute only the interference term, usually for testing.

\Code{LoopSquare} determines whether the square of the one-loop
amplitude is added to the one-loop result, \ie whether the
\Code{SquaredME} subroutine shall compute $2\Re\Mtree^*\Mloop +
|\Mloop|^2$ rather than only $2\Re\Mtree^*\Mloop$.  Without the
$|\Mloop|^2$ term (which completes the square $|\Mtree + \Mloop|^2$),
the one-loop result may become negative if $\Mtree$ is very small.  The
contribution of the same order from the two-loop result, $2\Re\Mtree^*
\M_{\text{2-loop}}$, is much smaller than $|\Mloop|^2$ in this case so
that there is no inconsistency with the order in perturbation theory.

\Code{Folder} specifies the name of the subdirectory relative to the
code directory into which the generated code is written.  When using
this option, the default makefile will usually have to be adapted
accordingly.

\Code{FilePrefix} specifies a string which will be prepended to the
names of all generated files.  This complements the \Code{Folder} option
as another way of making files unique.

\Code{SymbolPrefix} is a way of avoiding symbol collisions when more than
one process generated by \Code{WriteSquaredME} is linked together.  The
given string must be acceptable to Fortran or C as it is prepended to all
global symbols like the names of subroutines and common blocks.

\Code{FileHeader} specifies a string to be used as file header.  This 
string may contain \Code{\%f}, \Code{\%d}, and \Code{\%t}, which are 
substituted at file creation by file name, description, and time stamp, 
respectively.

\Code{SubroutineIncludes} gives declarations to be inserted in the 
declaration section of each subroutine, usually in the form of 
\Code{\#include} statements.  In the most general form a list of three 
strings is given, the first of which included before, the second after 
local variable declarations, and the third at the end of the routine.  
It is admissible to provide a list with fewer strings, or just a string 
(without a list), in which case the unspecified ones remain empty.

\Code{FileIncludes} gives declarations to be inserted at the beginning 
of each generated file.  It accepts the same input as 
\Code{SubroutineInclude} except that there is no subroutine relative to 
which the placement can be made.  Instead, the first list element goes 
into the central include file \Code{vars.h}, the other two into the 
individual code files, after the \Code{\#include "vars.h"} statement.

\biitab
\Code{SetLanguage["Fortran"]} &
	set the output language to Fortran (default) \\
\Code{SetLanguage["C"]} &
	set the output language to C99 \\
\Code{SetLanguage[\Var{lang},\,"novec"]} &
	generate unvectorized code
\etab
\indextt{SetLanguage}%
\indextt{novec}%
\index{C output}%
\index{Fortran output}%
\index{vectorization}%
\index{language choice}

%------------------------------------------------------------------------

\subsubsection{Libraries and Makefiles}

The code is organized in \FC\ into a main code directory, which contains 
the main program and all its prerequisite files, and subsidiary 
`folders' (subdirectories to the main code directory).  The default 
setup looks like this:
\begin{center}
\begin{picture}(300,145)(0,-40)
\Text(55,92)[b]{main code directory}
\Text(55,80)[b]{\small (created by \Code{SetupCodeDir})}
\Line(-5,77)(115,77)
%
\Line(65,77)(65,62)
\Line(65,62)(75,62)
\Text(80,63)[l]{\Code{squaredme/}}
\Text(80,52)[l]{\Blue{\Code{squaredme.a}}}
\Text(160,63)[l]{\small (generated by \Code{WriteSquaredME})}
%
\Line(55,77)(55,37)
\Line(55,37)(75,37)
\Text(80,38)[l]{\Code{renconst/}\hphantom{p}}
\Text(80,27)[l]{\Blue{\Code{renconst.a}}\hphantom{p}}
\Text(160,38)[l]{\small (generated by \Code{WriteRenConst})}
%
\Line(45,77)(45,12)
\Line(45,12)(75,12)
\Text(80,13)[l]{\Code{util/}\hphantom{p}}
\Text(80,2)[l]{\Blue{\Code{util.a}}\hphantom{p}}
%
\Line(35,77)(35,-12)
\Line(35,-12)(75,-12)
\Text(80,-11)[l]{\Code{F/}\hphantom{p}}
%
\Line(25,77)(25,-25)
\Line(25,-25)(75,-25)
\Text(80,-24)[l]{\Code{C/}\hphantom{p}}
%
\Line(15,77)(15,-37)
\Line(15,-37)(75,-37)
\Text(80,-36)[l]{\Code{tools/}\hphantom{p}}
%
\Text(145,-13)[l]{$\left.\vrule width 0pt depth 5.5ex height 5.5ex\right\}$}
\Text(160,-13)[l]{\small (come with \FC)\hphantom{p}}
\end{picture}
\end{center}
Folders equipped with an own makefile produce a library of the same 
name, \eg the makefile in \Code{util/} makes the library \Code{util.a}.
These sub-makefiles are orchestrated by the master makefile.  Libraries
required for the main program are listed in the \Code{LIBS} variable
and built automatically by invoking the sub-makefiles:
\begin{verbatim}
  LIBS = squaredme.a renconst.a util.a
\end{verbatim}
\indextt{makefile}%
\indextt{LIBS}%
Note that \Code{configure} overwrites \Code{makefile}, hence `permanent' 
changes should be made in \Code{makefile.in} since \Code{configure} 
overwrites \Code{makefile}.

The \Code{util} library is a collection of ancillary routines which
currently includes:
\begin{itemize}
\item System utilities (log file management),
\item Kinematic functions (\Code{Pair}, \Code{Eps}, $\dots$),
\item Diagonalization routines (\textsc{Diag} library \cite{Ha06}),
\item Univariate integrators (\Code{Gauss}, \Code{Patterson}),
\item Multivariate integrators (\cuba\ library \cite{Ha04}).
\end{itemize}
The \Code{util.a} library is compiled once when \FC\ is installed and 
then copied to the main code directory, thus avoiding unnecessary 
compiles.%
\indextt{util.a}

%------------------------------------------------------------------------

\subsubsection{Partonic Composition}%
\index{partonic process}

Partonic processes can be combined in the final result.  The number of 
incoming and outgoing legs must be the same, so that the phase-spaces 
have identical dimension for integration, but otherwise each partonic 
process may have distinct kinematics.  It is up to the user to ensure
that the combined result makes sense physically.

Technically one invokes \Code{WriteSquaredME} for each partonic process, 
but with different values for the \Code{Folder} and \Code{SymbolPrefix} 
option.  The \Code{Folder} option chooses different output directories 
and the \Code{SymbolPrefix} option disambiguates globally visible 
symbols in the generated code.  Both options admit the use of 
\Code{ProcName}, as in: \Code{Folder -> \Brac{"squaredme", ProcName}} 
which will be substituted by the process name, suitably arranged as file 
and symbol name.  The above folder name might be expanded as
\Code{squaredme/Fb21F21Fb33iF33i}.%
\indextt{Folder}%
\indextt{SymbolPrefix}%
\indextt{ProcName}

The actual composition of the final result is determined through the 
\Code{partonic.h} header in the drivers directory.  The default setup, 
for an only partonic process, is
\indextt{partonic.h}%
\begin{verbatim}
#define NPID 1

*** BEGIN PARTONIC PROCESS #1

#define PID 1
#define PARTON1 -1
#define PARTON2 -1
#include "squaredme/specs.h"
#include "parton.h"

*** END PARTONIC PROCESS #1
\end{verbatim}
\indextt{NPID}%
\indextt{PID}%
\indextt{PARTON1,2}%
\indextt{specs.h}%
\indextt{PDG code}%
For more than one partonic process, \Code{NPID} has to be increased 
appropriately and the lines between \Code{BEGIN} and \Code{END PARTONIC 
PROCESS} have to be replicated for each partonic process, with the 
following quantities adapted:
\begin{itemize}
\item \Code{PID} is the running number of the partonic process, counting 
from 1 to \Code{NPID} (though not necessarily in ascending order),

\item \Code{PARTON1} and \Code{PARTON2} identify the incoming partons in 
\Code{lumi\uscore hadron.F} by their PDG code: 0 = gluon, 1 = down, 2 = 
up, 3 = strange, 4 = charm, 5 = bottom, 6 = top.

\item The \Code{specs.h} file from the correct directory (cf.\ 
\Code{Folder} option above) has to be included.

\item The \Code{parton.h} stays put, \ie is the same for each partonic 
process.
\end{itemize}

%------------------------------------------------------------------------

\subsubsection{Specifying model parameters}
\label{sec:numpar}

Numerical model parameters are specified in the model initialization file.
The initialization file must implement three subroutines:
\begin{verbatim}
        subroutine ModelDefaults
\end{verbatim}
sets all model parameters to their initial values.  This subroutine is
invoked before the user gets to set the input parameters.
\begin{verbatim}
        subroutine ModelConstIni(fail)
        integer fail
\end{verbatim}
initializes the constant model parameters, \ie the ones that do not depend 
on $\sqrt s$, the center-of-mass energy.  If the routine determines that 
the input parameters yield unphysical or otherwise unacceptable model 
parameters, it must return a non-zero value in \Code{fail}.
\begin{verbatim}
        subroutine ModelVarIni(fail, sqrtS)
        integer fail
        double precision sqrtS
\end{verbatim}
is called after \Code{ModelConstIni} to initialize the model parameters 
that depend on $\sqrt s$, typically $\alpha_s$ and derived quantities.
Finally, the
\begin{verbatim}
        subroutine ModelDigest
\end{verbatim}
is invoked to print a digest of the model parameters.%
\indextt{ModelConstIni}%
\indextt{ModelVarIni}%
\indextt{ModelDigest}%
\indextt{model\uscore sm.F}%
\indextt{model\uscore mssm.F}%
\indextt{model\uscore thdm.F}%
\index{parameters}%
\index{numerical parameters}

Note one detail: some model constants are declared in 
\Code{model\uscore\Var{x}.h} with \Code{parameter} statements, \ie as 
numerical constants. \Code{WriteSquaredME} can optimize the generated 
code based on the information which symbols are numerical constants to
the compiler and which are variables.

The trick is to move the numerical constants to the front of each 
product so that the compiler can combine these constants into one 
number, thus reducing the number of multiplications at run time.  For 
example, if \Code{WriteSquaredME} knows \Code{Alfa2} and \Code{SW2} to 
be numerical constants, it will write 
\Code{(3*Alfa2)/(8.D0*SW2**2)*(Abb1*AaA0i44*MH2)} instead of \mma's 
default ordering \Code{(3*Abb1*Alfa2*AaA0i44*MH2)/(8.D0*SW2**2)}.  While 
this may seem a minor rearrangement, the increase in execution speed can 
be 30\%!

To tell \Code{WriteSquaredME} which symbols are numerical constants to
the compiler, simply assign the symbol a numerical value like in (this
statement is near the end of \Code{FormCalc.m})
\begin{verbatim}
  Scan[ (N[#] = Random[])&, {Alfa, Alfa2, MW, MW2, ...} ]
\end{verbatim}

%------------------------------------------------------------------------

\subsubsection*{Details of the code generation}

The following excerpt is from a Fortran code generated by 
\Code{WriteSquaredME}:
\begin{verbatim}
        subroutine vert
        implicit none

#include "vars.h"
#include "inline.h"

        Cloop(HelInd(1)) = Cloop(HelInd(1)) + 
     &    1/(4.D0*CW2**3*SW2**2)*
     &     ((8*MW2*(Pair1*Pair2*Sub437) - Alfa2*CW2**3*Sub445)/
     &        (MH2 - S) + (8*MW2*(Pair3*Pair4*Sub437) - 
     &          Alfa2*CW2**3*Sub453)/(MH2 - T) + 
     &       (8*MW2*(Pair5*Pair6*Sub437) - Alfa2*CW2**3*Sub461)/
     &        (MH2 - U))

#include "contains.h"

        end
\end{verbatim}
Note the use of the C preprocessor in statements like \Code{\#include}.
Because of this, the code files have the extension \Code{.F} which is
recognized by almost all Fortran compilers.

\Code{WriteSquaredME} pulls the calculation of several kinds of objects
out of the actual computation of the amplitude.  That is, these objects
are calculated first and stored in variables, and then the amplitude is
computed.  Because the replacement of more complex objects by scalar
variables shortens the expressions, these objects are referred to as
abbreviations.

The following objects are treated as abbreviations:
\begin{enumerate}
\item
variables replacing some function call: \Code{Pair\Vn} (dot products),
\Code{Eps\Vn} (contracted epsilon tensors), \Code{F\Vn} (fermion 
chains), \Code{Aa\VX0i\Vn} (loop integrals),

\item
products or sums of variables of type 1: \Code{Abb\Vn},
\Code{AbbSum\Vn},

\item
matrix elements: \Code{Mat\Var{Type}(\Vi,\,\Vj)}.
\end{enumerate}
All of these abbreviations except the \Code{Aa\VX0i\Vn} are in fact the 
ones already introduced by \FC.  These abbreviations share the general 
feature of being costly in CPU time (in particular the loop integrals), 
which means that for optimal performance they should be calculated as 
few times as possible.  \Code{WriteSquaredME} splits them into $2\times 
3$ categories:
\begin{itemize}
\item \Code{abbr0\uscore\Var{cat}.F} -- tree-level, and
\item \Code{abbr1\uscore\Var{cat}.F} -- one-loop,
\item \Code{abbr\Vi\uscore s.F} -- abbreviations which depend only on $\sqrt s$,
\item \Code{abbr\Vi\uscore angle.F} -- abbreviations which depend also on  
      other phase-space variables, but not on the helicities, nad
\item \Code{abbr\Vi\uscore hel.F} -- helicity-dependent abbreviations.
\end{itemize}
Needless to say, each of these \Code{abbr\Vi\uscore\Var{cat}} modules is
invoked only when necessary.%
\indextt{abbr0\uscore\Var{cat}}%
\indextt{abbr1\uscore\Var{cat}}%
\indextt{abbr\Vi\uscore s}%
\indextt{abbr\Vi\uscore angle}%
\indextt{abbr\Vi\uscore hel}

\indextt{SquaredME}%
All modules generated by \Code{WriteSquaredME} are invoked in the proper
fashion by the master subroutine \Code{SquaredME}, as shown in the 
following figure.
\begin{center}
\fbox{\includegraphics{SquaredME}}
\end{center}

As long as one sticks to the default driver programs, one does not have
to worry about actually calling \Code{SquaredME}.  For certain
applications (\eg Monte Carlo generators) it might nevertheless be
attractive to invoke \Code{SquaredME} directly.  Its declaration is
\begin{verbatim}
        subroutine SquaredME(result, helicities, flags)
        double precision result(2)
        integer*8 helicities
        integer flags
\end{verbatim}
The results are returned in \Code{result}, \Code{helicities} encodes the
helicities to include, and \Code{flags} is an integer specifying flags 
for the computation.

The results of \Code{SquaredME} are $\Code{result(1)} = |\Mtree|^2$ 
and $\Code{result(2)} = 2\Re\Mtree^* \Mloop$, except when there is no 
tree-level contribution, in which case $\Code{result(1)} = 0$ and 
$\Code{result(2)} = |\Mloop|^2$ is returned.

The helicities are encoded bitwise in the integer argument
\Code{helicities}.  For each external particle, five bits represent,
from most to least significant bit, right-circular (spin 2 or 3/2),
right-circular (spin 1 or 1/2), longitudinal, left-circular (spin 1 or
1/2) and left-circular (spin 2 or 3/2) polarization:
$$
\begin{array}{cccccccccccc}
       & \circlearrowright
       & \curvearrowright
       & \leftrightarrow
       & \curvearrowleft
       & \circlearrowleft
       &
       & \circlearrowright
       & \curvearrowright
       & \leftrightarrow
       & \curvearrowleft
       & \circlearrowleft \\
\cline{2-12}
\kern -1ex\Code{helicities}
       & \multicolumn{1}{|c}{b_{5n-1}}
	& b_{5n-2} & b_{5n-3} & b_{5n-4}
         & \multicolumn{1}{c|}{b_{5n-5}}
       & \multicolumn{1}{c|}{\ldots}
       & b_4 & b_3 & b_2 & b_1
	& \multicolumn{1}{c|}{b_0} \\
\cline{2-12} \\[-6ex]
       & \multicolumn{5}{c}{\underbrace{\hspace*{17em}}_
            {\text{Particle 1}}}
       &
       & \multicolumn{5}{c}{\underbrace{\hspace*{10em}}_
            {\text{Particle $n$}}} \\
\end{array}
$$
For instance, a left- and right-circularly polarized fermion scattering 
into an unpolarized two-vector-boson final state would be given by 
$\Code{helicities} = 00010\,01000\,01110\,01110_2 = 74190_{10}$.  Some 
compilers allow binary notation directly as 
\Code{B'00010\,01000\,01110\,01110'} (this is non-portable, however).

Currently three flags are passed:
\begin{itemize}
\item
Bit 0 is the `set mass' flag: if 1, the masses of the external particles 
are returned in the `\Code{result}' argument.  No cross-section is 
computed in this case.

\item
Bit 1 is the `reset' flag: if 1, the abbreviations must be re-calculated 
completely.  The latter is the case when the center-of-mass energy or 
the model parameters have changed.

\item
Bit 2 is the `loop' flag: if 1, the loop corrections are computed.
This flag allows \eg to integrate phase space separately for tree-level 
and one-loop part.
\end{itemize}

The following picture shows the default constellation of driver programs
and code modules generated by \Code{WriteSquaredME}:
\begin{center}
\includegraphics[scale=.9]{WriteSquaredME}
\end{center}%
\indextt{SquaredME.F}%
\indextt{main.F,.h}%
\indextt{run.F}%
\indextt{xsection.F}%
\indextt{process.h}

%------------------------------------------------------------------------

\subsection{Running the Generated Code}

The code produced by \Code{WriteSquaredME} needs in addition a driver 
program which supplies it with the necessary parameters, kinematics, 
etc.  The default driver program that comes with \FC\ is organized in a 
very modular way and spreads out over several files: Technically, the 
main file is \Code{run.F}, \ie this is the file the compiler sees as 
main program.  However, \Code{run.F} is just a clever way of defining 
parameters at the right places, and the real work is done in other files 
which are included by \Code{run.F}.%
\index{driver program}

There is almost no cross-talk between different modules which are in 
that sense `universal.'  The actual main program, \Code{main.F}, only 
scans the command line and invokes
\begin{verbatim}
   call ProcessIni(...)
   call ParameterScan(...)
\end{verbatim}
All further action is decoupled from the main program and can easily be
called from any application.  It is thus relatively straightforward to
use \FC-generated code in own programs.

The distribution of the code over the various include files and their
interdependencies is shown in the following figure, where arrows indicate
code inclusion via \Code{\#include}.
\begin{center}
\includegraphics{DependenceTree}
\end{center}%
\index{dependences}%
Organizing the code in this seemingly entangled, but highly modular way
makes it possible for one program to perform many different tasks simply
by setting preprocessor flags.  The different modules have the following
duties:
\begin{itemize}
\item	\Code{run.F} defines a ``run,'' \eg the ranges over which to
	scan model parameters,
\item	\Code{process.h} defines all process-dependent parameters,
\item	\Code{main.F} does the command-line parsing,
\item	\Code{xsection.F} contains the kinematics-independent code, \\
	\Code{xsection.h} contains the declarations for 
	\Code{xsection.F},
\item	\Code{partonic.h} determines the partonic composition of the
	result, \\
	\Code{parton.h} contains the code for a single partonic process,
\item	\MtoN.F contains the kinematics-dependent
	code for a $\Var{M}\to\Var{N}$ process, \\
	\MtoN.h contains the declarations for \MtoN.F,
\item	\Code{model\uscore\Var{x}.F} (currently one of
	\Code{model\uscore sm.F}, \Code{model\uscore mssm.F},
	\Code{model\uscore thdm.F}) initializes the model parameters, \\
	\Code{model\uscore\Var{x}.h} contains the declarations for 
	\Code{model\uscore\Var{x}.F},
\item	\Code{lumi\uscore\Var{y}.F} (currently one of
	\Code{lumi\uscore parton.F}, \Code{lumi\uscore hadron.F},
	\Code{lumi\uscore photon.F}) calculates the parton luminosity, \\
	\Code{lumi\uscore\Var{y}.h} contains the declarations for
	\Code{lumi\uscore\Var{y}.F},
\item	\Code{util.h} contains the declarations for the functions in the 
	util library,
\item	\Code{looptools.h} is the \LT\ include file,
\item	\Code{renconst.h} declares the renormalization constants (see
	Sect.\ \ref{sect:renconst}).
\end{itemize}%
\indextt{run.F}%
\indextt{process.h}%
\indextt{main.F,.h}%
\indextt{1to2.F,.h}%
\indextt{2to2.F,.h}%
\indextt{2to3.F,.h}%
\indextt{model\uscore sm.F}%
\indextt{model\uscore mssm.F}%
\indextt{model\uscore thdm.F}%
\indextt{lumi\uscore parton.F}%
\indextt{lumi\uscore hadron.F}%
\indextt{lumi\uscore photon.F}%
\indextt{looptools.h}%
\indextt{renconst.h}

In this setup, the choice of parameters is directed by the two files 
\Code{process.h} and \Code{run.F}, which include one each of
\begin{itemize}
\item Kinematics definitions: \\
      \Code{1to2.F}, \\
      \Code{2to2.F}, \\
      \Code{2to3.F},

\item Convolution with PDFs: \\
      \Code{lumi\uscore parton.F}, \\
      \Code{lumi\uscore hadron.F}, \\
      \Code{lumi\uscore photon.F}, 

\item Model initialization: \\
      \Code{model\uscore sm.F}, \\
      \Code{model\uscore mssm.F}, \\
      \Code{model\uscore thdm.F}.
\end{itemize}

%------------------------------------------------------------------------

\subsubsection{Process definition}
\index{process definition}

All process-specific definitions are given in \Code{process.h}.  The
external particles are declared in the lines
\begin{alltt}
#define TYPE\Vi \(t\sb{\Vi}\)
#define MASS\Vi \(m\sb{\Vi}\)
#define CHARGE\Vi \(c\sb{\Vi}\)
\end{alltt}
\indextt{TYPE\Vi}%
\indextt{MASS\Vi}%
\indextt{CHARGE\Vi}%
\indextt{SCALAR}%
\indextt{FERMION}%
\indextt{VECTOR}%
\indextt{PHOTON}%
where each $t_\Vi$ is one of the symbols \Code{SCALAR}, \Code{FERMION},
\Code{VECTOR}, \Code{PHOTON}, or \Code{GLUON}.  \Code{PHOTON} is the
same as \Code{VECTOR} except that longitudinal polarization states are
not allowed and \Code{GLUON} is just an alias for \Code{PHOTON}.

As in \FC, the momenta, masses, and polarization vectors are numbered
sequentially like in the following figure.
\begin{center}
\begin{picture}(190,80)(-55,0)
\ArrowLine(0,10)(27,27)
\ArrowLine(0,70)(27,53)
\ArrowLine(53,53)(80,70)
\ArrowLine(60,42)(80,45)
\GCirc(40,40){20}{.9}
\Text(-5,70)[rb]{$k_1$, $m_1$, $\varepsilon_1$}
\Text(-5,11)[rt]{$k_2$, $m_2$, $\varepsilon_2$}
\Text(85,70)[lb]{$k_3$, $m_3$, $\varepsilon_3$}
\Text(85,45)[l]{$k_4$, $m_4$, $\varepsilon_4$}
\Text(105,30)[l]{$\vdots$}
\end{picture}
\end{center}
\index{numbering of momenta}%

In addition to the external particles, the following items are defined
in \Code{process.h}:
\begin{itemize}
\item	a colour factor (\Code{COLOURFACTOR}), to account for coloured
	particles in the initial state,
\item	a combinatorial factor (\Code{IDENTICALFACTOR}), to account for
	identical particles in the final state,
\item	a wave-function renormalization (\Code{WF\uscore RENORMALIZATION}), 
	if a non-onshell renormalization scheme is used,
\item	whether bremsstrahlung shall be added (\Code{PHOTONRADIATION}),
	and the maximum energy a soft photon may have (\Code{ESOFTMAX})
	(see Sect.\ \ref{sect:qed} for details).
\end{itemize}
\indextt{COLOURFACTOR}%
\indextt{IDENTICALFACTOR}%
\indextt{WF\uscore RENORMALIZATION}%
\indextt{PHOTONRADIATION}%
\indextt{ESOFTMAX}%

%------------------------------------------------------------------------

\subsubsection{Building up phase space}%
\index{phase space}%
\index{integration}

\FC\ builds up the $n$-particle phase space iteratively by nested
integrations over the invariant mass $M_i$ and solid angle $\Omega_i$ of
each outgoing particle $i$.  This procedure is encoded in the subroutine
\Code{Split}:
\begin{center}
\includegraphics{PhaseSpace}
\end{center}
Counting the degrees of freedom, there are $(n - 1)$ $M$-integrations 
and $n$ $\Omega$-integrations.  The corresponding phase-space 
parameterization is
\begin{align*}
&\frac 1{2\sqrt s}\int_{m_2+\dots+m_n}^{\sqrt s - m_1}
  \rd M_1\,\rd\Omega_1\,\frac{k_1}{2} \\
&\times\int_{m_3+\dots+m_n}^{M_1 - m_2}
  \rd M_2\,\rd\Omega_2\,\frac{k_2}{2} \\
&\times\cdots \notag \\
&\times\int_{m_n}^{M_{n-2} - m_{n-1}}
  \rd M_{n-1}\,\rd\Omega_{n-1}\,\frac{k_{n-1}}{2} \\
&\times\int
  \rd\Omega_n\,\frac{k_n}{2}
\end{align*}
\indextt{Split}%
where $\rd\Omega_i = \rd\cos\theta_i\:\rd\varphi_i$.  The particle's
momentum $k_i$ and $\cos\theta_i$ are given in the respective decay's
rest frame.  The $\varphi_1$-integration is trivial because of axial 
symmetry.  From the practical point of view this looks as follows
(this code is taken almost verbatim from \FC's \Code{2to3.F}):
\begin{verbatim}
       p = 0
       ex = 0
       ey = 0
       ez = 1
       minv = sqrtS
       msum = mass(3) + mass(4) + mass(5)

       call Split(5, mass(5),
     &   p, ex,ey,ez, minv, msum, fac, 0, 
     &   Var(XMREM5), Var(XCOSTH5), Var(TRIVIAL))

       call Split(4, mass(4),
     &   p, ex,ey,ez, minv, msum, fac, 0,
     &   Var(FIXED), Var(XCOSTH4), Var(XPHI4))

       call VecSet(3, mass(3), p, ex,ey,ez)
\end{verbatim}
One starts with the initial reference direction in $(\Code{ex},
\Code{ey}, \Code{ez})$ and no boost, \Code{p = 0}.  The available energy
is given in \Code{minv} and the sum of external masses in \Code{msum}.
The \Code{Split} subroutine is then called $(n - 1)$ times for an
$n$-particle final state.  The reference direction, the boost,
\Code{minv}, and \Code{msum} are automatically adjusted along the way
for the respective remaining subsystem and ultimately determine the
remaining $n$-th vector unambigously, which is then simply set by
\Code{VecSet}.

About the integration variables more will be said in the next section. 
For the moment, note that the \Code{X} in \Code{XMREM5} refers to
the ratio, \ie \Code{XMREM5} runs from 0 to 1.  The actual integration
borders are determined internally by Split.

After invoking \Code{Split} or \Code{VecSet} for external particle \Vi, 
several kinematical quantities are available:
\begin{itemize}
\item	\Code{momspec(SPEC\uscore M,\Vi)}
	--- mass $m_i$,

\item	\Code{momspec(SPEC\uscore E,\Vi)}
	--- energy $E_i$,

\item	\Code{momspec(SPEC\uscore K,\Vi)}
	--- momentum $|\vec k_i|$,

\item	\Code{momspec(SPEC\uscore ET,\Vi)}
	--- transverse energy $E_i^T$,

\item	\Code{momspec(SPEC\uscore KT,\Vi)}
	--- transverse momentum $|\vec k_i^T|$,

\item	\Code{momspec(SPEC\uscore RAP,\Vi)}
	--- rapidity $y_i$,

\item	\Code{momspec(SPEC\uscore PRAP,\Vi)}
	--- pseudo-rapidity $\eta_i$,

\item	\Code{momspec(SPEC\uscore DELTAK,\Vi)}
	--- the difference $E_i - k_i$,

\item	\Code{momspec(SPEC\uscore PHI,\Vi)}
	--- aximuthal angle $\varphi_i$,

\item	\Code{momspec(SPEC\uscore EX,\Vi)},
	\Code{momspec(SPEC\uscore EY,\Vi)},
	\Code{momspec(SPEC\uscore EZ,\Vi)} \\
	--- direction of motion $\vec e_i$.
\end{itemize}

%------------------------------------------------------------------------

\subsubsection{Variables}

The kinematic input variables are organized in a homogeneous system.  
Each variable is referred to by a preprocessor constant, \eg 
\Code{SQRTS} or \Code{XCOSTH} (variables starting with \Code{X} are 
generally scaled, \ie run from 0 to 1).  The following parts can be 
accessed via preprocessor macros:
\indextt{Var}%
\indextt{Show}%
\indextt{Lower}%
\indextt{Upper}%
\indextt{Step}%
\indextt{CutMin}%
\indextt{CutMax}%
\begin{itemize}
\item
\Code{Var(\Vi)} = the actual value of \Vi.

\item
\Code{Show(\Vi)} = the value printed in the output
-- to print \eg $t$ instead of $\cos\theta$.

\item
\Code{Lower(\Vi)}, \Code{Upper(\Vi)}, \Code{Step(\Vi)} =
the lower limit, upper limit, and step width of \Vi.

If the step is zero, the cross-section is integrated over \Vi.

If the step is $-999$, the variable is considered `spurious', \ie used
for output only, not integrated over or stepped through.

\item
\Code{CutMin(\Vi)}, \Code{CutMax(\Vi)} = the lower and upper cuts 
on \Vi.
\end{itemize}
\indextt{FIXED}%
\indextt{TRIVIAL}%
There are two special variables: \Code{FIXED} for fixed values,
\ie no integration, and \Code{TRIVIAL} for trivial integrations.

%------------------------------------------------------------------------

\subsubsection{Cuts}

There are two principal ways to apply cuts in FormCalc.  The first is by 
actually restricting the integration limits.
%-- this is possible only for the `natural' integration variables.  
The second is by selectively setting the integrand (differential 
cross-section) to zero whenever the cut condition(s) are met.

\paragraph{Restricting integration limits}

The subroutine \Code{Split} allows to restrict the integration region of 
the $M$- and $\cos\theta$-integration.  The $\varphi$-integration is not 
modified in the present setup.  The application of cuts works \eg as 
follows:
\begin{verbatim}
       key = 0

       CutMin(XMREM5) = E5MIN
       key = key + Cut(CUT_MREM_E, CUT_MIN)

       CutMin(XCOSTH5) = -(1 - COSTH5CUT)
       CutMax(XCOSTH5) = +(1 - COSTH5CUT)
       key = key + Cut(CUT_COSTH, CUT_MIN + CUT_MAX)

       call Split(5, Re(MASS5),
     &   p, ex,ey,ez, minv, msum, fac, key,
     &   Var(XMREM5), Var(XCOSTH5), Var(TRIVIAL))
       ...
\end{verbatim}
The value of the cut is deposited in \Code{CutMin} or \Code{CutMax} and 
`registered' by setting a bit in the integer variable \Code{key} passed 
to \Code{Split}, \eg \Code{Cut(CUT\uscore MREM\uscore E,\,CUT\uscore MIN)}
specifies a cut on the energy (\Code{CUT\uscore MREM\uscore\underline{E}})
from below (\Code{CUT\uscore\underline{MIN}}) which is used to restrict 
the invariant-mass integration (\Code{CUT\uscore\underline{MREM}\uscore E}).
Available restrictions are:
\begin{center}
\begin{tabular}{|c|l|l|c|l|}
\multicolumn{2}{c}{Cuts restricting $M_i$} &
\multicolumn{1}{c}{\qquad} &
\multicolumn{2}{c}{Cuts restricting $\cos\theta_i$} \\
\cline{1-2}\cline{4-5}
Cut on & Key & & Cut on & Key \\
\cline{1-2}\cline{4-5}
$M_i$     & \verb=CUT_MREM=      & & $\cos\theta_i$     & \verb=CUT_COSTH= \\
$E_i$     & \verb=CUT_MREM_E=    & &			& \\
$k_i$     & \verb=CUT_MREM_K=    & &			& \\
$E_{T,i}$ & \verb=CUT_MREM_ET=   & &			& \\
$k_{T,i}$ & \verb=CUT_MREM_KT=   & &			& \\
$y_i$     & \verb=CUT_MREM_RAP=  & &			& \\
$\eta_i$  & \verb=CUT_MREM_PRAP= & &			& \\
\cline{1-2}\cline{4-5}
\end{tabular}
\end{center}
The transverse energy cut has the slight anomaly that it corresponds to 
$E_T = \sqrt{\smash[b]{k_T^2} + m^2}$ rather than $k^0 
\sqrt{\smash[b]{e_x^2 + e_y^2}}$ as the veto cut (see below).  The 
reason is that, because the cuts are applied by actually restricting the 
integration bounds, solvability of the cut equations is a limiting 
factor.


\paragraph{Imposing veto cuts}

In many interesting cases, the cut condition(s) cannot straightforwardly 
be translated into restrictions of the integration limits.  They are 
more easily applied through veto functions (1 in wanted, 0 in unwanted 
areas) cuts, \ie by setting the integrand to zero in phase-space 
regions where cut conditions apply.

The veto function is constructed from the following preprocessor macros:
\begin{itemize}
\item	\Code{CUT\uscore E(\Vi)}
	--- the energy $E_i$ of particle \Vi,
\item	\Code{CUT\uscore k(\Vi)}
	--- the momentum $|\vec k_i|$ of particle \Vi,
\item	\Code{CUT\uscore ET(\Vi)}
	--- the transverse energy $E_i^T = k_i^0 \sqrt{\smash[b]{e_{ix}^2 +
	    e_{iy}^2}}$ of particle \Vi,
\item	\Code{CUT\uscore kT(\Vi)}
	--- the transverse momentum $|\vec k_i^T|$ of particle \Vi,
\item	\Code{CUT\uscore y(\Vi)}
	--- the rapidity $y_i$ of particle \Vi,
\item	\Code{CUT\uscore eta(\Vi)}
	--- the pseudo-rapidity $\eta_i$ of particle \Vi,
\item	\Code{CUT\uscore deltatheta(\Vi)}
	--- the scattering angle $\theta_i$ between particle \Vi\ and
	    the $z$-axis ($\leqslant\tfrac\pi 2$), \\
	\Code{CUT\uscore cosdeltatheta(\Vi)}
	--- $\cos\theta_i$,
\item	\Code{CUT\uscore deltaalpha(\Vi,\Vj)}
	--- the angle $\alpha_{ij}$ between particles \Vi\ and \Vj\
	    ($\leqslant\tfrac\pi 2$), \\
	\Code{CUT\uscore cosdeltaalpha(\Vi,\Vj)}
	--- $\cos\alpha_{ij}$,
\item	\Code{CUT\uscore deltay(\Vi,\Vj)}
	--- the rapidity gap $\Delta y_{ij}$
	    between particles \Vi\ and \Vj,
\item	\Code{CUT\uscore deltaeta(\Vi,\Vj)}
	--- the pseudo-rapidity gap $\Delta\eta_{ij}$
	    between particles \Vi\ and \Vj,
\item	\Code{CUT\uscore R(\Vi,\Vj)}
	--- the separation variable
	    $\Delta R = \sqrt{\smash[b]{\Delta y_{ij}^2 + \Delta\varphi_{ij}^2}}$
	    of particles \Vi\ and \Vj,
\item	\Code{CUT\uscore rho(\Vi,\Vj)}
	--- the separation variable
	    $\Delta\rho = \sqrt{\smash[b]{\Delta\eta_{ij}^2 + \Delta\varphi_{ij}^2}}$
	    of particles \Vi\ and \Vj,
\item	\Code{CUT\uscore yprod(\Vi,\Vj)}
	--- the opposite-hemisphere variable $y_i y_j$ of
	    particles \Vi\ and \Vj,
\item	\Code{CUT\uscore etaprod(\Vi,\Vj)}
	--- the opposite-hemisphere variable $\eta_i \eta_j$ of
	    particles \Vi\ and \Vj,
\item	\Code{CUT\uscore invmass(\Vi,\Vj)}
	--- the invariant mass $M_{ij}$ of particles \Vi\ and \Vj.
\end{itemize}
The cuts are listed in the \Code{CUT1}$\dots$\Code{CUT20} definitions in
\Code{run.F}.  Together they make up a single logical expression in 
Fortran and may include \eg logical operators.  For example, the
following cut forces particle 3 to have a transverse energy of at
least 10 GeV:
\begin{verbatim}
#define CUT1 CUT_ET(3) .lt. 10
\end{verbatim}

%------------------------------------------------------------------------

\subsubsection{Convolution}

With the system of integration variables, the convolution with arbitrary 
parton distribution functions can easily be achieved.  Three modules are
already included in \FC:
\begin{itemize}
\item \Code{lumi\uscore parton.F}
      = initial-state partons, no convolution.

\item \Code{lumi\uscore hadron.F}
      = initial-state hadrons,
      convolution with hadronic PDFs from the LHAPDF library 
      \cite{WhBG05}.

\item \Code{lumi\uscore photon.F}
      = initial-state photons,
      convolution with CompAZ spectrum \cite{Za03}.
\end{itemize}

%------------------------------------------------------------------------

\subsubsection{Integration parameters}

Depending on the integrand, the actual integration can be fairly tricky to
carry out numerically.  \Code{2to3.F} employs the \cuba\ library
\cite{Ha04} which offers four integration routines.  The \cuba\ 
parameters are chosen in \Code{run.F} as preprocessor variables:
\begin{verbatim}
#define METHOD DIVONNE
#define RELACCURACY 1D-3
#define ABSACCURACY 1D-7
#define VERBOSE 1
#define MINEVAL 0
#define MAXEVAL 50000
#define STATEFILE ""
#define SPIN -1

* for Vegas:
#define NSTART 1000
#define NINCREASE 500
#define NBATCH 1000
#define GRIDNO 0

* for Suave:
#define NNEW 1000
#define NMIN 2
#define FLATNESS 50

* for Divonne:
#define KEY1 47
#define KEY2 1
#define KEY3 1
#define MAXPASS 5
#define BORDER 1D-6
#define MAXCHISQ 10
#define MINDEVIATION .25D0 

* for Cuhre:
#define KEY 0
\end{verbatim} 
The integration algorithm is selected with \Code{METHOD}, which can take 
the values \Code{VEGAS}, \Code{SUAVE}, \Code{DIVONNE}, and 
\Code{CUHRE}.  The other preprocessor variables determine parameters of 
the integrators and may/should be tuned for a particular integrand, for 
details see \cite{Ha04}.%
\indextt{VEGAS}%
\indextt{SUAVE}%
\indextt{DIVONNE}%
\indextt{CUHRE}%
\index{Cuba@\cuba}

%------------------------------------------------------------------------

\subsubsection{Compiling and running the code}
\index{compiling!generated code}

The code produced by \Code{WriteSquaredME} is compiled with the
commands
\begin{verbatim}
   ./configure
   make
\end{verbatim}%
\indextt{configure}%
\indextt{make}%
\indextt{makefile}%
\indextt{run.F}%
The \Code{configure} script searches for the compilers and necessary
libraries and writes out a \Code{makefile} by adding the appropriate
locations and flags to \Code{makefile.in}.  The `usual' environment
variables like \Code{FC} (Fortran compiler) and \Code{FFLAGS} (Fortran
flags) can be used to force particular choices.

Based on the makefile, the \Code{make} command then builds the
executable.  Its default name is \Code{run}, which is quite natural
because the Fortran compiler sees \Code{run.F} as the main program, as
mentioned before.  The advantage is that for a different run, one can
make a copy of \Code{run.F}, say \Code{run1.F}, with different
parameters.  This new run is compiled with ``\Code{make run1}'' and
results in the executable \Code{run1}.  Thus, one can have several
executables for different runs in one directory.

The way the makefile compiles the code is also convenient if one wants to
use the generated subroutine \Code{SquaredME} alone, \ie without the 
surrounding driver programs.  The necessary object files are all placed
in the library \Code{squaredme.a} so that only a single file needs
to be linked.  It is possible to build just this library with
\begin{verbatim}
   make squaredme.a
\end{verbatim}%
\indextt{squaredme.a}

\label{page:run}%
\index{invoking \Code{run}}%
The executables (\Code{run}, \Code{run1}, etc.)\ are able to calculate
differential and integrated cross-sections for particles of arbitrary
polarization depending on the command line.
\biitab
\ownline{\Code{run \Var{p_1\dots p_n}
  \Var{sqrtS}
  [\Var{serialfrom}[,\Var{serialto}[,\Var{serialstep}]]]}} \\
&	compute the differential cross-section at a center-of-mass
	energy \Var{sqrtS} with the external particles polarized
	according to \Var{p_1\dots p_n}. \\
\ownline{\Code{run \Var{p_1\dots p_n}
  \Var{sqrtSfrom},\Var{sqrtSto}[,\Var{sqrtSstep}]
  [\Var{serialfrom}[,\Var{serialto}[,\Var{serialstep}]]]}} \\
&	compute the integrated cross-section in the energy range
	\Var{sqrtSfrom}--\Var{sqrtSto} in steps of \Var{sqrtSstep},
	\Var{p_1\dots p_n} being the polarizations of the external 
	particles.
\etab
The \Var{p_i} can take the following values:
\begin{quote}
\Code{u}\quad for an unpolarized particle, \\
\Code{t}\quad for a transversely polarized particle, \\
\Code{+}\quad for right-circular polarization, \\
\Code{-}\quad for left-circular polarization, and \\
\Code{l}\quad for longitudinal polarization.
\end{quote}
For a scalar particle, the polarization parameter must be present on the
command line although its value is ignored.  The energies are specified
in GeV.  If an energy is below threshold, \Code{run} issues a warning
and sets the energy to the threshold + 0.01 GeV.

A serial number range may optionally be added as a third argument, 
restricting the parameter scans.  This is normally used only internally 
by the \Code{submit} script (see below).
%
\index{polarization}%
\index{unpolarized particles}%
\index{left-circular polarization}%
\index{right-circular polarization}%
\index{longitudinal polarization}%
\index{threshold}%
\index{parameter scan}%
\index{serial number}

\medskip
\fbox{\parbox{.985\linewidth}{%
\emph{Important:} If a particular polarization was already fixed
during the algebraic simplification, \eg if \Code{\uscore Hel = 0} was
set in \FC\ to calculate with unpolarized external fermions, the code
thus produced can never compute a cross-section for a different
polarization, no matter how the executable is invoked.  Running such
code with an incompatible polarization will in general only result in a
wrong averaging factor.}}

%------------------------------------------------------------------------

\subsubsection{Vectorization}%
\index{vectorization}

The assembly of the squared matrix element in code generated by FormCalc 
can be sketched as in the following figure, where the helicity loop sits 
at the center of the calculation:
\begin{center}
\begin{picture}(205,120)
\CBox(0,0)(205,120){Blue}{PastelBlue}
\Text(5,115)[tl]{Loop(s) over $\sqrt s$ \& model parameters}
\CBox(15,5)(200,100){OliveGreen}{PastelGreen}
\Text(20,95)[tl]{Loop(s) over angular variables}
\CBox(30,10)(195,80){Red}{PastelRed}
\Text(35,75)[tl]{Loop over helicities $\lambda_1,\dots,\lambda_n$}
\Text(40,50)[tl]{$\sigma \mathrel{{+}{=}}
  \sum_c C_c\,\mathcal{M}^0_c(\lambda_1,\dots,\lambda_n)^*$}
\Text(97,33)[tl]{$\mathcal{M}^1_c(\lambda_1,\dots,\lambda_n)$}
\end{picture}
\end{center}

The helicity loop is not only strategically the most desirable but also 
the most obvious candidate for concurrent execution.  FormCalc can 
vectorize the helicity loop using the CPU's SIMD instructions (that is, 
the squared matrix element is computed for several helicity combinations 
at once).  The include file \Code{distrib.h} chooses the distribution 
properties of the compiled code.  Distribution of scans over parameter 
space is described in Sect.\ \ref{sect:scans}.

Vectorization is turned on automatically by the configure script if it 
detects that the CPU and compiler are capable of it.  If this is in 
conflict with plans to send the executable on a cluster of computers 
with heterogeneous SIMD capabilities, run configure with the 
\Code{--generic} option.  Then again, SIMD instructions are typically 
available on recent computers: SSE3 since 2004, AVX since 2011.  To 
control vectorization `by hand' one needs to specify the preprocessor 
variable \Code{SIMD} in \Code{distrib.h} rather than including the file 
\Code{simd.h} generated by configure.  \Code{SIMD} specifies the length 
of the vector, sensible values are 2 for AVX, 1 for SSE3, and 0 
otherwise.  In Fortran the vector length can in principle been chosen 
arbitrarily though a speedup is expected only for the mentioned values 
commensurate with the hardware.

On top of this, the calculation automatically detects and omits helicity 
combinations contributing negligibly.  This feature is controlled by the 
two environment variables \Code{FCHSELN} and \Code{FCHSELEPS} in the 
following way: for the first \Code{FCHSELN} phase-space points, the 
computation runs over all helicity combinations $h$ and the absolute 
value of the squared matrix element is added up in an array $t(h)$.  For 
subsequent phase-space points, then, only helicity combinations with a 
$t$-value larger than $\bigl(\Code{FCHSELEPS}\cdot\max_h t(h)\bigr)$ are 
actually computed.  Unless one intentionally begins sampling \eg on the 
borders of phase-space, the default values \Code{FCHSELN} = 10 and 
\Code{FCHSELEPS} = $10^{-7}$ should be sufficient.  In case of doubt 
about the validity of the result, the value of \Code{FCHSELN} should be 
increased, or set to zero (which turns the detection off).

If algebraic relations between helicities are known from analytic 
considerations, these can be specified in Mathematica before calling 
\Code{WriteSquaredME} and will restrict the sum over helicities 
accordingly.  For example:
\begin{verbatim}
   Hel[2] = -Hel[1];
   WriteSquaredME[...]
\end{verbatim}

%------------------------------------------------------------------------

\subsubsection{Scans over parameter space}%
\label{sect:scans}%
\index{scans}%
\index{parameter space}

To perform a scan, the actual calculation needs to be enclosed in a number
of \Code{do}-loops in which the values of the scanned parameters are
changed, \ie a structure of the form

\medskip
\verb|      do 1 para1 = |$\dots$ \\
\verb|      do 1 para2 = |$\dots$ \textit{etc.} \\
\verb|      |\textit{calculate the cross-section} \\
\verb|1     continue|
\medskip

\Code{run.F} provides several preprocessor variables,
\Code{LOOP1}$\dots$\Code{LOOP20}, which may be defined to contain the
\Code{do}-statements that initiate the loops.  Each loop must terminate
on statement 1, just as above.  For example, the lines
\begin{verbatim}
#define LOOP1 do 1 TB = 10, 50, 5
#define LOOP2 do 1 MA0 = 250, 1000, 250
\end{verbatim}
result in a scan of the form

\medskip
\verb|      do 1 TB = 10, 50, 5| \\
\verb|      do 1 MA0 = 250, 1000, 250| \\
\verb|      |\textit{calculate the cross-section} \\
\verb|1     continue|
\medskip

The loops are nested inside each other with \Code{LOOP1} being the
outermost loop.  It is not mandatory to declare a loop with each
\Code{LOOP\Vn}, also a fixed value may be given as in the following
definition:
\begin{verbatim}
#define LOOP1 TB = 30
\end{verbatim}

When scanning over parameter space, it is further necessary to keep
track of the parameter values for each data set.  This is done by
defining \Code{SHOW} commands in the preprocessor variables
\Code{PRINT1}$\dots$\Code{PRINT20} in \Code{run.F}.  The \Code{SHOW}
command two arguments, a string identifying the variable and then the 
variable itself.  Defining, for example,
\begin{verbatim}
#define PRINT1 SHOW "Mh0=", Mh0
\end{verbatim}
causes each data set to be preceded by a line of the form
\begin{verbatim}
# Mh0=  125.00
\end{verbatim}

Such a scan can be a real CPU hog, but on the other hand, the
calculation can be performed completely independently for each parameter
set and is thus an ideal candidate for parallelization.  The real
question is thus not how to parallelize the calculation, but how to
automate the parallelization.

The obstacle to automatic parallelization is that the loops are
user-defined and in general nested.  A serial number is introduced to
unroll the loops:

\medskip
\verb|      serial = 0| \\
\verb|      LOOP1| \\
\verb|      LOOP2| \\
\verb|        |$\vdots$ \\
\verb|      serial = serial + 1| \\
\verb|      if( serial |\textit{not in allowed range}\verb| ) goto 1| \\
\verb|      |\textit{calculate cross-section} \\
\verb|  1   continue|
\medskip

As mentioned before, the serial number range can be specified on the 
command line so that it is quite straightforward to distribute patches 
of serial numbers on different machines.  Most easily this is done in an 
interleaved manner, since one then does not need to know to which upper 
limit the serial number runs, \ie if there are $N$ machines available, 
send serial numbers 1, $N + 1$, $2N + 1$, $\dots$ on machine 1, send 
serial numbers 2, $N + 2$, $2N + 2$, $\dots$ on machine 2, etc.

This procedure is automated in \FC: The user once creates a 
\Code{.submitrc} file in his home directory and lists there all machines 
that may be used, one on each line.  The only requirement is that the 
machines are binary compatible because the same executable will be 
started on each.  In the case of multi-processor machines the number of 
processors is given after the host name.  Empty lines and lines 
beginning with \Code{\#} are treated as comments.  Furthermore, a line 
of the form \Code{nice \Var{n}} determines the nice value at which the 
remote jobs are started.  For example:
\begin{verbatim}
# .submitrc for FormCalc for the institute cluster
# start jobs with nice 10:
nice 10

pcxeon1 8
pcxeon2 8

pcjoe
pcath6
pcath7
\end{verbatim}
The executable compiled from \FC\ code, typically called \Code{run},
is then simply prefixed with \Code{submit}.  For instance, instead of
\begin{verbatim}
   run uuuu 500,1000
\end{verbatim}
the user invokes
\begin{verbatim}
   submit run uuuu 500,1000
\end{verbatim}
The \Code{submit} script uses \Code{ruptime} to determine the load
of the machines and \Code{ssh} to log in.  Handling of the serial
number is invisible to the user.%
\indextt{submit}
\indextt{.submitrc}

%------------------------------------------------------------------------

\subsubsection{Log files, Data files, and Resume}
\index{log files}%
\index{data files}%
\index{resume}

Due to the parallelization mechanism, a single output file is not
sufficient.  Instead, \FC\ creates a directory for each invocation of
\Code{run}, \eg \Code{run.UUUU.00200} for a differential cross-section
or \Code{run.UUUU.00200-00500:00010} for an integrated cross-section 
(where \Code{00010} is the step width in the loop over the energy),
opens one log file for each serial number in this directory, and
redirects console output to this file.%
\index{log directory}

Each log file contains both the `real' data and the `chatter' (progress,
warning, and error messages).  This has the advantage that no unit
numbers must be passed between subroutines -- every bit of output is
simply written to the console (unit \Code{*} in Fortran).  It also makes
it easier to pinpoint errors, since the error message appears right next
to the corrupted data.  The `real' data are marked by a ``\verb=|='' in
column 1 and there exists a simple shell script, \Code{data}, to extract
the real data from the log file.  For example,
\begin{verbatim}
  data run.UUUU.00200
\end{verbatim}
creates a data file \Code{run.UUUU.00200.data} containing only the
`real' data arranged in three columns, where the first column is the
scattering angle in radians, the second column is the tree-level
cross-section, and the third column is the one-loop correction.  For an
integrated cross-section, there are five columns: $\sqrt s$, the
tree-level cross-section, the one-loop correction, and the integration
errors for the tree-level and one-loop cross-sections.  Cross-sections 
are computed in picobarn.%
\indextt{data}

The log-file management also provides an easy way to resume an aborted
calculation.  This works as follows: when running through the loops of a
parameter scan, the log file for a particular serial number
\begin{itemize}
\item may not exist:
      then it is created with execute permissions,
\item may exist, but have execute permissions:
      then it is overwritten,
\item may exist and have read-write permissions:
      then this serial number is skipped.
\end{itemize}
The execute permissions, which serve here merely as a flag to indicate
an ongoing calculation, are reduced to ordinary read-write permissions
when the log file is closed.%
\index{permissions}

In other words, the program skips over the parts of the calculation that
are already finished, so all the user has to do to resume an aborted 
calculation is start the program again with the same parameters.

%------------------------------------------------------------------------

\subsubsection{Shell scripts}

\Code{turnoff} switches off (and on) the evaluation of certain parts
of the amplitude, which is a handy thing for testing.  For example,
``\Code{./turnoff box}'' switches off all parts of the amplitude with
`box' in their name.  Invoking \Code{turnoff} without any argument
restores all modules.%
\indextt{turnoff}

\smallskip
\Code{sfx} packs all source files (but not object, executable, or log
files) in the directory it is invoked in into a mail-safe
self-extracting archive.  For example, if \Code{sfx} is invoked in the
directory \Code{myprocess}, it produces \Code{myprocess.sfx}.  This file
can \eg be mailed to a collaborator, who needs to say
``\Code{./myprocess.sfx x}'' to unpack the contents.%
\indextt{sfx}

\smallskip
\Code{pnuglot} produces a high-quality plot in Encapsulated PostScript
format from a data file in just one line.  In fact, \Code{pnuglot}
does not even make the plot itself, it writes out a shell script to do
that, thus ``\Code{./pnuglot mydata}'' creates \Code{mydata.gpl}
which then runs gnuplot, \LaTeX, and dvips to create
\Code{mydata.eps}.  The advantage of this indirect method is that the
default gnuplot commands in \Code{mydata.gpl} can subsequently be
edited to suit the user's taste.  Adding a label or choosing a different
line width is, for example, a pretty trivial matter.  Needless to say,
all labels are in \LaTeX\ and Type 1 fonts are selected to make the EPS
file nicely scalable.

\Code{pnuglot} by default uses commands only available in \Code{gnuplot}
version 3.7 or higher.  This version can be obtained from
\Code{http://www.gnuplot.info}.
\biitab
\Code{pnuglot [\Var{opts}] \Var{file_1} \Var{file_2} $\dots$} &
	make a plot of the data files \Var{file_1}, \Var{file_2},
	$\dots$ \\
\textit{options:} \\
\Code{-o \Var{outfile}} &
	how to name the output file: the plotting script will be
	called \Code{\Var{outfile}.gpl} and the actual plot
	\Code{\Var{outfile}.eps}.  The default is \Var{outfile} =
	\Var{file_1}. \\
\Code{-2} &
	use only columns 1:2 (usually the tree-level cross-section)
	for plotting \\
\Code{-3} &
	use only columns 1:(2+3) (usually the one-loop corrected
	cross-section) for plotting
\etab%
\indextt{pnuglot}%
\indextt{gnuplot}%
\indextt{-o}%
\indextt{-2}%
\indextt{-3}

%------------------------------------------------------------------------

\subsection{The Mathematica Interface}

The Mathematica interface turns the stand-alone code into a Mathematica 
function for evaluating the cross-section or decay rate as a function of 
user-selected model parameters.  The benefits of such a function are 
obvious, as the whole instrumentarium of Mathematica commands can be 
applied to them.  For example, it is quite straightforward, using 
Mathematica's \Code{FindMinimum}, to determine the minimum (or maximum) 
of the cross-section over a piece of parameter space.

Interfacing is done using the MathLink protocol.  The changes necessary
to produce a MathLink executable are quite superficial and affect only
the file \Code{run.F}, where the user has to choose which model
parameters are interfaced from Mathematica.

To make the obvious even clearer, the cross-section is \emph{not} 
evaluated in Mathematica, but in Fortran or C, and only the numerical 
results are transferred back to Mathematica.  One thing one cannot do 
thus is to increase the numerical precision of the calculation using 
Mathematica commands like \Code{SetPrecision}.


\subsubsection{Setting up the Interface}

The model parameters are specified in the file \Code{run.F}.  Typical
definitions for stand-alone code look like (here from an MSSM 
calculation with \Code{TB} = $\tan\beta$ and \Code{MA0} = $M_{A^0}$):
\begin{verbatim}
   #define LOOP1 do 1 TB = 5, 50, 5
   #define LOOP2 MA0 = 500
   ...
\end{verbatim}
These lines declare \Code{TB} to be scanned from 5 to 50 in steps of 5 
and set \Code{MA0} to 500 GeV.  To be able to specify \Code{TB} in 
Mathematica instead, the only change is
\begin{verbatim}
   #define LOOP1 call MmaGetReal(TB)
\end{verbatim}
Such invocations of \Code{MmaGetReal} and its companion subroutines 
serve two purposes.  At compile time they determine with which arguments 
the Mathematica function is generated (for details see below), and at 
run time they actually transfer the function's arguments to the 
specified variables.

\biitab
\Code{MmaGetInteger(\Vi)} &
	read the integer/real/complex parameter \Vi/\Vr/\Vc \\[-.5ex]
\Code{MmaGetReal(\Vr)} &
	from Mathematica \\[-.5ex]
\Code{MmaGetComplex(\Vc)} &
	\\[.5ex]
\Code{MmaGetIntegerList(\Vi,\,\Vn)} &
	read the integer/real/complex parameter list \Vi/\Vr/\Vc \\[-.5ex]
\Code{MmaGetRealList(\Vr,\,\Vn)} &
	of length \Vn\ from Mathematica \\[-.5ex]
\Code{MmaGetComplexList(\Vc,\,\Vn)} &
	\\
\etab%
\indextt{MmaGetInteger}%
\indextt{MmaGetReal}%
\indextt{MmaGetComplex}%
\indextt{MmaGetIntegerList}%
\indextt{MmaGetRealList}%
\indextt{MmaGetComplexList}

Note that without a separate \Code{MmaGetReal} call, \Code{MA0} would
still be fixed by the Fortran statement, \ie not be accessible from
Mathematica.

Once the makefile detects the presence of these subroutines, it
automatically generates interfacing code and compiles a MathLink
executable.  For a file \Code{run.F} the corresponding MathLink
executable is also called \Code{run}, as in the stand-alone case. 
This file is not started from the command-line, but used in Mathematica 
as
\begin{verbatim}
   Install["run"]
\end{verbatim}


\subsubsection{The Interface Function in Mathematica}

After loading the MathLink executable with \Code{Install}, a
Mathematica function of the same name is available.  For definiteness,
we will call this function `\Code{run}' in the following since 
`\Code{run.F}' is the default parameter file.  This function has the
arguments
\begin{alltt}
   run[\Var{sqrtS}, \Var{arg\sb{1}}, \Var{arg\sb{2}}, ..., \Var{options}]

   run[\Brac{\Var{sqrtSfrom}, \Var{sqrtSto[}, \Var{sqrtSstep]}}, \Var{arg\sb{1}}, \Var{arg\sb{2}}, ..., \Var{options}]
\end{alltt}
The first form computes a differential cross-section at $\sqrt s = 
\Var{sqrtS}$.  The second form computes a total cross-section for 
energies $\sqrt s$ varying from \Var{sqrtSfrom} to \Var{sqrtSto}
in steps of \Var{sqrtSstep}.  This is in one-to-one correspondence 
with the command-line invocation of the stand-alone executable.

The \Var{arg_1}, \Var{arg_2}, $\dots$, are the model parameters declared
automatically by the presence of their \realcplx{MmaGet} calls (see
above).  They appear in the argument list in the same order as the
corresponding \realcplx{MmaGet} calls.

Other parameters are specified through the \Var{options}.

\biiitab{Default Value}
\Code{Polarizations} & \Code{"UUUU"} &
	the polarizations of the external particles \\
\Code{Serial} & \Code{\Brac{}} &
	the range of serial numbers to compute \\
\Code{SetNumber} & \Code{1} &
	a set number beginning with which parameters and data are 
	stored \\
\Code{ParaHead} & \Code{Para} &
	the head under which parameters are stored \\
\Code{DataHead} & \Code{Data} &
	the head for the data storage \\
\Code{LogFile} & \Code{""} &
	the log file to save screen output in
\etab%
\indextt{SetNumber}%
\indextt{ParaHead}%
\indextt{DataHead}%
\indextt{LogFile}

\Code{Polarizations} determines the polarizations of the external 
particles, specified as in the stand-alone version, \ie a string of 
characters for each external leg:
\begin{center}
\begin{tabular}{|cl|cl|} \hline
\Code{u} & unpolarized, &
	\Code{l} & longitudinal polarization, \\
\Code{t} & transversely polarized, \quad &
	\Code{-} & left-handed polarization, \\
&&	\Code{+} & right-handed polarization. \\ \hline
\end{tabular}
\end{center}

\Code{Serial} gives the range of serial numbers for which to perform the 
calculation, specified as 
\Code{\Brac{\Var{serialfrom[},\,\Var{serialto[},\,\Var{serialstep]]}}}.  
The concept of serial numbers, used to distribute parameter scans, is 
described in Sect.~\ref{sect:scans}.  This option applies only to 
parameters scanned by do-loops in the parameter statements.  Parameters 
read from Mathematica are unaffected by this option.

\Code{SetNumber} specifies the set number beginning with which 
parameters and data are stored (see next Section).

\Code{ParaHead} gives the head under which parameters are stored, \ie 
parameters are retrievable from \Code{parahead[setnumber]} (see next
Section).

\Code{DataHead} gives the head under which data are stored, \ie data are
retrievable from \Code{datahead[setnumber]} (see next Section).

\Code{LogFile} specifies the log-file to save screen output in.  An
empty string indicates no output redirection, \ie the output will appear 
on screen.


\subsubsection{Return values, Storage of Data}

The return value of the generated function is an integer which records 
how many parameter and data sets were transferred.  Assigning parameter 
and data sets as the data become available has several advantages:
\begin{itemize}
\item
the return value of \Code{run} is an integer rather than a large, 
bulky list,

\item 
the parameters corresponding to a particular data set are easy to
identify, \eg \Code{Para[4711]} contains the parameters corresponding
to \Code{Data[4711]},

\item 
most importantly, if the calculation is prematurely aborted, the 
parameters and data transferred so far are still accessible.
\end{itemize}
Both, the starting set number and the heads of the parameter and data 
assignments can be chosen with the options \Code{SetNumber}, 
\Code{ParaHead}, and \Code{DataHead}, respectively.

The parameters which are actually returned are chosen by the user in the
\Code{PRINT\Vn} statements in \Code{run.F} in much the same way 
as parameters are selected for printout in the stand-alone code.  To 
specify that \Code{TB} and \Code{MA0} be returned, one needs the 
definitions
\begin{verbatim}
   #define PRINT1 call MmaPutReal("TB", TB)
   #define PRINT2 call MmaPutReal("MA0", MA0)
\end{verbatim}
Notwithstanding, parameters can still be printed out, in which case 
they end up in the log file (or on screen, if no log file is chosen).  
To transfer \eg \Code{TB} to Mathematica \emph{and} print it out, one 
would use
\begin{verbatim}
   #define PRINT1 call MmaPutReal("TB", TB)
   #define PRINT2 SHOW "TB", TB
\end{verbatim}
An analogous subroutine exists of course for integer and complex
parameters, too.

\biitab
\Code{MmaPutInteger(\Vs,\,\Vi)} &
	transfer the integer/real/complex parameter \Vi/\Vr/\Vc \\[-.5ex]
\Code{MmaPutReal(\Vs,\,\Vr)} &
	to Mathematica under the name \Vs \\[-.5ex]
\Code{MmaPutComplex(\Vs,\,\Vc)} &
	\\[.5ex]
\Code{MmaPutIntegerList(\Vs,\,\Vi,\,\Vn)} &
	transfer the integer/real/complex parameter list \\[-.5ex]
\Code{MmaPutRealList(\Vs,\,\Vr,\,\Vn)} &
	\Vi/\Vr/\Vc\ of length \Vn\ to Mathematica under the name \Vs \\[-.5ex]
\Code{MmaPutComplexList(\Vs,\,\Vc,\,\Vn)} &
\etab%
\indextt{MmaPutInteger}%
\indextt{MmaPutReal}%
\indextt{MmaPutComplex}%
\indextt{MmaPutIntegerList}%
\indextt{MmaPutRealList}%
\indextt{MmaPutComplexList}

The parameters are stored in the form of rules in Mathematica, \ie as
\Code{\Var{name} -> \Var{value}}.  The first argument specifies the
left-hand side of this rule.  It need not be a symbol in the strict
sense, but can be an arbitrary Mathematica expression.  But note that in
particular the underscore has a special meaning in Mathematica and may
not be used in symbol names.  The second argument is then the right-hand
side of the rule and can be an arbitrary Fortran expression containing
model parameters, kinematic variables, etc.

The following example demonstrates the form of the parameter and data
assignments.  Shown are results of a differential cross-section for a
$2\to 2$ reaction at one point in MSSM parameter space.  Within the 
data the varied parameter is $\cos\theta$, the scattering angle.
\begin{verbatim}
   Para[1] = { TB -> 1.5, MUE -> -1000., MSusy -> 1000.,
               MA0 -> 700., M2 -> 100. }

   Data[1] = { DataRow[{500., -0.99},
                       {0.10592302458950732, 0.016577997941111422},
                       {0., 0.}],
               DataRow[{500., -0.33}, 
                       {0.16495552191438356, 0.014989931149150608},
                       {0., 0.}], 
               DataRow[{500., 0.33},
                       {0.2986891221231292, 0.015013326141014818},
                       {0., 0.}],
               DataRow[{500., 0.99},
                       {0.5071238252157443, 0.012260927614082411},
                       {0., 0.}] }
\end{verbatim}

\biitab
\Code{DataRow[\Vv,\,\Vr,\,\Ve]} &
	a row of data with kinematic variables \Vv, cross-section or
	decay-rate results \Vr, and integration error \Ve
\etab%
\indextt{DataRow}

The \Code{DataRow[\Vv,\,\Vr,\,\Ve]} function has three arguments:
\begin{itemize}
\item
the unintegrated kinematic variables ($\Vv = \{\sqrt s, \cos\theta\}$ 
above),

\item
the cross-section or decay-rate results (\Vr\ = \{tree-level result,
one-loop correction\} above), and

\item
the respective integration errors ($\Ve = \{0, 0\}$ above, as this 
example originates from the computation of a differential cross-section 
where no integration is performed).
\end{itemize}


\subsubsection{Using the Generated Mathematica Function}

To the Mathematica novice it may not be obvious how to use the function 
described above to analyse data, produce plots, etc.

As an example, let us produce a contour plot of the cross-section in the
$M_{A^0}$--$\tan\beta$ plane.  It is assumed that the function
\Code{run} has the two parameters \Code{MA0} and \Code{TB} in its
argument list:
\begin{verbatim}
   Install["run"]

   xs[sqrtS_, MA0_, TB_] := (
     run[{sqrtS, sqrtS}, MA0, TB];
     Data[1][[1,2]] )

   ContourPlot[xs[500, MA0, TB], {MA0, 100, 500}, {TB, 5, 50}]
\end{verbatim}
The function \Code{xs} runs the code and selects the data to plot.  
The first argument of \Code{run}, \Code{\Brac{sqrtS, sqrtS}}, instructs
the code to compute the total cross-section for just one point in energy.  
We then select the first (and only) \Code{DataRow} in the output and 
choose its second argument, the cross-section results:
\Code{Data[1][[1,2]]}.

This example can be extended a little to produce a one-dimensional
plot where \eg for each value of $\tan\beta$ the minimum and maximum
of the cross-section with respect to $M_{A^0}$ is recorded:
\begin{verbatim}
   << Graphics`FilledPlot`

   xsmin[sqrtS_, TB_] :=
     FindMinimum[xs[sqrtS, MA0, TB], {MA0, 100}][[1]]
   xsmax[sqrtS_, TB_] :=
     -FindMinimum[-xs[sqrtS, MA0, TB], {MA0, 100}][[1]]

   FilledPlot[{xsmin[500, TB], xsmax[500, TB]}, {TB, 5, 50}]
\end{verbatim}

%------------------------------------------------------------------------

\subsection{Renormalization Constants}%
\label{sect:renconst}%
\index{renormalization constants}

\FC\ provides a number of functions to facilitate the computation of
renormalization constants (RCs).  \FC\ makes a conceptual distinction
between the definition and the calculation of RCs.

%------------------------------------------------------------------------

\subsubsection{Definition of renormalization constants}

\FC\ regards those symbols as RCs for which a definition of the form
\begin{verbatim}
   RenConst[rc] := ...
\end{verbatim}
\indextt{RenConst}%
exists.  The purpose of the definition is to provide the functional
relationship between the RC and the self-energy from which it is
calculated, for example
\begin{verbatim}
   RenConst[dMHsq1] := ReTilde[SelfEnergy[S[1] -> S[1], MH]]
\end{verbatim}
Note that this definition is quite generic, in particular it contains no
details about the selection of diagrams.  It is intentional that the
specifics of the diagram generation and calculation are chosen only when
the RCs are actually calculated because this allows the user to make
selections on a process-by-process basis.  For example, one can calculate
the main process and the RCs in one program such that the options chosen
for the former (\eg for \Code{InsertFields}) automatically apply to the
latter.

The definitions of the RCs can be made anywhere before one of the
functions that calculates them is invoked.  A particularly convenient
location is in the model file to which the RCs belong.  This is the case
for the model files that come with the current versions of \FA, where the
definitions of the RCs are implemented according to the on-shell scheme of
\cite{De93}.

The definition of an RC may make use of the following functions.  They may
be used in the model file or anywhere else, even if \FC\ is not loaded.
\biitab
\Code{SelfEnergy[\Vi\,->\,\Vf,\,\Vm]} &
	the self-energy $\Sigma_{\Vi\Vf}(k^2 = \Vm^2)$ \\
\Code{DSelfEnergy[\Vi\,->\,\Vf,\,\Vm]} &
	the derivative $\left.\partial\Sigma_{\Vi\Vf}(k^2)/
	\partial k^2\right|_{k^2 = \Vm^2}$ \\
\Code{TreeCoupling[\Vi\,->\,\Vf]} &
	the tree-level contribution to $\Vi\to\Vf$ without external
	spinors \\
\Code{VertexFunc[\Vi\,->\,\Vf]} &
	the one-loop contribution to $\Vi\to\Vf$ without external
	spinors \\
\Code{LVectorCoeff[\Var{expr}]} &
	the coefficient of $\kslash P_L$ in \Var{expr} \\
\Code{RVectorCoeff[\Var{expr}]} &
	the coefficient of $\kslash P_R$ in \Var{expr} \\
\Code{LScalarCoeff[\Var{expr}]} &
	the coefficient of $P_L$ in \Var{expr} \\
\Code{RScalarCoeff[\Var{expr}]} &
	the coefficient of $P_R$ in \Var{expr} \\
\Code{ReTilde[\Var{expr}]} &
	takes the real part of loop integrals in \Var{expr} \\
\Code{ImTilde[\Var{expr}]} &
	takes the imaginary part of loop integrals in \Var{expr}
\etab%
\indextt{SelfEnergy}%
\indextt{DSelfEnergy}%
\indextt{TreeCoupling}%
\indextt{VertexFunc}%
\indextt{LVectorCoeff}%
\indextt{RVectorCoeff}%
\indextt{LScalarCoeff}%
\indextt{RScalarCoeff}%
\indextt{ReTilde}%
\indextt{ImTilde}

Most RCs fall into one of the following categories, for which special
functions exist to save typing and reduce errors.  (The width is of 
course not an RC, but is also listed here as its computation is very 
similar to that of an RC.)

\biitab
\Code{MassRC[\Vf]} &
	the mass RC $\delta M_{\Vf}$ \\
\Code{MassRC[\Var{f_1},\,\Var{f_2}]} &
	the mass RC $\delta M_{\Var{f_1}\Var{f_2}}$ \\
\Code{FieldRC[\Vf]} &
	the field RC $\delta Z_{\Vf}$ \\
\Code{FieldRC[\Var{f_1},\,\Var{f_2}]} &
	the field RC $\delta Z_{\Var{f_1}\Var{f_2}}$ \\
\Code{TadpoleRC[\Vf]} &
	the tadpole RC $\delta T_{\Vf}$ \\
\Code{WidthRC[\Vf]} &
	the width $\Gamma_f$
\etab%
\indextt{MassRC}%
\indextt{FieldRC}%
\indextt{TadpoleRC}%
\indextt{WidthRC}

The explicit formulas for computing the RCs are given in the following.  
For compactness of notation, $f$ refers to a fermion and $B$ to a boson 
field and $\partial\Sigma(m^2)$ is short for 
$\left.\partial\Sigma(k^2)/\partial k^2\right|_{k^2 = m^2}$.
\begin{align*}
\delta M_f &= \Retilde\Sigma^F_{ff}(m_f, \tfrac 12, \tfrac 12)\,, &
\delta M_B &= \Retilde\Sigma_{BB}(m_B^2)\,, \\
& & \delta M_{B_1 B_2} &= \frac 12\Retilde\left(
  \Sigma_{B_2 B_1}(m_{B_1}^2) +
  \Sigma_{B_2 B_1}(m_{B_2}^2) \right), \\[1ex]
%
\delta Z_f &= -\Retilde\begin{bmatrix}
    \Sigma_{ff}^{VL} + \partial\Sigma^F_{ff}(m_f, m_f, m_f) \\
    \Sigma_{ff}^{VR} + \partial\Sigma^F_{ff}(m_f, m_f, m_f)
  \end{bmatrix}, &
\delta Z_B &= -\Retilde\partial\Sigma_{BB}(m_B^2)\,, \\
\delta Z_{f_1 f_2} &= \frac 2{m_{f_1}^2 - m_{f_2}^2}
  \Retilde\begin{bmatrix}
    \Sigma^F_{f_2 f_1}(m_{f_2}, m_{f_2}, m_{f_1}) \\
    \Sigma^F_{f_2 f_1}(m_{f_2}, m_{f_1}, m_{f_2})
  \end{bmatrix}, &
\delta Z_{B_1 B_2} &= \frac 2{m_{B_1}^2 - m_{B_2}^2}
  \Retilde\Sigma_{B_2 B_1}(m_{B_2}^2)\,, \\
& & \delta T_B &= -\Sigma_B(m_B^2)\,, \\
\Gamma_f &= \Imtilde\Sigma^F_{ff}(m_f, 1, 1)\,, &
\Gamma_B &= \frac 1{m_B} \Imtilde\Sigma_{BB}(m_B^2)\,,
\end{align*}
where
$$
\Sigma^F(m, \alpha, \beta) =
  m \bigl[\alpha\,\Sigma^{VL}(m^2) + \beta\,\Sigma^{VR}(m^2)\bigr] +
  \beta\,\Sigma^{SL}(m^2) + \alpha\,\Sigma^{SR}(m^2)\,.
$$

%------------------------------------------------------------------------

\subsubsection{Calculation of renormalization constants}
\index{renormalization constants}

For the actual calculation of the RCs, \FC\ provides the two functions
\Code{CalcRenConst} and \Code{WriteRenConst}.  Both functions search the
expressions they receive as arguments for symbols which are RCs.
\Code{CalcRenConst} returns the calculated RCs as a list of rules whereas
\Code{WriteRenConst} writes them to a program file.
\biitab
\Code{CalcRenConst[\Var{expr}]} &
	calculate the RCs appearing in \Var{expr} and return the
	results as a list of rules \\
\Code{WriteRenConst[\Var{expr},\,\Var{dir}]} &
	the same, but write the results to a program in
	the code directory \Var{dir}
\etab%
\indextt{CalcRenConst}%
\indextt{WriteRenConst}

\indextt{renconst.h}%
\Code{WriteRenConst} writes out the subroutine \Code{CalcRenConst} as well
as the declarations of the RCs in \Code{renconst.h}.  It shares a number 
of options regarding the technicalities of code generation with 
\Code{WriteSquaredME}.  In the following table, if the default value 
coincides with the option name this means that the value is taken from 
the \Code{WriteSquaredME} options.
\biiitab{option}
\Code{Folder} & \Code{"renconst"} &
	the subdirectory of the code directory into which the generated 
	code is written \\
\Code{FilePrefix} & \Code{FilePrefix} &
	a string prepended to the filenames of the generated code \\
\Code{SymbolPrefix} & \Code{SymbolPrefix} &
	a string prepended to global symbols to prevent
	collision of names when more than one process is linked \\
\Code{FileHeader} & \Code{FileHeader} &
	the file header \\
\Code{FileIncludes} & \Code{FileIncludes} &
	per-file \Code{\#include} statements \\
\Code{SubroutineIncludes} & \Code{Subroutine\backsl} &
	per-subroutine \Code{\#include} statements \\[-\parskip]
& \Code{Includes} &
\etab%
\indextt{Folder}%
\indextt{FilePrefix}%
\indextt{SymbolPrefix}%
\indextt{FileHeader}%
\indextt{FileIncludes}%
\indextt{SubroutineIncludes}

The calculation of the RCs and the underlying self-energies can be 
influenced and inspected in various ways.

The functions \Code{SelfEnergy} and \Code{DSelfEnergy}, used in the 
definitions of the RCs, are invoked when the RCs are calculated.  They 
call \Code{CreateTopologies}, \Code{InsertFields}, \Code{CreateFeynAmp}, 
and \Code{CalcFeynAmp} and use whatever options have been set with 
\Code{SetOptions} at that time.  Computing the RCs with the same options 
as the virtual diagrams is just the right thing in most cases.  For 
finer control, the amplitude generation can be modified in two ways.

Firstly, hooks are provided for \Code{CreateTopologies}, 
\Code{InsertFields}, and \Code{CreateFeynAmp}.
\biitab
\Code{CreateTopologiesHook[\Var{args}]} &
	\Code{CreateTopologies} for \Code{[D]SelfEnergy} \\
\Code{InsertFieldsHook[\Var{args}]} &
	\Code{InsertFields} for \Code{[D]SelfEnergy} \\
\Code{CreateFeynAmpHook[\Var{args}]} &
	\Code{CreateFeynAmp} for \Code{[D]SelfEnergy}
\etab
\indextt{CreateTopologiesHook}%
\indextt{InsertFieldsHook}%
\indextt{CreateFeynAmpHook}%
For example, \Code{[D]SelfEnergy} does not call \Code{InsertFields} 
directly, but the intermediate function \Code{InsertFieldsHook} which 
by default simply redirects to the \FA\ function:
\begin{verbatim}
   InsertFieldsHook[args__] := InsertFields[args]
\end{verbatim}
\Code{InsertFieldsHook} can be redefined either generally or for a 
specific process, \eg
\begin{verbatim}
   InsertFieldsHook[tops_, f1_F -> f2_F] :=
     InsertFields[tops, f1 -> f2, ExcludeParticles -> V[1]]
\end{verbatim}
would exclude photons in fermion self-energies.

Secondly, it is possible to specify options for individual RCs, as in
\begin{verbatim}
   Options[ dMWsq1 ] = {Restrictions -> NoSUSYParticles}
\end{verbatim}
Any \Code{CreateTopologies}, \Code{InsertFields}, and 
\Code{CreateFeynAmp} options may be given here and apply only to
the calculation of this particular renormalization constant.%
\indextt{Options}%
\index{renormalization constants!options for}

The \Code{SEHook} function finally governs setting the computed 
self-energy on-shell.
\biitab
\Code{SEHook[\Var{se},\,\Var{amp},\,K2 -> $\Vm^2$]} &
	returns \Var{amp} with \Code{K2} replaced by $\Vm^2$
\etab
\indextt{SEHook}%
Attention: \Code{SEHook} has attribute \Code{HoldAll} (otherwise it 
would not be very useful).  The \Var{se} argument contains the original 
\Code{[D]SelfEnergy} call and is for pattern matching or printout (in 
\Code{HoldForm}) only -- it will recurse if evaluated directly.  The 
\Var{amp} argument should not be evaluated more than once, as it 
triggers the computation of the self-energy.  The $\Vm^2$ argument 
should be evaluated after \Var{amp} as it likely contains instances of 
\Code{TheMass} which can successfully be resolved only after model 
initialization in \Var{amp}.

The various intermediate results are stored in global variables where
they can be inspected immediately after calling \Code{[D]SelfEnergy}.
\biitab
\Code{\$RCTop} & the output of \Code{CreateTopologies}\dots \\
\Code{\$RCIns} & the output of \Code{InsertFields}\dots \\
\Code{\$RCAmp} & the output of \Code{CreateFeynAmp}\dots \\
\Code{\$RCRes} & the output of \Code{CalcFeynAmp}\dots \\
&	\dots in the last invocation of \Code{[D]SelfEnergy}
\etab
\indextt{\$RCTop}%
\indextt{\$RCIns}%
\indextt{\$RCAmp}%
\indextt{\$RCRes}%

During the calculation, \Code{PaintSE[\$RCIns]} and 
\Code{PutSE[\Brac{\$RCTop,\$RCIns,\$RCAmp,\$RCRes}]} are called, which
allows to paint the diagrams and store the intermediate results by 
setting \Code{\$PaintSE} and \Code{\$PutSE}, respectively.
\biitab
\Code{PaintSE[\Var{ins}]} &
	same as \Code{PaintSE[\Var{ins},\,\$PaintSE]} \\
\Code{PaintSE[\Var{ins},\,True]} &
	paint the diagrams \Var{ins} \\
\Code{PaintSE[\Var{ins},\,\Var{pre}]} &
	paint the diagrams \Var{ins} and store the output in
	\Code{\Var{pre}\Var{N}.ps},
	where \Var{N} = \Code{ProcName[\Var{ins}]} \\
\Code{PutSE[\Brac{\Var{top},\,\Var{ins},\,\Var{amp},\,\Var{res}}]} &
	same as 
	\Code{PutSE[\Brac{\Var{top},\Var{ins},\Var{amp},\Var{res}},\,\$PutSE]} \\
\Code{PutSE[\Brac{\Var{top},\Var{ins},\Var{amp},\Var{res}},\,\Var{pre}]} &
	store the results in
	\Code{\Var{pre}\Var{N}.\Brac{top,ins,amp,res}},
	where \Var{N} = \Code{ProcName[\Var{ins}]}
\etab
\indextt{PaintSE}%
\indextt{PutSE}%
\biiitab{variable}
\Code{\$PaintSE} & \Code{False} &
	whether to paint the diagrams \\
\Code{\$PutSE} & \Code{False} &
	whether to save the intermediate results
\etab
\indextt{\$PaintSE}%
\indextt{\$PutSE}%
If a string is given for \Code{\$PaintSE} or \Code{\$PutSE} it is used 
as a prefix for the filename and may include a path name, where
subdirectories are created as needed.

For performance reasons, \Code{SelfEnergy} and \Code{DSelfEnergy} remember
the values they have computed.  In unlikely cases it may happen that the
user changes settings that affect the generation of diagrams but are not
recognized by \Code{SelfEnergy} and \Code{DSelfEnergy}.  This would mean 
that results are returned which are no longer correct.  In such a case, 
the cached values can be removed with \Code{ClearSE[]}.

Similar to the functions \Code{FormSub}, \Code{FormDot}, etc., there are 
simplification wrappers specifically for RCs:
\biitab
\Code{RCSub[\Var{subexpr}]} &
	a function applied to subexpressions extracted by FORM \\
\Code{RCInt[\Var{intcoeff}]} &
	a function applied to the coefficients of loop integrals
	in the FORM output \\
\etab%
\indextt{RCSub}%
\indextt{RCInt}%

%------------------------------------------------------------------------

\subsection{Infrared Divergences and the Soft-photon Factor}
\label{sect:qed}
\index{infrared divergences}%
\index{soft-photon factor}%
\index{bremsstrahlung}%

Infrared divergences appear in processes with charged external particles.
They originate from the exchange of virtual photons.  More precisely they
come from diagrams containing structures of the form
\begin{center}
\begin{picture}(130,125)(0,0)
\Line(5,10)(30,30)
\Line(30,30)(100,30)
\Vertex(30,30){2}
\Photon(30,30)(30,95){-2}{4.5}
\Line(30,95)(5,115)
\Line(30,95)(100,95)
\Vertex(30,95){2}
\multiput(100,44)(0,17){3}{\makebox(0,0){$.$}}
\Text(0,115)[r]{$k_i$}
\Text(0,7)[r]{$k_j$}
\Text(20,62)[r]{$\gamma$}
\Text(65,62)[]{loop}
\Text(65,25)[t]{$m_{j-1}^2=k_j^2$}
\Text(65,100)[b]{$m_i^2=k_i^2$}
\end{picture}
\end{center}
Such diagrams are IR divergent because the photon is massless; if the
photon had a mass $\lambda$, the divergent terms would be proportional to
$\log\lambda$.

\index{photon mass}%
A note on the numerical implementation: In \LT\ it is not necessary to
introduce a photon mass by hand: if a requested integral is IR divergent,
\LT\ automatically regularizes the divergence with a photon mass
$\lambda$, but treats $\lambda$ as an infinitesimal quantity, which means
that terms of order $\lambda$ or higher are discarded.  In practice,
$\lambda$ is a user-definable numerical constant.  Since the final result
should not depend on it after successful removal of the IR divergences, it
can be given an arbitrary numerical value despite its infinitesimal
character.

The divergences in the diagrams with virtual photon exchange exactly
cancel those in the emission amplitude of soft (\ie low-energetic) photons
off the external legs.  Soft-photon radiation is always present
experimentally: because photons are massless, they can have arbitrarily
low energies and therefore escape undetected.  So while the unphysical
$\lambda$-dependence cancels, the final result depends instead on a
detector-dependent quantity $E_{\text{soft}}^{\text{max}}$, the maximum
energy a soft photon may have without being detected.

In a one-loop calculation one needs to take into account the
bremsstrahlung off the Born diagrams to be consistent in the order of
$\alpha$.  Dealing with soft photons is relatively easy since the
soft-photon cross-section is always proportional to the Born
cross-section:
$$
\left[\frac{\rd\sigma}{\rd\Omega}\right]_{\text{SB}} =
\delta_{\text{SB}}
\left[\frac{\rd\sigma}{\rd\Omega}\right]_{\text{Born}}
$$
where ``SB'' stands for soft bremsstrahlung.

The soft-photon factor $\delta_{\text{SB}}$ is implemented in 
\Code{main.F} using the formulas of \cite{De93}.  It is controlled by the 
following two preprocessor statements in \Code{process.h}.
\begin{verbatim}
c#define PHOTONRADIATION SOFT
#define ESOFTMAX .1D0*sqrtS
\end{verbatim}
The first flag, \Code{PHOTONRADIATION}, is commented out by default.  
To switch on soft-photon corrections, take out the \Code{c} before the
{\tt\#define}.  The second line determines the energy cutoff
$E_{\text{soft}}^{\text{max}}$.%
\indextt{PHOTONRADIATION}%
\indextt{SOFT}%
\indextt{ESOFTMAX}

%------------------------------------------------------------------------

\section{Post-processing of the Results}%
\index{post-processing}

\subsection{Reading the data files into \mma}

The format in which data are written out by \Code{main.F} is directly
practical only for making plots of the cross-section over the energy or
some phase-space variable.  In many circumstances one needs to make more
sophisticated figures in which the data are either arranged differently,
or are run through some filter or calculation.  Consider, for example,
performing a scan over some region in parameter space and then wanting to
plot the band of allowed values of the cross-section, \ie the minimum and
maximum with respect to the parameter variations for each point in phase
space.

Given the variety of operations one could conceivably perform on the data,
a Fortran or C program would sooner or later prove too inflexible.
Instead, there is a utility program with which it is possible to read the
data into \mma, where it is much easier to further process the data sets
and eventually produce plots of whatever function of whatever data set one
is interested in.

The data files are read into \mma\ with the \textit{MathLink} program
\Code{ReadData}, which is used in the following way:
\begin{verbatim}
In[1]:= Install["ReadData"];

In[2]:= ReadData["run.uuuu.00300-01000.data"]

Out[2]= 24
\end{verbatim}
\indextt{ReadData}%
The data are stored in \mma\ as separate data and parameter sets.  These
sets are accessed through an integer argument, \eg \Code{Data[1]} and
\Code{Para[1]}.  \Code{ReadData} returns the number of sets read (24 in
this example).

The data files have to be roughly in the format used by \Code{main.F}, 
\ie a number of lines beginning by \Code{\#} in which the parameters are 
given, followed by columns of data.  The following example shows this 
format:
\begin{verbatim}
# TB=    1.50
# MUE=-1000.00
# MSusy= 1000.00
# MA0=  700.00
# M_2=  100.00
    160.790000000000       0.264226254743272       0.502096473139054
    190.482475649966        10.7918866098049        9.58435238790029
    211.567524166608        11.8170727823835        10.3862900941921

# TB=    1.50
# MUE=-1000.00
# MSusy= 1000.00
# MA0=  700.00
# M_2=  400.00
    160.790000000000       0.264226254743272       0.502510502734037
    190.482475649966        10.7918866098049        9.60119470492217
    211.567524166608        11.8170727823835        10.4047052203442
\end{verbatim}
The output of \Code{ReadData} for this file is 2, hence there are then 
two data and two parameter sets defined,
\begin{verbatim}
Para[1] = { TB -> 1.5, MUE -> -1000., MSusy -> 1000.,
            MA0 -> 700., M$2 -> 100. }

Data[1] = { {160.79, 0.264226, 0.502096},
            {190.482, 10.7919, 9.58435}, 
            {211.568, 11.8171, 10.3863} }

Para[2] = { TB -> 1.5, MUE -> -1000., MSusy -> 1000.,
            MA0 -> 700., M$2 -> 400. }

Data[2] = { {160.79, 0.264226, 0.502511},
            {190.482, 10.7919, 9.60119},
            {211.568, 11.8171, 10.4047} }
\end{verbatim}
\indextt{Data}%
\indextt{Para}%
The numbers have not suffered in accuracy; it is a peculiarity of \mma\ 
to display real numbers with no more than six digits by default.  Note 
also that the underscore in \Code{M\uscore 2} which is not allowed in 
symbol names in \mma\ has been replaced by a \Code{\$}.
\biitab
\Code{ReadData[\Vf,\,\Vn,\,\Var{h_{\mathrm{para}}},\,\Var{h_{\mathrm{data}}}]} &
	read the data file \Vf\ into parameter and data sets in \mma;
	with the optional parameters \Vn, \Var{h_{\mathrm{para}}}, and 
	\Var{h_{\mathrm{data}}} one can choose to start numbering the 
	sets with \Vn, use head \Var{h_{\mathrm{para}}} instead of
	\Code{Para}, and use head \Var{h_{\mathrm{data}}} instead of
	\Code{Data} \\
\Code{Data[\Vn]} &
	the \Vn th data set \\
\Code{Para[\Vn]} &
	the \Vn th parameter set
\etab
In addition, \Code{ReadData} has a handy way of applying operations to each
line of data: it does not, in fact, put each line of numbers in a list,
but in a function called \Code{DataRow}.  By default, \Code{DataRow} is 
equal to \Code{List}.  Now consider that instead of the absolute values of 
the cross-section, which is what \Code{main.F} calculates by default, you 
want \eg the relative deviation of the one-loop and the tree-level
cross-section.  All it needs to achieve this is to redefine the
\Code{DataRow} function as
follows:
\begin{verbatim}
  Clear[DataRow];
  DataRow[x_, tree_, loop_] := {x, loop/tree}
\end{verbatim}
(This works because \Code{main.F} puts the tree-level result in the 
second column and the one-loop correction in the third column.)
\indextt{DataRow}%
\biiitab{function}
\Code{DataRow} & \Code{List} &
	the function which is applied to each row of data in a file
	read by \Code{ReadData}
\etab

%------------------------------------------------------------------------

\subsection{Special graphics functions for Parameter Scans}

A common way of displaying information from scans over the parameter space
is either as a three-dimensional plot, or as a density plot.  Usually the
height of the figure (represented by levels of grey in the case of a
density plot) then determines exclusion limits or similar information.

The standard \mma\ plotting functions \Code{Plot3D} and
\Code{DensityPlot} require functions to be plotted, not arrays of
numbers.  Although this can be circumvented by using interpolating
functions, there are two considerable disadvantages of this method:
first, the grid used by the plotting function in general does not
represent the calculated
points\footnote{In fact, if the plotting function uses a grid which 
	differs very slightly from the grid of data points, the plot may 
	even display some funny bumps which are artifacts of the 
	interpolation.};
second, there is no control over missing values.

For this reason, two graphics functions for 3D and density plots have been
included in \FC\ in the \Code{ScanGraphics} package.  They are especially
designed for parameter-scan plots in that they use a grid which precisely
matches the computed values, and that they can deal with missing values.
\biitab
\ownline{\Code{ScanPlot3D[\Var{v_1},\,\Var{v_2},\,\Vn,\,\Var{opts}]}} \\
\ownline{\Code{ScanDensityPlot[\Var{v_1},\,\Var{v_2},\,\Vn,\,\Var{opts}]}} \\
\ownline{\Code{ScanContourPlot[\Var{v_1},\,\Var{v_2},\,\Vn,\,\Var{opts}]}} \\
&	make a 3D, density, or contour plot of a quantity which has been 
	computed for different values of \Var{v_1} and \Var{v_2}, with 
	\Var{v_1} und \Var{v_2} displayed on the $x$- and $y$-axes.  The 
	data to be plotted are assumed to be in the data sets 
	\Code{Data[1]}$\dots$\Code{Data[\Vn]}.  The argument \Var{opts}
	may contain additional 3D (2D) graphics options.
\etab%
\indextt{ScanPlot3D}%
\indextt{ScanDensityPlot}%
\indextt{ScanContourPlot}%
Both functions cooperate closely with the \Code{ReadData} function
described in the last section.  The argument \Vn\ which specifies the 
number of data sets to be taken is usually just the output of
\Code{ReadData}.

\Code{ScanPlot3D} and \Code{ScanDensityPlot} determine the grid on a
democratic-vote basis: they make a list of all the spacings $\{\Delta v_1,
\Delta v_2\}$ between any two points and take the values of $\Delta v_1$
and $\Delta v_2$ that occur most often.  (This of course assumes that the
grid is equidistantly spaced.)  Points on this grid which do not
correspond to a data point are missing points.  If missing points are
detected, both graphics functions issue a warning and deposit the
coordinates of the missing points in the variable \Code{\$MissingPoints} 
for checking.%
\index{missing points}%
\indextt{\$MissingPoints}

%------------------------------------------------------------------------

\section{Low-level functions for code output}%
\index{code output}%
\index{Fortran output}%
\index{C output}

\FC's code-generation functions, used internally \eg by
\Code{WriteSquaredME}, can also be used directly to write out an
arbitrary Mathematica expression as optimized code.  The basic
syntax is very simple:
\begin{enumerate}
\item
\Code{\Var{handle} = OpenCode["\Var{file.F}"]} \\
opens \Var{file.F} as a Fortran file for writing,

\item
\Code{WriteExpr[\Var{handle}, \Brac{\Var{var} -> \Var{expr}, $\dots$}]} \\
writes out Fortran code to calculate \Var{expr} and store the
result in \Var{var},

\item
\Code{Close[\Var{handle}]} \\
closes the file again.
\end{enumerate}

The code generation is fairly sophisticated and goes well beyond
merely applying Mathematica's \Code{FortranForm}.  The generated code 
is optimized, \eg common subexpressions are pulled out and computed in 
temporary variables.  Expressions too large for Fortran are split into 
parts, as in
\begin{verbatim}
     var = part1
     var = var + part2
     ...
\end{verbatim}
If the expression is too large even to be sensibly evaluated in one 
file, the \Code{FileSplit} function can distribute it on several 
files and optionally write out a master subroutine which calls the 
individual parts.%
\indextt{FileSplit}

To further automate the code generation, such that the resulting code 
needs few or no changes by hand, many ancillary functions are available.

%------------------------------------------------------------------------

\subsection{File handling, Type conversion}%
\index{file handling}%
\index{type conversion}

\biitab
\Code{MkDir[\Var{dirs}]} &
	make sure the directory \Var{dirs} exists, creating
	subdirectories as necessary \\
\Code{OpenCode[\Var{file}]} &
	open \Var{file} for writing code \\
\Code{TimeStamp[]} &
	return a string with the current date and time \\
\Code{ToCode[\Var{expr}]} &
	return a string with the Fortran (or C) form of \Var{expr} \\
\Code{ToSymbol[\Var{args}]} &
	concatenate \Var{args} into one symbol \\
\Code{ToList[\Var{expr}]} &
	return a list of summands of \Var{expr}
\etab%
\indextt{MkDir}%
\indextt{OpenCode}%
\indextt{TimeStamp}%
\indextt{ToCode}%
\indextt{ToSymbol}%
\indextt{ToList}

\Code{MkDir["\Var{dir_1}",\,"\Var{dir_2}",\,$\dots$]} makes sure the 
directory \Var{dir_1}/\Var{dir_2}/$\dots$ exists, creating the 
individual subdirectories \Var{dir_1}, \Var{dir_2}, $\dots$ as 
necessary.  It works roughly as \Code{mkdir -p} in Unix.

\Code{OpenCode[\Var{file}]} opens \Var{file} for writing code.

\Code{TimeStamp[]} returns a string with the current date and time.

\Code{ToCode[\Var{expr}]} returns the Fortran (or C) form of \Var{expr} 
as a string.

\Code{ToSymbol[\Var{args}]} concatenates its arguments into a new
symbol, \eg \Code{ToSymbol[a,\,1,\,\Brac{x,\,y}]} gives \Code{a1xy}.

\Code{ToList[\Var{expr}]} returns a list of summands of \Var{expr}, \ie
turns a sum of items into a list of items.

%------------------------------------------------------------------------

\subsection{Writing Expressions}%
\index{writing expressions}%
\index{Fortran output}%
\index{C output}

\biitab
\Code{PrepareExpr[\Var{exprlist}]} &
	prepare \Var{exprlist} for write-out to code \\
\Code{WriteExpr[\Var{file},\,\Var{exprlist}]} &
	write \Var{exprlist} to \Var{file} \\
\Code{CodeExpr[\Vv,\,\Vt,\,\Var{exprlist}]} &
	the expressions \Var{exprlist} with variables \Vv\ and temporary 
	variables \Vt\ in a form ready for write-out by \Code{WriteExpr} \\
\Code{RuleAdd[\Va,\Vb]} &
	same as \Code{\Va\ -> \Va\ + \Vb} \\
\Code{DebugLine[\Vs]} &
	a debugging statement for variable \Vs \\
\Code{NoDebug[\Var{expr}]} &
	do not generate a debugging statement for \Var{expr} \\
\Code{\$SymbolPrefix} &
	a string prepended to all externally visible symbols
	to avoid symbol conflicts
\etab%
\indextt{PrepareExpr}%
\indextt{WriteExpr}%
\indextt{CodeExpr}%
\indextt{RuleAdd}%
\indextt{DebugLine}%
\indextt{NoDebug}%
\indextt{\$SymbolPrefix}

\Code{PrepareExpr[\Brac{\Var{var_1} -> \Var{expr_1},\,\Var{var_2} -> \Var{expr_2},\,$\dots$}]}
prepares a list of variable assignments for write-out to a code file.
Expressions with a leaf count larger than \Code{\$BlockSize} are split 
into several pieces, as in
\begin{verbatim}
      var = part1
      var = var + part2
      ...
\end{verbatim}
thereby possibly introducing temporary variables for subexpressions.  
\Code{SumOver}, \Code{DoLoop}, and \Code{IndexIf} objects are properly 
taken into account as do-loops and if-statements.  The output is a 
\Code{CodeExpr[\Var{vars},\,\Var{tmpvars},\,\Var{exprlist}]} object, 
where \Var{vars} are the original and \Var{tmpvars} the temporary 
variables introduced by \Code{PrepareExpr}.

\Code{WriteExpr[\Var{file},\,\Var{exprlist}]} writes a list of variable 
assignments to \Var{file}.  The \Var{exprlist} can either be a 
\Code{CodeExpr} object, \ie the output of \Code{PrepareExpr}, or a list 
of expressions of the form \Code{\Brac{\Var{var_1} -> \Var{expr_1}, 
\Var{var_2} -> \Var{expr_2}, $\dots$}}, which is first converted to a 
\Code{CodeExpr} object using \Code{PrepareExpr}.  \Code{WriteExpr} 
returns a list of the subexpressions that were actually written.

\Code{CodeExpr[\Var{vars},\,\Var{tmpvars},\,\Var{exprlist}]} is the 
output of \Code{PrepareExpr} and contains a list of expressions ready to 
be written to a file, where \Var{vars} are the original variables and 
\Var{tmpvars} are temporary variables introduced in order to shrink 
individual expressions to a size small enough for Fortran.

\Code{RuleAdd[\Var{var},\,\Var{expr}]} is equivalent to \Code{\Var{var} 
-> \Var{var} + \Var{expr}}.

\Code{DebugLine[\Vs]} translates to a debugging statement for variable
\Vs\ (print-out of variable \Vs) in the code.

Assignments of the form \Code{NoDebug[\Var{var} -> \Var{expr}]} never
generate debugging/checking statements, regardless of the value of
\Code{DebugLines} or \Code{DebugLabel}.

\Code{\$SymbolPrefix} is a string prepended to all externally visible
symbols in the generated code to avoid symbol collisions.

\biiitab{PrepareExpr option}
\Code{Optimize} & \Code{False} &
	whether to introduce temporary variables for common
	subexpressions \\
\Code{Expensive} & \Code{\Brac{}} &
	objects which should be hoisted from inner loops
	as far as possible \\
\Code{MinLeafCount} & \Code{10} &
	the mininum leaf count above which a subexpression becomes
	eligible for abbreviationing \\
\Code{DebugLines} & \Code{0} &
	whether to generate debugging/ checking statements for every
	variable \\
\Code{DebugLabel} & \Code{True} &
	whether and which label to use in debugging/checking
	statements \\
\Code{MakeTmp} & \Code{Identity} &
	a function for introducing user-defined temporary variables \\
\Code{Declarations} & \rlap{\Code{(Rule\,\pipe\,RuleAdd)[\Vv\uscore,\,\uscore] :> \Vv}} \\
&&	a \Code{Cases} pattern for selecting variable declarations \\
\Code{FinalTouch} & \Code{Identity} &
	a function which is applied to the final subexpressions \\
\Code{ResetNumbering} & \Code{True} &
	whether to number \Code{dup\Vn} and \Code{tmp\Vn}
	variables from $\Vn = 1$
\etab%
\indextt{Optimize}%
\indextt{Expensive}%
\indextt{MinLeafCount}%
\indextt{DebugLines}%
\indextt{DebugLabel}%
\indextt{MakeTmp}%
\indextt{Declarations}%
\indextt{FinalTouch}%
\indextt{ResetNumbering}

\Code{Optimize} determines whether variables should be introduced
for subexpressions which are used more than once.

\Code{Expensive} lists patterns of objects which are expensive in terms
of CPU time and should therefore be hoisted from inner do-loops if
possible.

\Code{MinLeafCount} specifies the minimum leaf count a common
subexpression must have in order that a variable is introduced for it.

\Code{DebugLines} specifies whether debugging and/or checking statements 
are generated for each expression.  Admissible values are
0 = no statements are generated,
1 = debugging statements are generated,
2 = checking statements are generated,
3 = debugging and checking statements are generated.
Debugging messages are usually generated for the expressions specified
by the user only.  To cover intermediate variables (\eg the ones
introduced for optimization), too, specify the negative of the values
above.

\Code{DebugLabel} specifies with which label debugging/checking 
statements are printed.  \Code{False} disables printing, \Code{True}
prints the variable name, and a string prefixes the variable name.
Any other value is understood as a function which is queried for each
variable assignment and its output, \Code{True}, \Code{False}, or a 
string, individually determines generation of the debug statement
for each variable assignment.

Debugging/checking statements come in three kinds, $1$, $-2$, and $2$.
Type $1$ are debugging statements of the form
\begin{alltt}
   \Var{var} = \Var{expr}
   DEB("\Var{var}", \Var{var})
\end{alltt}
Type $-2$ and $2$ are (pre and post) checking statements of the form
\begin{alltt}
   CHK_PRE(\Var{var})
   \Var{var} = \Var{expr}
   CHK_POST("\Var{var}", \Var{var})
\end{alltt}
The actual statements are constructed from \Code{\$DebugCmd[\Vt]},
$\Vt = 1, -2, 2$, which gives the debugging statement in
\Code{StringForm} format with three arguments: prefix (string), 
variable name (string), variable value (number).  For example, the 
default setting for debugging statements is
\begin{verbatim}
  $DebugCmd[1] = "DEB(\"`1``2`\", `3`)\n"
\end{verbatim}
\indextt{\$DebugCmd}%
which refers to the preprocessor macro \Code{DEB} that might be defined 
in Fortran as
\begin{verbatim}
  #define DEB(tag,var) print *, tag, var
\end{verbatim}
The statements are further enclosed by the strings in
\Code{\$DebugPre[\Vt]} and \Code{\$DebugPost[\Vt]}, which serves \eg
to enable debugging, as in:
\begin{verbatim}
  $DebugPre[1, level_:4] := "#if DEBUG >= " <> ToString[level] <> "\n"
  $DebugPost[1] = "#endif\n"
\end{verbatim}
\indextt{\$DebugPre}%
\indextt{\$DebugPost}%
Here debugging would be activated at compile time if the preprocessor 
variable \Code{DEBUG} (debug level) is larger than 2.

\Code{MakeTmp} specifies a function for introducing user-defined 
temporary variables, \eg \Code{ToVars}.

\Code{Declarations} specifies a pattern suitable for use in \Code{Cases}
that selects all objects to be declared as variables.

\Code{FinalTouch} gives a function which is applied to each final
subexpression, just before write-out to file.

\Code{ResetNumbering} resets the internal numbering of variable names 
created by \Code{PrepareExpr}, \ie variables of the form \Code{dup\Vn} 
(from \Code{Optimize}) and \Code{tmp\Vn} (from \Code{Expensive}) start 
at $\Vn = 1$.  It may be necessary to disable this reset \eg if more 
than one generated expression shall be put in the same program unit.

The prefixes for temporary and optimization variables, \ie the
\Code{tmp} in \Code{tmp123} and the \Code{dup} in \Code{dup123}, may
respectively be changed with \Code{\$TmpPrefix} and \Code{\$DupPrefix}.%
\indextt{\$TmpPrefix}
\indextt{\$DupPrefix}

\bigskip

\emph{Note:} these options may also be given to \Code{WriteExpr} which 
simply hands them down to \Code{PrepareExpr}.  They cannot be set using
\Code{SetOptions[WriteExpr,\,\dots]}, however.


\biiitab{WriteExpr option}
\Code{HornerStyle} & \Code{True} &
	whether to order expressions according in Horner form \\
\Code{FinalCollect} & \Code{False} &
	whether to collect factors in the final expression \\
\Code{FinalFunction} & \Code{Identity} &
	a function to apply to the final expression before write-out \\
\Code{Type} & \Code{False} &
	the type of the expressions, \Code{False} to omit declarations \\
\Code{TmpType} & \Code{"ComplexType"} &
	the type of temporary variables \\
\Code{IndexType} & \Code{False} &
	the type of indices \\
\Code{DeclIf} & \Code{False} &
	whether to separate declarations and code by \Code{\#if}
	statements \\
\Code{RealArgs} & \Code{\lbrac A0,A00,B0,B1,} &
	functions whose arguments must be \\
	& \Code{B00,B11,B001,} &
	of a guaranteed type (default: real) \\
	& \Code{B111,DB0,DB1,} & \\
	& \Code{DB00,DB11,} & \\
	& \Code{B0i,C0i,D0i,} & \\
	& \Code{E0i,F0i,Bget,} & \\
	& \rlap{\Code{Cget,Dget,Eget,}} & \\
	& \Code{Log,Sqrt\rbrac} & \\
\Code{Newline} & \Code{""} &
	a string to be printed after each expression
\etab%
\indextt{HornerStyle}%
\indextt{FinalCollect}%
\indextt{FinalFunction}%
\indextt{Type}%
\indextt{TmpType}%
\indextt{IndexType}%
\indextt{DeclIf}%
\indextt{RealArgs}%
\indextt{Newline}

\Code{HornerStyle} specifies whether expressions are ordered in Horner
form before writing them out as code.

\Code{FinalCollect} chooses a final collecting of common factors, after 
going over to Horner form, just before write-out to code.

\Code{FinalFunction} specifies a function to be applied to the final
expressions, just before write-out to code.  This function can be used
to apply language-specific translations.

\Code{Type} determines whether declarations shall be generated for
the variables and of which type.  If a string is given, \eg
\Code{Type -> "double precision"}, \Code{WriteExpr} writes out
declarations of that type for the given expressions.  Otherwise
no declarations are produced.

\Code{TmpType} is the counterpart of \Code{Type} for the temporary
variables.  \Code{TmpType -> Type} uses the settings of the \Code{Type}
option.

\Code{IndexType} likewise determines the generation of declarations 
for do-loop indices.

\Code{DeclIf -> \Var{var}}, where \Var{var} is a string suitable for a 
preprocessor variable, separates declarations and code in the form
\begin{alltt}
   #ifndef \Var{var}
   #define \Var{var}
   \textrm{[declarations]}
   #else
   \textrm{[code]}
   #endif
\end{alltt}
A file so generated is supposed to be included once in the declarations 
section and once in the code part.  This can be necessary in particular 
in Fortran, if \eg non-declaration statements such as statement 
functions need to be placed between declarations and code.

\Code{RealArgs} gives a list of functions whose numerical arguments must 
be of a guaranteed type, usually real (double precision).  For example, 
if the function \Var{foo} expects a single real argument, it must be 
invoked as \Code{\Var{foo}(0D0)} in Fortran, not \Code{\Var{foo}(0)}.

\Code{RealArgs[\Var{foo}] := ...} defines the actual conversion for
\Var{foo}.  The default is to map \Code{NArgs} over all arguments. 
\Code{NArgs[\Var{args}]} returns \Var{args} with all integers turned
into reals.  Note that \Code{NArgs} is not quite the same as \Code{N}:
while it changes \Code{1} to \Code{1.}, it leaves \eg \Code{m[1]} intact
so that array indices remain integers.%
\indextt{NArgs}

\Code{Newline} specifies a string to be printed after each statement.

%------------------------------------------------------------------------

\subsection{Variable lists and Abbreviations}%
\index{variable lists}%
\index{abbreviations}

\biitab
\Code{OnePassOrder[\Var{list}]} &
	order \Var{list} such that the definition of each item
	comes before its first use \\
\Code{MoveDepsRight[\Var{r_1},\,$\dots$,\Var{r_n}]} &
	move variables among the lists \Var{r_1},\,$\dots$,\Var{r_n}
	such that a definition does not depend on \Var{r_i} further
	to the right \\
\Code{MoveDepsLeft[\Var{r_1},\,$\dots$,\Var{r_n}]} &
	move variables among the lists \Var{r_1},\,$\dots$,\Var{r_n}
	such that a definition does not depend on \Var{r_i} further
	to the left
\etab%
\indextt{OnePassOrder}%
\indextt{MoveDepsLeft}%
\indextt{MoveDepsRight}

\Code{OnePassOrder[\Vr]} orders a list of interdependent rules such that
the definition of each item (\Code{\Var{var}\,->\,\Var{value}}) comes
before its use in the right-hand sides of other rules.  When
\Code{OnePassOrder} detects a recursion among the definitions of a list,
it deposits the offending rules in an internal format in
\Code{\$OnePassDebug} as debugging hints.

\Code{MoveDepsRight[\Var{r_1},\,$\dots$,\,\Var{r_n}]} shuffles variable
definitions (\Code{\Var{var}\,->\,\Var{value}}) among the lists of rules
\Var{r_i} such that the definitions in each list do not depend on
definitions in \Var{r_i} further to the left.  For example,
\Code{MoveDepsRight[\Brac{a\,->\,b}, \Brac{b\,->\,5}]} produces
\Code{\Brac{\Brac{}, \Brac{b\,->\,5, a\,->\,b}}}, \ie it moves
\Code{a\,->\,b} to the right list because it depends on \Code{b}.

\Code{MoveDepsLeft[\Var{r_1},\,$\dots$,\,\Var{r_n}]} shuffles variable
definitions (\Code{\Var{var}\,->\,\Var{value}}) among the lists of rules
\Var{r_i} such that the definitions in each list do not depend on
definitions in \Var{r_i} further to the right.  For example,
\Code{MoveDepsLeft[\Brac{a\,->\,b}, \Brac{b\,->\,5}]} produces
\Code{\Brac{\Brac{b\,->\,5, a\,->\,b}, \Brac{}}}, \ie it moves
\Code{b\,->\,5} to the left list because that depends on \Code{b}.

\biitab
\Code{ToVars[\Var{patt},\,\Var{name}][\Var{exprlist}]} &
	introduce variables for all subexpressions in \Var{exprlist}
	matching \Var{patt}
\etab
\indextt{ToVars}

\Code{ToVars[\Var{patt},\,\Var{name}][\Var{exprlist}]} introduces 
variables for subexpressions matching \Var{patt} and expects to be used 
on an expression list such as given to \Code{PrepareExpr}.  To make the 
new variables temporary (in the \Code{CodeExpr} sense), \Code{ToVars} 
should be applied through \Code{PrepareExpr}'s \Code{MakeTmp} option.  
Unlike with \Code{Abbreviate}, the variables introduced are not 
subscripted by loop indices and hence also not hoisted outside a 
\Code{DoLoop}.  The new variable definitions are inserted at the right 
places into the \Var{exprlist}.

Why move certain subexpressions to variables at all?  Firstly, it 
makes it easier to inspect/ debug their values, using the 
\Code{DebugLine} option.  Secondly, and in contrast to abbreviations 
introduced for optimization reasons (\eg by the \Code{Expensive} or 
\Code{Optimize} options), it can improve the readability of the 
generated code by well-chosen variable names.

The names for the variables are determined by the function \Var{name} 
which receives the expression being abbreviated and must return a symbol 
name for it.  If \Var{name} is a string instead, 
\Code{NewSymbol[\Var{name},\,0]} is taken as naming function, \ie the 
variable names will be \Code{\Var{name}1}, \Code{\Var{name}2}, etc.  The 
numbering is consecutive across \Code{ToVars} calls but can be reset by 
assigning \Code{SymbolNumber[\Var{name}] = 0}.

For example, \Code{PrepareExpr[\Var{exprlist}, MakeTmp -> 
ToVars[LoopIntegral[\uscore\uscore],\,Head]]} introduces variables for 
all loop integrals in \Var{exprlist}, with names like
\Code{B0i1}, \Code{B0i2}, etc.

\biitab
\Code{PaVeIntegral} &
	a pattern matching the head of all one-loop Passarino--Veltman
	integrals (\Code{A0i}, \Code{B0i}, etc.) \\
\Code{CutIntegral} &
	a pattern matching the head of all one-loop OPP integrals
	(\Code{Acut}, \Code{Bcut}, etc.) \\
\Code{LoopIntegral} &
	the union of \Code{PaVeIntegral} and \Code{CutIntegral}
\etab
\indextt{PaVeIntegral}%
\indextt{CutIntegral}%
\indextt{LoopIntegral}

\biitab
\Code{SplitSums[\Var{expr}]} &
	split \Var{expr} into a list of expressions such that index
	summations apply to the whole of each part \\
\Code{ToDoLoops[\Var{list}]} &
	categorize \Var{list} into patches that must be summed over
	the same set of indices \\
\Code{DoLoop[\Var{expr},\,\Var{ind}]} &
	a do-loop of \Var{expr} over the indices \Var{ind}
\etab%
\indextt{SplitSums}%
\indextt{ToDoLoops}%
\indextt{DoLoop}

\Code{SplitSums[\Var{expr}]} splits expr into a list of expressions such
that index sums (marked by \Code{SumOver}) always apply to the whole of
each part.  \Code{SplitSums[\Var{expr},\,\Var{wrap}]} applies \Var{wrap}
to the coefficients of the \Code{SumOver}.

\Code{ToDoLoops[\Var{list},\,\Var{ifunc}]} splits list into patches 
which must be summed over the same set of indices.  \Var{ifunc} is an 
optional argument: \Code{\Var{ifunc}[\Var{expr}]} must return the 
indices occurring in \Var{expr}.

\Code{DoLoop[\Var{ind},\,\Var{expr}]} is a symbol introduced by
\Code{ToDoLoops} indicating that \Var{expr} is to be summed over the set
of indices \Var{ind}.

\biitab
\Code{ToIndexIf[\Var{expr}]} &
	turn the \Code{IndexDelta} and \Code{IndexDiff} in \Var{expr}
	into \Code{IndexIf} \\
\Code{IndexIf[\Var{cond},\,\Va,\,\Vb]} &
	same as \Code{If[\Var{cond},\,\Va,\,\Vb]} except that the
	expressions \Va\ and \Vb\ are not held unevaluated \\
\Code{IndexDelta[\Vi,\,\Vj]} &
	Kronecker's $\delta_{\Vi\Vj}$ \\
\Code{IndexDiff[\Vi,\,\Vj]} &
	$1 - \delta_{\Vi\Vj}$ \\
\Code{MapIf[\Vf,\,\Var{expr}]} &
	maps \Vf\ over the \Var{expr} except for the conditional parts
	of an \Code{IndexIf}
\etab%
\indextt{ToIndexIf}%
\indextt{IndexIf}%
\indextt{IndexDelta}%
\indextt{IndexDiff}%
\indextt{MapIf}

\Code{ToIndexIf[\Var{expr}]} converts all \Code{IndexDelta} and 
\Code{IndexDiff} objects in \Var{expr} to \Code{IndexIf}, which will be 
written out as if-statements in the generated code.  
\Code{ToIndexIf[\Var{expr},\,\Var{patt}]} operates only on indices 
matching patt.

\Code{IndexIf[\Var{cond},\,\Va,\,\Vb]} is the same as 
\Code{If[\Var{cond},\,\Va,\,\Vb]} except that expressions \Va\ and \Vb\ 
are not held unevaluated.  \Code{IndexIf[\Var{cond},\,\Va]} is 
equivalent to \Code{IndexIf[\Var{cond},\,\Va,\,0]}, \ie the ``else''
part defaults to 0.  Several conditions can be combined as
\Code{IndexIf[\Var{cond_1},\,\Var{a_1},
\Var{cond_2},\,\Var{a_2}, $\dots$]}, which is equivalent to 
\Code{IndexIf[\Var{cond_1},\,\Var{a_1},
IndexIf[\Var{cond_2},\,\Var{a_2}, $\dots$]]}.  Despite its name, 
the statement is not restricted to index nor to integer comparisons.  
It is furthermore written out as a regular if statement, 
\ie
\begin{alltt}
   if( \Var{cond} ) then
     \Va
   else
     \Vb
   endif
\end{alltt}

\Code{IndexDelta[\Vi,\,\Vj]} is Kronecker's delta $\delta_{\Vi\Vj}$.

\Code{IndexDiff[\Vi,\,\Vj]} is $1 - \delta_{\Vi\Vj}$.

\Code{MapIf[\Vf,\,\Var{expr}]} is equivalent to \Code{Map} except that it
does not modify the conditional parts if \Var{expr} is an \Code{IndexIf}.

\biitab
\Code{BlockSplit[\Var{var}\,->\,\Var{expr}]} &
	split \Var{expr} into subexpressions with leaf count less than
	\Code{\$BlockSize} \\
\Code{FileSplit[\Var{exprlist},\,\Var{mod},\,%
\rlap{\Var{writemod},\,\Var{writeall}]}} & \\
&	split \Var{exprlist} into batches with leaf count less than
	\Code{\$FileSize}, call \Var{writemod} to write a each
	module to file and finally \Var{writeall} to generate a `master 
	subroutine' which invokes the modules \\
\Code{ToArray[\Vs]} &
	take the symbol \Vs\ apart into letters and digits, \eg
	\Code{X123} $\to$ \Code{X[123]} \\
\Code{Renumber[\Var{expr},\,\Var{v_1},\,\Var{v_2},\,$\dots$]} &
	renumber all \Var{v_1[n]}, \Var{v_2[n]}, $\dots$ in \Var{expr} \\
\Code{MaxDims[\Var{args}]} &
	find the maximum indices of all functions in \Var{args} \\
\Code{\$BlockSize} &
	the size of each block for \Code{BlockSplit} \\
\Code{\$FileSize} &
	the size of each file for \Code{FileSplit}
\etab%
\indextt{BlockSplit}%
\indextt{FileSplit}%
\indextt{ToArray}%
\indextt{Renumber}%
\indextt{\$BlockSize}%
\indextt{\$FileSize}

\Code{BlockSplit[\Var{var}\,->\,\Var{expr}]} tries to split the
calculation of \Var{expr} into subexpressions each of which has a leaf 
count less than \Code{\$BlockSize}.

\Code{FileSplit[\Var{exprlist},\,\Var{mod},\,\Var{writemod},\,\Var{writeall}]}
splits \Var{exprlist} into batches with leaf count less than
\Code{\$FileSize}.  If there is only one batch,
\Code{\Var{writemod}[\Var{batch},\,\Var{mod}]} is invoked to write it to
file.  Otherwise, \Code{\Var{writemod}[\Var{batch},\,\Var{modN}]} is
invoked on each batch, where \Var{modN} is \Var{mod} suffixed by a
running number, and in the end
\Code{\Var{writeall}[\Var{mod},\,\Var{res}]} is called, where \Var{res} 
is the list of \Var{writemod} return values.  The optional 
\Var{writeall} function can be used \eg to write out a master subroutine 
which invokes the individual modules.  If \Var{mod} is given as a dot 
product \Code{\Var{name}.\Var{delim}}, the delimiter is used to separate 
the \Var{N} suffix.  For example, \Code{"foo"."\uscore"} will evaluate 
to \Code{"foo"} for a single file and to \Code{"foo\uscore 1"},
\Code{"foo\uscore 2"} etc.\ for multiple files.

\Code{ToArray[\Vs]} turns the symbol \Vs\ into an array reference by
taking it apart into letters and digits, \eg \Code{Var1234} becomes
\Code{Var[1234]}. 
\Code{ToArray[\Var{expr},\,\Var{s_1},\,\Var{s_2},$\dots$]} turns all
occurrences of the symbols \Var{s_1NNN}, \Var{s_2NNN}, etc.\ in
\Var{expr} into \Var{s_1[NNN]}, \Var{s_2[NNN]}, etc.
  
\Code{Renumber[\Var{expr},\,\Var{var_1},\,\Var{var_2},\,$\dots$]} 
renumbers all \Var{var_1[n]}, \Var{var_2[n]}, $\dots$ in \Var{expr}.

\Code{MaxDims[\Var{args}]} returns a list of all distinct functions
in args with the highest indices that appear, \eg \Code{MaxDims[foo[1,\,2],
foo[2,\,1]]} returns \Code{\Brac{foo[2,\,2]}}.

%------------------------------------------------------------------------

\subsection{Declarations}%
\index{declarations}

\biitab
\Code{SubroutineDecl[\Var{name}]} &
	the declaration for the subroutine \Var{name} \\
\Code{VarDecl[$\underbrace{\Var{vars},\,\Var{type}}_{\Va}$]} &
	the declaration of \Var{vars} as variables of type \Var{type} \\
\Code{VarDecl[Common[\Var{com}][\Va]]} &
	the declaration of common block \Var{com} with individual
	variables \Va\ (\Va\ as above) \\
\Code{VarDecl[NameMap[\Var{com}][\Va]]} &
	the declaration of common block \Var{com} with an array
	plus a preprocessor map of the \Va\ onto array elements
	(\Va\ as above) \\
\Code{VarDecl[NotEmpty[\Va]]} &
	output only if at least one variable list in \Va\ is non-empty \\
\Code{DoDecl[\Vi,\,\Var{range}]} &
	the declaration of a do-loop of \Vi\ over \Var{range} \\
\Code{CallDecl[\Var{subs}]} &
	the invocations of subroutines \Var{subs} \\
\Code{Dim[\Vi]} &
	the highest value the index \Vi\ takes on \\
\Code{DoDim[\Vi]} &
	like \Code{Dim[\Vi]} but including indices from \Code{SumOver}
	in the amplitudes \\
\Code{Enum[\Var{names}]} &
	set up \Var{names} as named indices \\
\Code{ClearEnum[]} &
	clear \Code{Enum} definitions
\etab%
\indextt{SubroutineDecl}%
\indextt{VarDecl}%
\indextt{Common}%
\indextt{NameMap}%
\indextt{NotEmpty}%
\indextt{DoDecl}%
\indextt{CallDecl}%
\indextt{Dim}%
\indextt{DoDim}%
\indextt{Enum}%
\indextt{ClearEnum}

\Code{SubroutineDecl[\Var{name}]} returns a string with the declaration
of the subroutine \Var{name}.

\Code{VarDecl[\Brac{\Var{v_1,v_2,\ldots}},\,\Vt]} returns a string with 
the declaration of \Var{v_1,v_2,\ldots} as variables of type \Vt.  Any 
other strings are output verbatim, \eg \Code{"\#ifdef COND\backsl n"}, 
\Code{"\#endif\backsl n"}.

\Code{VarDecl[Common[\Vc][\Brac{\Var{v_1,v_2,\ldots}},\,\Vt]]}
declares \Var{v_1,v_2,\ldots} to be members of common block \Vc.

\Code{VarDecl[NameMap[\Vc][\Brac{\Var{v_1,v_2,\ldots}},\,\Vt]]}
works much like \Code{Common} but puts only arrays into the common 
block (one for each data type), together with preprocessor statements 
mapping the \Var{v_1,v_2,\ldots} onto array elements.

\Code{VarDecl} arguments wrapped in \Code{NotEmpty} are output only if 
at least one of its variable lists is non-empty.

\Code{DoDecl[\Vi,\,\Vm]} returns a string containing the declaration of 
a loop over \Vi\ from 1 to \Vm.  \Code{DoDecl[\Vi,\,\Va,\,\Vb]} returns 
the same for a loop from \Va\ to \Vb.  \Code{DoDecl[\Vi]} invokes 
\Code{Dim[\Vi]} to determine the upper bound on \Vi.

\Code{CallDecl[\Brac{\Var{sub_1},\,\Var{sub_2},\,\dots}]} returns a 
string with the invocations of the subroutines \Var{sub_1}, \Var{sub_2}, 
\dots, taking into account possible loops indicated by 
\Code{DoLoop}.

\Code{Dim[\Vi]} and \Code{DoDim[\Vi]} return the highest value the index
\Vi\ takes on, excluding (\Code{Dim}) or including (\Code{DoDim})
indices found in \Code{SumOver} statements of amplitudes evaluated
so far.  A manual assignment \Code{Dim[\Vi]\,=\,\Vn} generates correct
array dimensions for index \Vi\ only.  A manual assignment
\Code{DoDim[\Vi]\,=\,\Vn} also generates a loop over index \Vi.

\Code{Enum} associates index names with integers, used for determining 
array bounds during code generation (the index names themselves remain 
unchanged).  The syntax is similar to that of C's \Code{enum}, \eg 
\Code{Enum[\Va,\,\Vb,\,\Vc\,->\,5,\,\Vd]} associates $\Va\to 1$,
$\Vb\to 2$, $\Vc\to 5$, $\Vd\to 6$.  \Code{ClearEnum[]} removes any 
\Code{Enum} assignments previously made.

%------------------------------------------------------------------------

\subsection{Compatibility Functions}%

\biitab
\Code{ToOldBRules} &
	rules to convert to the conventions for two-point functions
	of \LT\ 2.2 and before \\
\Code{ToNewBRules} &
	rules to convert to the conventions for two-point functions
	of \LT\ 2.3 or later
\etab%
\indextt{ToOldBRules}%
\indextt{ToNewBRules}

\Code{ToOldBRules} and \Code{ToNewBRules} are two sets of transformation
rules that convert between the old (\Code{B0}, \Code{B1}, $\dots$) and
new (\Code{B0i[b0,b1,$\dots$]}) conventions for two-point functions in
\LT.


\pagebreak

\begin{flushleft}
\begin{thebibliography}{999}
%\frenchspacing

\newcommand{\acm}[3]{\textsl{ACM Trans.\ Math.\ Software} \textbf{#1} (#2) #3}
\newcommand{\ajp}[3]{\textsl{Am.\ J.\ Phys.} \textbf{#1} (#2) #3}
\newcommand{\app}[3]{\textsl{Acta Phys.\ Polon.} \textbf{#1} (#2) #3}
\newcommand{\cpc}[3]{\textsl{Comp.\ Phys.\ Commun.} \textbf{#1} (#2) #3}
\newcommand{\fp}[3]{\textsl{Fortschr.\ Phys.} \textbf{#1} (#2) #3}
\newcommand{\np}[3]{\textsl{Nucl.\ Phys.} \textbf{#1} (#2) #3}
\newcommand{\npps}[3]{\textsl{Nucl.\ Phys.\ Proc.\ Suppl.} \textbf{#1} (#2) #3}
\newcommand{\pl}[3]{\textsl{Phys.\ Lett.} \textbf{#1} (#2) #3}
\newcommand{\pr}[3]{\textsl{Phys.\ Rev.} \textbf{#1} (#2) #3}
\newcommand{\prl}[3]{\textsl{Phys.\ Rev.\ Lett.} \textbf{#1} (#2) #3}
\newcommand{\zp}[3]{\textsl{Z.\ Phys.} \textbf{#1} (#2) #3}

\bibitem[Al09]{Al09}
\cpc{180}{2009}{1614} [arXiv:0806.4194].

\bibitem[Al14]{Al14}
A.~Alloul, N.~Christensen, C.~Degrande, C.~Duhr, B.~Fuks
\cpc{185}{2014}{2250} [arXiv:1310.1921].

\bibitem[Ch79]{ChFH79}
M.~Chanowitz, M.~Furman, I.~Hinchliffe, \np{B159}{1979}{225}.

\bibitem[Cv76]{Cv76}
P.~Cvitanovic, \pr{D14}{1976}{1536}.

\bibitem[dA98]{dACTP98}
F.~del~Aguila, A.~Culatti, R.~Mu\~noz Tapia, M.~P\'erez-Victoria,
\np{B537}{1999}{561} [hep-ph/9806451].

\bibitem[De93]{De93}
A.~Denner, \fp{41}{1993}{307}.

\bibitem[Ha98]{HaP98}
T.~Hahn and M.~P\'erez-Victoria, \cpc{118}{1999}{153} [hep-ph/9807565].

\bibitem[Ha00]{Ha00}
T.~Hahn, \cpc{140}{2001}{418} [hep-ph/0012260].

\bibitem[Ha02]{Ha02}
T.~Hahn, \npps{116}{2003}{363} [hep-ph/0210220].

\bibitem[Ha04]{Ha04}
T.~Hahn, \cpc{168}{2005}{78} [hep-ph/0404043].

\bibitem[Ha04a]{Ha04a}
T.~Hahn, \npps{135}{2004}{333} [hep-ph/0406288].

\bibitem[Ha06]{Ha06}
T.~Hahn, physics/0607103.

\bibitem[Ni05]{Ni05}
C.C.~Nishi, \ajp{73}{2005}{1160} [hep-ph/0412245].

\bibitem[Si79]{Si79}
W.~Siegel, \pl{B84}{1979}{193}.

\bibitem[tH72]{tHV72}
G.~'t~Hooft, M.~Veltman, \np{B44}{1972}{189}.

\bibitem[Ve00]{Ve00}
J.A.M.~Vermaseren, math-ph/0010025.
See also \Code{http://www.nikhef.nl/\home form}.

\bibitem[Ve96]{Ve96}
J.A.M.~Vermaseren, The use of computer algebra in QCD, in: H.~Latal, 
W.~Schweiger, Proceedings Schladming 1996, Springer Verlag, ISBN
3-540-62478-3.

\bibitem[WhBG05]{WhBG05}
M.R.~Whalley, D.~Bourilkov, R.C.~Group, hep-ph/0508110.
\Code{http://hepforge.cedar.ac.uk/lhapdf/}.

\bibitem[Za03]{Za03}
A.F.~Zarnecki, \app{B34}{2003}{2741} [hep-ex/0207021].

\end{thebibliography}
\end{flushleft}

\section*{Acknowledgements}

\FC\ would not be able to calculate in four dimensions without the hard
work of Manuel P\'erez-Victoria and his deep understanding of constrained
differential renormalization.

The fermionic matrix elements would not be half as well implemented
without the relentless testing of Christian Schappacher.


\printindex

\end{document}
